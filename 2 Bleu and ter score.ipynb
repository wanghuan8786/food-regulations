{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyter3","metadata":{"execution":{"iopub.status.busy":"2023-06-18T03:52:30.515809Z","iopub.execute_input":"2023-06-18T03:52:30.516261Z","iopub.status.idle":"2023-06-18T03:52:47.182997Z","shell.execute_reply.started":"2023-06-18T03:52:30.516220Z","shell.execute_reply":"2023-06-18T03:52:47.181622Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyter3\n  Downloading pyter3-0.3-py3-none-any.whl (4.1 kB)\nInstalling collected packages: pyter3\nSuccessfully installed pyter3-0.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# GPT-4 BLEU","metadata":{}},{"cell_type":"code","source":"import csv\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.tokenize import word_tokenize\n\ndef read_csv_file(file_path):\n    encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                reader = csv.reader(f)\n                lines = [row[0] for row in reader]\n            # tokenizing the sentences, BLEU score calculation requires tokenized sentences\n            lines = [word_tokenize(line) for line in lines]\n            return lines\n        except UnicodeDecodeError:\n            continue\n    raise ValueError('Unable to read the CSV file with the provided encodings.')\n\n# paths to your files\nreference_file_path = '/kaggle/input/bg-bleu-and-ter/BG REFERENCE.csv'\ncandidate_file_path = '/kaggle/input/bg-bleu-and-ter/BG GPT-4.csv'\n\n# read files\nreferences = read_csv_file(reference_file_path)\ncandidates = read_csv_file(candidate_file_path)\n\n# Calculate BLEU score\n# the corpus_bleu function requires a list of reference sentences for each candidate sentence\n# so we enclose each reference sentence in an additional list\nbleu_score = corpus_bleu([[ref] for ref in references], candidates)\n\nprint('BLEU score:', bleu_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T03:58:18.827653Z","iopub.execute_input":"2023-06-18T03:58:18.828202Z","iopub.status.idle":"2023-06-18T03:58:20.340167Z","shell.execute_reply.started":"2023-06-18T03:58:18.828125Z","shell.execute_reply":"2023-06-18T03:58:20.338618Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"BLEU score: 0.3552552641745027\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GOOGLE BLEU","metadata":{}},{"cell_type":"code","source":"import csv\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.tokenize import word_tokenize\n\ndef read_csv_file(file_path):\n    encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                reader = csv.reader(f)\n                lines = [row[0] for row in reader]\n            # tokenizing the sentences, BLEU score calculation requires tokenized sentences\n            lines = [word_tokenize(line) for line in lines]\n            return lines\n        except UnicodeDecodeError:\n            continue\n    raise ValueError('Unable to read the CSV file with the provided encodings.')\n\n# paths to your files\nreference_file_path = '/kaggle/input/bg-bleu-and-ter/BG REFERENCE.csv'\ncandidate_file_path = '/kaggle/input/bg-bleu-and-ter/BG GOOGLE.csv'\n\n# read files\nreferences = read_csv_file(reference_file_path)\ncandidates = read_csv_file(candidate_file_path)\n\n# Calculate BLEU score\n# the corpus_bleu function requires a list of reference sentences for each candidate sentence\n# so we enclose each reference sentence in an additional list\nbleu_score = corpus_bleu([[ref] for ref in references], candidates)\n\nprint('BLEU score:', bleu_score)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T04:02:52.424190Z","iopub.execute_input":"2023-06-18T04:02:52.424677Z","iopub.status.idle":"2023-06-18T04:02:52.622007Z","shell.execute_reply.started":"2023-06-18T04:02:52.424639Z","shell.execute_reply":"2023-06-18T04:02:52.619701Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"BLEU score: 0.4793512819043544\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GPT-4 TER","metadata":{}},{"cell_type":"code","source":"import csv\nfrom nltk.metrics.distance import edit_distance\n\n# Function to read CSV file content\ndef read_csv_file(file_path):\n    with open(file_path, 'r', encoding='latin-1') as file:\n        reader = csv.reader(file)\n        lines = [row for row in reader]\n    return lines\n\n# Paths to your files\nreference_file_path = '/kaggle/input/bg-bleu-and-ter/BG REFERENCE.csv'\ncandidate_file_path = '/kaggle/input/bg-bleu-and-ter/BG GPT-4.csv'\n\n# Read the files\nreference = read_csv_file(reference_file_path)\ncandidate = read_csv_file(candidate_file_path)\n\n# Check that you have the same number of lines in both files\nassert len(reference) == len(candidate), 'Number of lines in both files must be equal.'\n\n# Calculate TER for each pair of sentences and average over all sentences\ntotal_ter = 0.0\nfor ref_sentence, cand_sentence in zip(reference, candidate):\n    ref_sentence = ' '.join(ref_sentence)  # convert list of words to a sentence\n    cand_sentence = ' '.join(cand_sentence)  # convert list of words to a sentence\n    ter = edit_distance(ref_sentence.split(), cand_sentence.split()) / len(ref_sentence.split())\n    total_ter += ter\naverage_ter = total_ter / len(reference)\n\nprint('Average TER score:', average_ter)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-18T04:03:47.515979Z","iopub.execute_input":"2023-06-18T04:03:47.516480Z","iopub.status.idle":"2023-06-18T04:03:47.891640Z","shell.execute_reply.started":"2023-06-18T04:03:47.516397Z","shell.execute_reply":"2023-06-18T04:03:47.890205Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Average TER score: 0.5270677265499755\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GOOGLE TER","metadata":{}},{"cell_type":"code","source":"import csv\nfrom nltk.metrics.distance import edit_distance\n\n# Function to read CSV file content\ndef read_csv_file(file_path):\n    with open(file_path, 'r', encoding='latin-1') as file:\n        reader = csv.reader(file)\n        lines = [row for row in reader]\n    return lines\n\n# Paths to your files\nreference_file_path = '/kaggle/input/bg-bleu-and-ter/BG REFERENCE.csv'\ncandidate_file_path = '/kaggle/input/bg-bleu-and-ter/BG GOOGLE.csv'\n\n# Read the files\nreference = read_csv_file(reference_file_path)\ncandidate = read_csv_file(candidate_file_path)\n\n# Check that you have the same number of lines in both files\nassert len(reference) == len(candidate), 'Number of lines in both files must be equal.'\n\n# Calculate TER for each pair of sentences and average over all sentences\ntotal_ter = 0.0\nfor ref_sentence, cand_sentence in zip(reference, candidate):\n    ref_sentence = ' '.join(ref_sentence)  # convert list of words to a sentence\n    cand_sentence = ' '.join(cand_sentence)  # convert list of words to a sentence\n    ter = edit_distance(ref_sentence.split(), cand_sentence.split()) / len(ref_sentence.split())\n    total_ter += ter\naverage_ter = total_ter / len(reference)\n\nprint('Average TER score:', average_ter)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T04:04:46.102554Z","iopub.execute_input":"2023-06-18T04:04:46.103477Z","iopub.status.idle":"2023-06-18T04:04:46.490049Z","shell.execute_reply.started":"2023-06-18T04:04:46.103390Z","shell.execute_reply":"2023-06-18T04:04:46.488762Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Average TER score: 0.42636318553010655\n","output_type":"stream"}]}]}