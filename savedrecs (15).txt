FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Bhandari, A
   Ibrahim, M
   Sharma, C
   Liong, R
   Gustafson, S
   Prior, M
AF Bhandari, Abhishta
   Ibrahim, Muhammad
   Sharma, Chinmay
   Liong, Rebecca
   Gustafson, Sonja
   Prior, Marita
TI CT-based radiomics for differentiating renal tumours: a systematic
   review
SO ABDOMINAL RADIOLOGY
AB Purpose Differentiating renal tumours into grades and tumour subtype from medical imaging is important for patient management; however, there is an element of subjectivity when performed qualitatively. Quantitative analysis such as radiomics may provide a more objective approach. The purpose of this article is to systematically review the literature on computed tomography (CT) radiomics for grading and differentiating renal tumour subtypes. An educational perspective will also be provided. Methods The Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist was followed. PubMed, Scopus and Web of Science were searched for relevant articles. The quality of each study was assessed using the Radiomic Quality Score (RQS). Results 13 studies were found. The main outcomes were prediction of pathological grade and differentiating between renal tumour types, measured as area under the curve (AUC) for either the receiver operator curve or precision recall curve. Features extracted to predict pathological grade or tumour subtype included shape, intensity, texture and wavelet (a type of higher order feature). Four studies differentiated between low-grade and high-grade clear cell renal cell cancer (RCC) with good performance (AUC = 0.82-0.978). One other study differentiated low- and high-grade chromophobe with AUC = 0.84. Finally, eight studies used radiomics to differentiate between tumour types such as clear cell RCC, fat-poor angiomyolipoma, papillary RCC, chromophobe RCC and renal oncocytoma with high levels of performance (AUC 0.82-0.96). Conclusion Renal tumours can be pathologically classified using CT-based radiomics with good performance. The main radiomic feature used for tumour differentiation was texture. Fuhrman was the most common pathologic grading system used in the reviewed studies. Renal tumour grading studies should be extended beyond clear cell RCC and chromophobe RCC. Further research with larger prospective studies, performed in the clinical setting, across multiple institutions would help with clinical translation to the radiologist's workstation.
OI Bhandari, Abhishta/0000-0002-4509-9257; Liong,
   Rebecca/0000-0002-6780-8034
SN 2366-004X
EI 2366-0058
PD MAY
PY 2021
VL 46
IS 5
BP 2052
EP 2063
DI 10.1007/s00261-020-02832-9
EA NOV 2020
UT WOS:000584351500001
PM 33136182
ER

PT C
AU Carlson, J
   Tiberi, Z
   Safaei, M
   Ponder, RI
   Anton, SR
AF Carlson, Justin
   Tiberi, Zachariah
   Safaei, Mohsen
   Ponder, Robert I.
   Anton, Steven R.
GP ASME
TI Parametric Testing of Surrogate Knee Replacement Bearings with Embedded
   Piezoelectric Transducers
SO PROCEEDINGS OF THE ASME CONFERENCE ON SMART MATERIALS, ADAPTIVE
   STRUCTURES AND INTELLIGENT SYSTEMS, 2017, VOL 2
SE Proceedings of the ASME Conference on Smart Materials Adaptive
   Structures and Intelligent Systems
CT ASME Conference on Smart Materials, Adaptive Structures and Intelligent
   Systems (SMASIS2018)
CY SEP 10-12, 2018
CL San Antonio, TX
SP ASME, Aerosp Div
AB In the United States, Total Knee Replacement (TKR) is a surgery many people go through, but frequently, patients find that they are unhappy post -surgery due to misalignment and loosening of the knee. An estimated 20% of knee replacement recipients report discomfort or undesired functionality within their first few years after surgery. Surgical techniques currently rely heavily on experience and tactile feedback to correctly align the knee replacement. If surgical teams were to have access to data regarding compartmental forces within the knee over the life of the implant, then a more precise balancing procedure could be implemented. As it stands, the only way to obtain this in vivo data is for patients to undergo post -operative fluoroscopy procedures; unfortunately, patients have no incentive to undergo this process. This study tests the capabilities of knee bearings embedded with piezoelectric transducers to estimate the magnitude and location of loading given certain inputs. The prototype is fabricated from ultra high molecular weight (UHMW) polyethylene using Computer Numerical Control (CNC) machining. For this study, to simulate loads under both normal and irregular knee positions, a custom fixture is designed and fabricated for use in a uniaxial load frame. The problem at hand necessitates a more realistic knee testing environment that can simulate the loading types of both balanced and imbalanced knees. Thus, the fixture permits various degrees of internal and external rotation. Additionally, through use of an X Y translational table, the setup allows for in-plane translation between the condyles of the femoral component and the bearing prototype. This study compares values of force location from the piezoelectric sensors to measurements from pressure sensitive film. The piezoelectric knee bearing is tested to lay the groundwork for in vivo testing. Future work expanding upon this research would include designing and optimizing an in vivo knee bearing replacement to facilitate force location and magnitude data collection in a system free from external power sources by utilizing the energy harvesting capabilities of piezoelectrics.
SN 2153-2001
BN 978-0-7918-5195-1
PY 2018
AR V002T06A004
UT WOS:000457516500037
ER

PT C
AU Gupta, M
   Bharti, SS
   Agarwal, S
AF Gupta, Manish
   Bharti, Shambhu Shankar
   Agarwal, Suneeta
GP IEEE
TI Implicit Language Identification System based on Random Forest and
   Support Vector Machine for Speech
SO 2017 4TH INTERNATIONAL CONFERENCE ON POWER, CONTROL & EMBEDDED SYSTEMS
   (ICPCES)
CT 4th International Conference on Power, Control & Embedded Systems
   (ICPCES)
CY MAR 09-11, 2017
CL ALLAHABAD, INDIA
SP Motilal Nehru Natl Inst Technol, Dept Elect Engn, IEEE, TEQIP II
AB Speech uttered by the human beings contains the information about speakers, languages and contents. Language of uttered speech can easily be identified by extracting the language specific information from it. Identification of language of speech is known as Language Identification (LID). Identification of language from speech is helpful in its translation, speech recognition and speech activated automatic systems. LID system may also play an important role in speaker recognition as identification of language can be used to reduce search space. In this paper an approach based on Linear Predictive Coding (LPC) and Mel Frequency Cepstral Coefficients (MFCCs) features for language identification is proposed using SVM and Random Forest (RF) classification techniques. Both LPC and MFCC features are vocal tract features. LPC and MFCC features extracted from uttered speech contain language as well as speaker related informations. Identification of language highly depends upon extraction of language specific features. Both these vocal tract parameters of speech contain lot of information about languages spoken compared to other parameters like excitation source parameters and prosodic parameters. Hence combination of these features performs better than individual. Experiments have been performed on the database obtained from IIIT-Hyderabad consisting of 5000 multilingual clean speech signals (Hindi, Bengali, Telugu, Tamil, Marathi and Malayalam). For training the proposed model, 600 speech signals are taken arbitrarily from the above database. Language model are created for each language. Evaluation of the proposed models has been made using other 300 speech signals from same database. Language models are evaluated using individual features as well as combined features. Experiments performed by taking both features at a time give better result as compared to taking individual features one at a time. Using these features, the accuracy of language identification is not more than 80% so far as claimed by other researchers. In the proposed approach, the accuracy of language identification is improved to 92.6% using combination of same features and random forest model.
RI gupta, manish/HIK-2539-2022; Bharti, Shambhu Shankar/GPW-9483-2022
OI bharti, shambhu shankar/0000-0003-3563-8226; Gupta, Dr.
   Manish/0000-0002-8664-2244
BN 978-1-5090-4426-9
PY 2017
UT WOS:000427438000014
ER

PT J
AU Shaik, NA
   Kaleemuddin, M
   Banaganapalli, B
   Khan, F
   Shaik, NS
   Ajabnoor, G
   Al-Harthi, SE
   Bondagji, N
   Al-Aama, JY
   Elango, R
AF Shaik, Noor A.
   Kaleemuddin, Mohammed
   Banaganapalli, Babajan
   Khan, Fazal
   Shaik, Nazia S.
   Ajabnoor, Ghada
   Al-Harthi, Sameer E.
   Bondagji, Nabeel
   Al-Aama, Jumana Y.
   Elango, Ramu
TI Structural and Functional Characterization of Pathogenic Non-Synonymous
   Genetic Mutations of Human Insulin-Degrading Enzyme by In Silico Methods
SO CNS & NEUROLOGICAL DISORDERS-DRUG TARGETS
AB Insulin-degrading enzyme (IDE) is a key protease involved in degrading insulin and amyloid peptides in human body. Several non-synonymous genetic mutations of IDE gene have been recently associated with susceptibility to both diabetes and Alzheimer's diseases. However, the consequence of these mutations on the structure of IDE protein and its substrate binding characteristics is not well elucidated. The computational investigation of genetic mutation consequences on structural level of protein is recently found to be an effective alternate to traditional in vivo and in vitro approaches. Hence, by using a combination of empirical rule and support vector machine based in silico algorithms, this study was able to identify that the pathogenic non-synonymous genetic mutations corresponding to p.I54F, p.P122T, p.T533R, p.P581A and p.Y609A have more potential role in structural and functional deviations of IDE activity. Moreover, molecular modeling and secondary structure analysis have also confirmed their impact on the stability and secondary properties of IDE protein. The molecular docking analysis of IDE with combinational substrates has revealed that peptide inhibitors compared to small non-peptide inhibitor molecules possess good inhibitory activity towards mutant IDE. This finding may pave a way to design novel potential small peptide inhibitors for mutant IDE. Additionally by un-translated region (UTR) scanning analysis, two regulatory pathogenic genetic mutations i.e., rs5786997 (3' UTR) and rs4646954 (5' UTR), which can influence the translation pattern of IDE gene through sequence alteration of upstream-Open Reading Frame and Internal Ribosome Entry Site elements were identified. Our findings are expected to help in narrowing down the number of IDE genetic variants to be screened for disease association studies and also to select better competitive inhibitors for IDE related diseases.
RI Elango, Ramu/C-7684-2014; Mohammed, Kaleemuddin/AAH-4465-2021; Bondagji,
   Nabeel/AAZ-5664-2020; Ali, Ahmed Shaker/AAE-9610-2019; Ali,
   Ahmed/HOF-4672-2023; Shaker, Ahmed/HIR-7380-2022; shaik, noor
   a/C-5509-2013; Al-aama, Jumana/AAA-8653-2021; Shaik, Noor
   ahmad/ABE-4084-2020; Banaganapalli, Babajan/A-6282-2013; KHAN,
   FAZAL/N-9792-2013
OI Elango, Ramu/0000-0003-4264-2135; Mohammed,
   Kaleemuddin/0000-0002-3781-2462; Ali, Ahmed Shaker/0000-0002-3341-8177;
   Shaik, Noor ahmad/0000-0002-7133-656X; Banaganapalli,
   Babajan/0000-0001-8089-2210; KHAN, FAZAL/0000-0003-1794-8167; Al-Harthi,
   Sameer/0000-0003-4293-3215
SN 1871-5273
EI 1996-3181
PY 2014
VL 13
IS 3
BP 517
EP 532
DI 10.2174/18715273113126660161
UT WOS:000340836000018
PM 24059301
ER

PT J
AU Humbard, MA
   Surkov, S
   De Donatis, GM
   Jenkins, LM
   Maurizi, MR
AF Humbard, Matthew A.
   Surkov, Serhiy
   De Donatis, Gian Marco
   Jenkins, Lisa M.
   Maurizi, Michael R.
TI The N-degradome of Escherichia coli LIMITED PROTEOLYSIS IN VIVO
   GENERATES A LARGE POOL OF PROTEINS BEARING N-DEGRONS
SO JOURNAL OF BIOLOGICAL CHEMISTRY
AB The N-end rule is a conserved mechanism found in Gram-negative bacteria and eukaryotes for marking proteins to be degraded by ATP-dependent proteases. Specific N-terminal amino acids (N-degrons) are sufficient to target a protein to the degradation machinery. In Escherichia coli, the adaptor ClpS binds an N-degron and delivers the protein to ClpAP for degradation. As ClpS recognizes N-terminal Phe, Trp, Tyr, and Leu, which are not found at the N terminus of proteins translated and processed by the canonical pathway, proteins must be post-translationally modified to expose an N-degron. One modification is catalyzed by Aat, an enzyme that adds leucine or phenylalanine to proteins with N-terminal lysine or arginine; however, such proteins are also not generated by the canonical protein synthesis pathway. Thus, the mechanisms producing N-degrons in proteins and the frequency of their occurrence largely remain a mystery. To address these issues, we used a ClpS affinity column to isolate interacting proteins from E. coli cell lysates under non-denaturing conditions. We identified more than 100 proteins that differentially bound to a column charged with wild-type ClpS and eluted with a peptide bearing an N-degron. Thirty-two of 37 determined N-terminal peptides had N-degrons. Most of the proteins were N-terminally truncated by endoproteases or exopeptidases, and many were further modified by Aat. The identities of the proteins point to possible physiological roles for the N-end rule in cell division, translation, transcription, and DNA replication and reveal widespread proteolytic processing of cellular proteins to generate N-end rule substrates.
RI Jenkins, Lisa/AAI-2490-2021
EI 1083-351X
PD OCT 4
PY 2013
VL 288
IS 40
BP 28913
EP 28924
DI 10.1074/jbc.M113.492108
UT WOS:000330298800043
PM 23960079
ER

PT J
AU Giberti, H
   Abbattista, T
   Carnevale, M
   Giagu, L
   Cristini, F
AF Giberti, Hermes
   Abbattista, Tommaso
   Carnevale, Marco
   Giagu, Luca
   Cristini, Fabio
TI A Methodology for Flexible Implementation of Collaborative Robots in
   Smart Manufacturing Systems
SO ROBOTICS
AB Small-scale production is relying more and more on personalization and flexibility as an innovation key for success in response to market needs such as diversification of consumer preferences and/or greater regulatory pressure. This can be possible thanks to assembly lines dynamically adaptable to new production requirements, easily reconfigurable and reprogrammable to any change in the production line. In such new automated production lines, where traditional automation is not applicable, human and robot collaboration can be established, giving birth to a kind of industrial craftsmanship. The idea at the base of this work is to take advantage of collaborative robotics by using the robots as other generic industrial tools. To overcome the need of complex programming, identified in the literature as one of the main issues preventing cobot diffusion into industrial environments, the paper proposes an approach for simplifying the programming process while still maintaining high flexibility through a pyramidal parametrized approach exploiting cobot collaborative features. An Interactive Refinement Programming procedure is described and validated through a real test case performed as a pilot in the Building Automation department of ABB in Vittuone (Milan, Italy). The key novel ingredients in this approach are a first translation phase, carried out by engineers of production processes who convert the sequence of assembly operations into a preliminary code built as a sequence of robot operations, followed by an on-line correction carried out by non-expert users who can interact with the machine to define the input parameters to make the robotic code runnable. The users in this second step do not need any competence in programming robotic code. Moreover, from an economic point of view, a standardized way of assessing the convenience of the robotic investment is proposed. Both economic and technical results highlight improvements in comparison to the traditional automation approach, demonstrating the possibility to open new further opportunities for collaborative robots when small/medium batch sizes are involved.
RI ; Giberti, Hermes/N-2601-2015
OI Carnevale, Marco/0000-0001-6044-8323; Giberti,
   Hermes/0000-0001-8840-8497
EI 2218-6581
PD FEB
PY 2022
VL 11
IS 1
AR 9
DI 10.3390/robotics11010009
UT WOS:000765091300001
ER

PT J
AU Razumovskaia, E
   Glavas, G
   Majewska, O
   Ponti, EM
   Korhonen, A
   Vulic, I
AF Razumovskaia, Evgeniia
   Glavas, Goran
   Majewska, Olga
   Ponti, Edoardo M.
   Korhonen, Anna
   Vulic, Ivan
TI Crossing the Conversational Chasm: A Primer on Natural Language
   Processing for Multilingual Task-Oriented Dialogue Systems
SO JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH
AB In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.
SN 1076-9757
EI 1943-5037
PY 2022
VL 74
BP 1351
EP 1402
UT WOS:000828331600001
ER

PT J
AU Abbas, S
   Al-Barhamtoshy, H
   Alotaibi, F
AF Abbas, Samah
   Al-Barhamtoshy, Hassanin
   Alotaibi, Fahad
TI Towards an Arabic Sign Language (ArSL) corpus for deaf drivers
SO PEERJ COMPUTER SCIENCE
AB Sign language is a common language that deaf people around the world use to communicate with others. However, normal people are generally not familiar with sign language (SL) and they do not need to learn their language to communicate with them in everyday life. Several technologies offer possibilities for overcoming these barriers to assisting deaf people and facilitating their active lives, including natural language processing (NLP), text understanding, machine translation, and sign language simulation. In this paper, we mainly focus on the problem faced by the deaf community in Saudi Arabia as an important member of the society that needs assistance in communicating with others, especially in the field of work as a driver. Therefore, this community needs a system that facilitates the mechanism of communication with the users using NLP that allows translating Arabic Sign Language (ArSL) into voice and vice versa. Thus, this paper aims to purplish our created dataset dictionary and ArSL corpus videos that were done in our previous work. Furthermore, we illustrate our corpus, data determination (deaf driver terminologies), dataset creation and processing in order to implement the proposed future system. Therefore, the evaluation of the dataset will be presented and simulated using two methods. First, using the evaluation of four expert signers, where the result was 10.23% WER. The second method, using Cohen's Kappa in order to evaluate the corpus of ArSL videos that was made by three signers from different regions of Saudi Arabia. We found that the agreement between signer 2 and signer 3 is 61%, which is a good agreement. In our future direction, we will use the ArSL video corpus of signer 2 and signer 3 to implement ML techniques for our deaf driver system.
RI Alotaibi, Fahad/AAT-2112-2021; Abbas, Samah Anwar/AEH-7795-2022
OI Abbas, Samah Anwar/0000-0003-4801-1705
EI 2376-5992
PD NOV 19
PY 2021
VL 7
AR e741
DI 10.7717/peerj-cs.741
UT WOS:000721214500003
ER

PT J
AU Tandon, A
   Mohan, N
   Jensen, C
   Burkhardt, BEU
   Gooty, V
   Castellanos, DA
   McKenzie, PL
   Abou Zahr, R
   Bhattaru, A
   Abdulkarim, M
   Amir-Khalili, A
   Sojoudi, A
   Rodriguez, SM
   Dillenbeck, J
   Greil, GF
   Hussain, T
AF Tandon, Animesh
   Mohan, Navina
   Jensen, Cory
   Burkhardt, Barbara E. U.
   Gooty, Vasu
   Castellanos, Daniel A.
   McKenzie, Paige L.
   Abou Zahr, Riad
   Bhattaru, Abhijit
   Abdulkarim, Mubeena
   Amir-Khalili, Alborz
   Sojoudi, Alireza
   Rodriguez, Stephen M.
   Dillenbeck, Jeanne
   Greil, Gerald F.
   Hussain, Tarique
TI Retraining Convolutional Neural Networks for Specialized Cardiovascular
   Imaging Tasks: Lessons from Tetralogy of Fallot
SO PEDIATRIC CARDIOLOGY
AB Ventricular contouring of cardiac magnetic resonance imaging is the gold standard for volumetric analysis for repaired tetralogy of Fallot (rTOF), but can be time-consuming and subject to variability. A convolutional neural network (CNN) ventricular contouring algorithm was developed to generate contours for mostly structural normal hearts. We aimed to improve this algorithm for use in rTOF and propose a more comprehensive method of evaluating algorithm performance. We evaluated the performance of a ventricular contouring CNN, that was trained on mostly structurally normal hearts, on rTOF patients. We then created an updated CNN by adding rTOF training cases and evaluated the new algorithm's performance generating contours for both the left and right ventricles (LV and RV) on new testing data. Algorithm performance was evaluated with spatial metrics (Dice Similarity Coefficient (DSC), Hausdorff distance, and average Hausdorff distance) and volumetric comparisons (e.g., differences in RV volumes). The original Mostly Structurally Normal (MSN) algorithm was better at contouring the LV than the RV in patients with rTOF. After retraining the algorithm, the new MSN + rTOF algorithm showed improvements for LV epicardial and RV endocardial contours on testing data to which it was naive (N = 30; e.g., DSC 0.883 vs. 0.905 for LV epicardium at end diastole, p < 0.0001) and improvements in RV end-diastolic volumetrics (median %error 8.1 vs 11.4, p = 0.0022). Even with a small number of cases, CNN-based contouring for rTOF can be improved. This work should be extended to other forms of congenital heart disease with more extreme structural abnormalities. Aspects of this work have already been implemented in clinical practice, representing rapid clinical translation. The combined use of both spatial and volumetric comparisons yielded insights into algorithm errors.
RI Gooty, Vasu/AAL-4132-2020
OI Gooty, Vasu/0000-0002-1826-3568; Hussain, Tarique/0000-0003-4091-992X;
   Mohan, Navina/0000-0002-6320-9579; Abou Zahr, Riad/0000-0001-5208-5331;
   Burkhardt, Barbara/0000-0002-0412-9958; Tandon,
   Animesh/0000-0001-9769-8801; Bhattaru, Abhijit/0000-0002-1383-1992;
   Hussain, Tariq/0000-0002-1332-2418
SN 0172-0643
EI 1432-1971
PD MAR
PY 2021
VL 42
IS 3
BP 578
EP 589
DI 10.1007/s00246-020-02518-5
EA JAN 2021
UT WOS:000604847100014
PM 33394116
ER

PT J
AU Fernandez-Gonzalez, D
   Gomez-Rodriguez, C
AF Fernandez-Gonzalez, Daniel
   Gomez-Rodriguez, Carlos
TI Faster shift-reduce constituent parsing with a non-binary, bottom-up
   strategy
SO ARTIFICIAL INTELLIGENCE
AB An increasingly wide range of artificial intelligence applications rely on syntactic information to process and extract meaning from natural language text or speech, with constituent trees being one of the most widely used syntactic formalisms. To produce these phrase-structure representations from sentences in natural language, shift-reduce constituent parsers have become one of the most efficient approaches. Increasing their accuracy and speed is still one of the main objectives pursued by the research community so that artificial intelligence applications that make use of parsing outputs, such as machine translation or voice assistant services, can improve their performance. With this goal in mind, we propose in this article a novel non-binary shift-reduce algorithm for constituent parsing. Our parser follows a classical bottom-up strategy but, unlike others, it straightforwardly creates non-binary branchings with just one Reduce transition, instead of requiring prior binarization or a sequence of binary transitions, allowing its direct application to any language without the need of further resources such as percolation tables. As a result, it uses fewer transitions per sentence than existing transition-based constituent parsers, becoming the fastest such system and, as a consequence, speeding up downstream applications. Using static oracle training and greedy search, the accuracy of this novel approach is on par with state-of-the-art transition-based constituent parsers and outperforms all top-down and bottom-up greedy shift-reduce systems on the Wall Street Journal section from the English Penn Treebank and the Penn Chinese Treebank. Additionally, we develop a dynamic oracle for training the proposed transition-based algorithm, achieving further improvements in both benchmarks and obtaining the best accuracy to date on the Penn Chinese Treebank among greedy shift-reduce parsers. (C) 2019 Elsevier B.V. All rights reserved.
RI Gómez-Rodríguez, Carlos/A-5935-2011
OI Gómez-Rodríguez, Carlos/0000-0003-0752-8812; Fernandez-Gonzalez,
   Daniel/0000-0002-6733-2371
SN 0004-3702
EI 1872-7921
PD OCT
PY 2019
VL 275
BP 559
EP 574
DI 10.1016/j.artint.2019.07.006
UT WOS:000485208100021
ER

PT J
AU Jelinek, F
   Smit, G
   Breedveld, P
AF Jelinek, Filip
   Smit, Gerwin
   Breedveld, Paul
TI Bioinspired Spring-Loaded Biopsy Harvester-Experimental Prototype Design
   and Feasibility Tests
SO JOURNAL OF MEDICAL DEVICES-TRANSACTIONS OF THE ASME
AB Current minimally invasive laparoscopic tissue-harvesting techniques for pathological purposes involve taking multiple imprecise and inaccurate biopsies, usually using a laparoscopic forceps or other assistive devices. Potential hazards, e.g., cancer spread when dealing with tumorous tissue, call for a more reliable alternative in the form of a single laparoscopic instrument capable of repeatedly taking a precise biopsy at a desired location. Therefore, the aim of this project was to design a disposable laparoscopic instrument tip, incorporating a centrally positioned glass fiber for tissue diagnostics; a cutting device for fast, accurate, and reliable biopsy of a precisely defined volume; and a container suitable for sample storage. Inspired by the sea urchin's chewing organ, Aristotle's lantern, and its capability of rapid and simultaneous tissue incision and enclosure by axial translation, we designed a crown-shaped collapsible cutter operating on a similar basis. Based on a series of in vitro experiments indicating that tissue deformation decreases with increasing penetration speed leading to a more precise biopsy, we decided on the cutter's forward propulsion via a spring. Apart from the embedded springloaded cutter, the biopsy harvester comprises a smart mechanism for cutter preloading, locking, and actuation, as well as a sample container. A real-sized biopsy harvester prototype was developed and tested in a universal tensile testing machine at TU Delft. In terms of mechanical functionality, the preloading, locking, and actuation mechanism as well as the cutter's rapid incising and collapsing capabilities proved to work successfully in vitro. Further division of the tip into a permanent and a disposable segment will enable taking of multiple biopsies, mutually separated in individual containers. We believe the envisioned laparoscopic optomechanical biopsy device will be a solution ameliorating timedemanding, inaccurate, and potentially unsafe laparoscopic biopsy procedures.
RI Smit, Gerwin/B-9994-2012; Jelínek, Filip/H-3854-2019
OI Smit, Gerwin/0000-0002-8160-3238; Jelínek, Filip/0000-0001-5909-6674
SN 1932-6181
EI 1932-619X
PD MAR
PY 2014
VL 8
IS 1
AR 015002
DI 10.1115/1.4026449
UT WOS:000330355200020
ER

PT J
AU Ispoglou, T
   King, RFGJ
   Polman, RCJ
   Zanker, C
AF Ispoglou, Theocharis
   King, Roderick F. G. J.
   Polman, Remco C. J.
   Zanker, Cathy
TI Daily L-Leucine Supplementation in Novice Trainees During a 12-Week
   Weight Training Program
SO INTERNATIONAL JOURNAL OF SPORTS PHYSIOLOGY AND PERFORMANCE
AB Purpose: To investigate the effects of daily oral L-leucine ingestion on strength, bone mineral-free lean tissue mass (LTM) and fat mass (FM) of free living humans during a 12-wk resistance-training program. Methods: Twenty-six initially untrained men (n = 13 per group) ingested either 4 g/d of L-leucine (leucine group: age 28.5 +/- 8.2 y, body mass index 24.9 +/- 4.2 kg/m(2)) or a corresponding amount of lactose (placebo group: age 28.2 +/- 7.3 y, body mass index 24.9 4.2 kg/m(2)). All participants trained under supervision twice per week following a prescribed resistance training program using eight standard exercise machines. Testing took place at baseline and at the end of the supplementation period. Strength on each exercise was assessed by five repetition maximum (5-RM), and body composition was assessed by dual energy X-ray absorptiometry (DXA). Results: The leucine group demonstrated significantly higher gains in total 5-RM strength (sum of 5-RM in eight exercises) and 5-RM strength in five out of the eight exercises (P < .05). The percentage total 5-RM strength gains were 40.8% (+/- 7.8) and 31.0% (+/- 4.6) for the leucine and placebo groups respectively. Significant differences did not exist between groups in either total percentage LTM gains or total percentage FM losses (LTM: 2.9% +/- 2.5 vs 2.0% +/- 2.1, FM: 1.6% +/- 15.6 vs 1.1% +/- 7.6). Conclusion: These results suggest that 4 g/d of L-leucine supplementation may be used as a nutritional supplement to enhance strength performance during a 12-week resistance training program of initially untrained male participants.
RI Ispoglou, Theocharis/N-2503-2016; Polman, Remco C J/D-1877-2013
OI Ispoglou, Theocharis/0000-0002-7608-6512; Polman, Remco C
   J/0000-0003-2951-0904
SN 1555-0265
EI 1555-0273
PD MAR
PY 2011
VL 6
IS 1
BP 38
EP 50
DI 10.1123/ijspp.6.1.38
UT WOS:000289025200008
PM 21487148
ER

PT J
AU Karczewski, AM
   Dingle, AM
   Poore, SO
AF Karczewski, Alison M.
   Dingle, Aaron M.
   Poore, Samuel O.
TI The Need to Work Arm in Arm: Calling for Collaboration in Delivering
   Neuroprosthetic Limb Replacements
SO FRONTIERS IN NEUROROBOTICS
AB Over the last few decades there has been a push to enhance the use of advanced prosthetics within the fields of biomedical engineering, neuroscience, and surgery. Through the development of peripheral neural interfaces and invasive electrodes, an individual's own nervous system can be used to control a prosthesis. With novel improvements in neural recording and signal decoding, this intimate communication has paved the way for bidirectional and intuitive control of prostheses. While various collaborations between engineers and surgeons have led to considerable success with motor control and pain management, it has been significantly more challenging to restore sensation. Many of the existing peripheral neural interfaces have demonstrated success in one of these modalities; however, none are currently able to fully restore limb function. Though this is in part due to the complexity of the human somatosensory system and stability of bioelectronics, the fragmentary and as-yet uncoordinated nature of the neuroprosthetic industry further complicates this advancement. In this review, we provide a comprehensive overview of the current field of neuroprosthetics and explore potential strategies to address its unique challenges. These include exploration of electrodes, surgical techniques, control methods, and prosthetic technology. Additionally, we propose a new approach to optimizing prosthetic limb function and facilitating clinical application by capitalizing on available resources. It is incumbent upon academia and industry to encourage collaboration and utilization of different peripheral neural interfaces in combination with each other to create versatile limbs that not only improve function but quality of life. Despite the rapidly evolving technology, if the field continues to work in divided "silos," we will delay achieving the critical, valuable outcome: creating a prosthetic limb that is right for the patient and positively affects their life.
SN 1662-5218
PD JUL 21
PY 2021
VL 15
AR 711028
DI 10.3389/fnbot.2021.711028
UT WOS:000681058100001
PM 34366820
ER

PT J
AU Schmitz, S
AF Schmitz, Sigrid
TI TechnoBrainBodies-in-Cultures: An Intersectional Case
SO FRONTIERS IN SOCIOLOGY
AB The cyborgization of brainbodies with computer hardware and software today ranges in scope from the realization of Brain-Computer Interfaces (BCIs) to visions of mind upload to silicon, the latter being targeted toward a transhuman future. Refining posthumanist concepts to formulate a posthumanities perspective, and contrasting those approaches with transhumanist trajectories, I explore the intersectional dimension of realizations and visions of neuro-technological developments, which I name TechnoBrainBodies-in-Cultures. In an intersectional analysis, I investigate the embedding and legitimation of transhumanist visions brought about by neuroscientific research and neuro-technological development based on a concept of modern neurobiological determinism. The conjoined trajectories of BCI research and development and transhumanist visions perpetuate the inscription of intersectional norms, with the concomitant danger of producing discriminatory effects. This culminates in normative capacity being seen as a conflation of the abled, successful, white masculinized techno-brain with competition. My deeper analysis, however, also enables displacements within recent BCI research and development to be characterized: from ''thought-translation" to affective conditioning and from controllability to obstinacy within the BCI, going so far as to open the closed loop. These realizations challenge notions about the BCI's actor status and agency and foster questions about shifts in the corresponding subject-object relations. Based on these analyses, I look at the effects of neuro-technological and transhumanist governmentality on the question of whose lives are to be improved and whose lives should be excluded from these developments. Within the framework of political feminist materialisms, I combine the concept of posthumanities with my concept of TechnoBrainBodies-in-Cultures to envision and discuss a material-discursive strategy, encompassing dimensions of affect, sociality, resistance, compassion, cultural diversity, ethnic diversity, multiple sexes/sexualities, aging, dis/abilities-in short, all of this "intersectional stuff"-as well as obstinate techno-brain agencies and contumacies foreseen in these cyborgian futures.
EI 2297-7775
PD APR 27
PY 2021
VL 6
AR 651486
DI 10.3389/fsoc.2021.651486
UT WOS:000679122800001
PM 33987221
ER

PT J
AU Cambuim, LFS
   Macieira, RM
   Neto, FMP
   Barros, E
   Ludermir, TB
   Zanchettin, C
AF Cambuim, Lucas F. S.
   Macieira, Rafael M.
   Neto, Fernando M. P.
   Barros, Edna
   Ludermir, Teresa B.
   Zanchettin, Cleber
TI An efficient static gesture recognizer embedded system based on ELM
   pattern recognition algorithm
SO JOURNAL OF SYSTEMS ARCHITECTURE
AB Millions of people throughout the world describe themselves as being deaf. Some of them suffer from severe hearing loss and consequently use an alternative manner with which to communicate with society by means of either written or visual language. There are several sign languages capable of dealing with such a need. Nonetheless, a communication gap still exists even when using such languages, since only a small fraction of the population is able to use them. Over the last few years, due to the increasing need for universal accessibility when using computational resources, gesture recognition has been widely researched. Thus, in an attempt to reduce this communication gap, our approach proposes a computational solution in order to translate static gesture symbols into text symbols, through computer vision, without the use of hand sensors or gloves. In order to guarantee the highest quality, with emphasis on the reliability of the system and real-time translation, we have developed an approach based on the Extreme Learning Machine (ELM) pattern recognition algorithms fully implemented in hardware, and have assessed it to measure these two metrics. Hardware components were designed in order to perform the best image processing and pattern recognition tasks used within the project. As a case study, and so as to validate the technique, a recognition system for the Brazilian Sign Language (LIBRAS) was implemented. Besides ensuring that this approach could be used for any static hand gesture symbol recognition, our main goal was to guarantee fast, reliable gesture recognition for communication between humans. Experimental results have demonstrated that the system is able to recognize LIBRAS symbols with an accuracy of 97%, a response time of 6.5ms per letter recognition, and using only 43% (about 64,851 logic elements) of the FPGA area. (C) 2016 Elsevier B.V. All rights reserved.
RI Zanchettin, Cleber/ABH-3328-2021; Ludermir, Teresa B/F-6766-2012;
   Zanchettin, Cleber/C-3196-2017
OI Zanchettin, Cleber/0000-0001-6421-9747; Ludermir, Teresa
   B/0000-0002-8980-6742; Zanchettin, Cleber/0000-0001-6421-9747; Barros,
   Edna/0000-0001-6479-3052; de Paula Neto, Fernando/0000-0003-4264-1124
SN 1383-7621
EI 1873-6165
PD AUG
PY 2016
VL 68
BP 1
EP 16
DI 10.1016/j.sysarc.2016.06.002
UT WOS:000380599700001
ER

PT J
AU Kaths, JM
   Echeverri, J
   Goldaracena, N
   Louis, KS
   Yip, P
   John, R
   Mucsi, I
   Ghanekar, A
   Bagli, D
   Selzner, M
   Robinson, LA
AF Kaths, J. Moritz
   Echeverri, Juan
   Goldaracena, Nicolas
   Louis, Kristine S.
   Yip, Paul
   John, Rohan
   Mucsi, Istvan
   Ghanekar, Anand
   Bagli, Darius
   Selzner, Markus
   Robinson, Lisa A.
TI Heterotopic Renal Autotransplantation in a Porcine Model: A Step-by-Step
   Protocol
SO JOVE-JOURNAL OF VISUALIZED EXPERIMENTS
AB Kidney transplantation is the treatment of choice for patients suffering from end-stage renal disease. It offers better life expectancy and higher quality of life when compared to dialysis. Although the last few decades have seen major improvements in patient outcomes following kidney transplantation, the increasing shortage of available organs represents a severe problem worldwide. To expand the donor pool, marginal kidney grafts recovered from extended criteria donors (ECD) or donated after circulatory death (DCD) are now accepted for transplantation. To further improve the postoperative outcome of these marginal grafts, research must focus on new therapeutic approaches such as alternative preservation techniques, immunomodulation, gene transfer, and stem cell administration.
   Experimental studies in animal models are the final step before newly developed techniques can be translated into clinical practice. Porcine kidney transplantation is an excellent model of human transplantation and allows investigation of novel approaches. The major advantage of the porcine model is its anatomical and physiological similarity to the human body, which facilitates the rapid translation of new findings to clinical trials. This article offers a surgical step-by-step protocol for an autotransplantation model and highlights key factors to ensure experimental success. Adequate pre- and postoperative housing, attentive anesthesia, and consistent surgical techniques result in favorable postoperative outcomes. Resection of the contralateral native kidney provides the opportunity to assess post-transplant graft function. The placement of venous and urinary catheters and the use of metabolic cages allow further detailed evaluation. For long-term follow-up studies and investigation of alternative graft preservation techniques, autotransplantation models are superior to allotransplantation models, as they avoid the confounding bias posed by rejection and immunosuppressive medication.
RI Selzner, Markus/AAE-9071-2020; Mucsi, Istvan/B-4850-2010
OI Selzner, Markus/0000-0002-4326-0887; Yip, Paul/0000-0002-7635-7407;
   Mucsi, Istvan/0000-0002-4781-4699; John, Rohan/0000-0001-7946-3013
SN 1940-087X
PD FEB
PY 2016
IS 108
AR e53765
DI 10.3791/53765
UT WOS:000372504100072
PM 26967919
ER

PT J
AU Guizilini, V
   Ramos, F
AF Guizilini, Vitor
   Ramos, Fabio
TI Semi-parametric learning for visual odometry
SO INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH
AB This paper addresses the visual odometry problem from a machine learning perspective. Optical flow information from a single camera is used as input for a multiple-output Gaussian process (MOGP) framework, that estimates linear and angular camera velocities. This approach has several benefits. (1) It substitutes the need for conventional camera calibration, by introducing a semi-parametric model that is able to capture nuances that a strictly parametric geometric model struggles with. (2) It is able to recover absolute scale if a range sensor (e.g. a laser scanner) is used for ground-truth, provided that training and testing data share a certain similarity. (3) It is naturally able to provide measurement uncertainties. We extend the standard MOGP framework to include the ability to infer joint estimates (full covariance matrices) for both translation and rotation, taking advantage of the fact that all estimates are correlated since they are derived from the same vehicle. We also modify the common zero mean assumption of a Gaussian process to accommodate a standard geometric model of the camera, thus providing an initial estimate that is then further refined by the non-parametric model. Both Gaussian process hyperparameters and camera parameters are trained simultaneously, so there is still no need for traditional camera calibration, although if these values are known they can be used to speed up training. This approach has been tested in a wide variety of situations, both 2D in urban and off-road environments (two degrees of freedom) and 3D with unmanned aerial vehicles (six degrees of freedom), with results that are comparable to standard state-of-the-art visual odometry algorithms and even more traditional methods, such as wheel encoders and laser-based Iterative Closest Point. We also test its limits to generalize over environment changes by varying training and testing conditions independently, and also by changing cameras between training and testing.
RI Vasilescu, Iuliu/AAD-8067-2020
SN 0278-3649
EI 1741-3176
PD APR
PY 2013
VL 32
IS 5
BP 526
EP 546
DI 10.1177/0278364912472245
UT WOS:000325691500002
ER

PT J
AU Barman, I
   Dingari, NC
   Singh, GP
   Soares, JS
   Dasari, RR
   Smulko, JM
AF Barman, Ishan
   Dingari, Narahara Chari
   Singh, Gajendra Pratap
   Soares, Jaqueline S.
   Dasari, Ramachandra R.
   Smulko, Janusz M.
TI Investigation of Noise-Induced Instabilities in Quantitative Biological
   Spectroscopy and Its Implications for Noninvasive Glucose Monitoring
SO ANALYTICAL CHEMISTRY
AB Over the past decade, optical spectroscopy has been employed in combination with multivariate chemometric models to investigate a wide variety of diseases and pathological conditions, primarily due to its excellent chemical specificity and lack of sample preparation requirements. Despite promising results in several proof-of-concept studies, its translation to the clinical setting has often been hindered by inadequate accuracy of the conventional spectroscopic models. To address this issue and the possibility of curved (nonlinear) effects in the relationship between the concentrations of the analyte of interest and the mixture spectra (due to fluctuations in sample and environmental conditions), support vector machine-based least-squares nonlinear regression (LS-SVR) has been recently proposed. In this paper, we investigate the robustness of this methodology to noise-induced instabilities and present an analytical formula for estimating modeling precision as a function of measurement noise and model parameters. This formalism can be readily used to evaluate uncertainty in information extracted from spectroscopic measurements, particularly important for rapid-acquisition biomedical applications. Subsequently, using field data (Raman spectra) acquired from a glucose clamping study on an animal model subject, we perform the first systematic investigation of the relative effect of additive interference components (namely, noise in prediction spectra, calibration spectra, and calibration concentrations) on the prediction error of nonlinear spectroscopic models. Our results show that the LS-SVR method gives more accurate results and is substantially more robust to additive noise when compared with conventional regression methods such as partial least-squares regression (PLS), when careful selection of the LS-SVR model parameters are performed. We anticipate that these results will be useful for uncertainty estimation in similar biomedical applications where the precision of measurements and its response to noise in the data set is as important, if not more so, than the generic accuracy level.
RI Soares, Jaqueline S./M-9103-2014; Smulko, Janusz M/P-6814-2014; Singh,
   Gajendra Pratap/A-2470-2011; Smulko, Janusz/Z-3398-2019; Singh, Gajendra
   Pratap/AFG-4966-2022
OI Soares, Jaqueline S./0000-0003-2674-5791; Smulko, Janusz
   M/0000-0003-1459-4199; Singh, Gajendra Pratap/0000-0001-8561-1385;
   Smulko, Janusz/0000-0003-1459-4199; 
SN 0003-2700
EI 1520-6882
PD OCT 2
PY 2012
VL 84
IS 19
BP 8149
EP 8156
DI 10.1021/ac301200n
UT WOS:000309493200012
PM 22950485
ER

PT J
AU Lee, TY
   Lu, CT
   Chen, SA
   Bretana, NA
   Cheng, TH
   Su, MG
   Huang, KY
AF Lee, Tzong-Yi
   Lu, Cheng-Tsung
   Chen, Shu-An
   Bretana, Neil Arvin
   Cheng, Tzu-Hsiu
   Su, Min-Gang
   Huang, Kai-Yao
TI Investigation and identification of protein gamma-glutamyl carboxylation
   sites
SO BMC BIOINFORMATICS
CT 1st ISCB Asia Joint Conference on Computational Biology
   (InCoB/ISCB-Asia)/Asia Pacific Bioinformatics Network (APBioNet) 10th
   International Conference on Bioinformatics
CY NOV 30-DEC 02, 2011
CL Kuala Lumpur, MALAYSIA
SP ISCB, Asia Pacific Bioinformat Network (APBioNet)
AB Background: Carboxylation is a modification of glutamate (Glu) residues which occurs post-translation that is catalyzed by gamma-glutamyl carboxylase in the lumen of the endoplasmic reticulum. Vitamin K is a critical co-factor in the post-translational conversion of Glu residues to gamma-carboxyglutamate (Gla) residues. It has been shown that the process of carboxylation is involved in the blood clotting cascade, bone growth, and extraosseous calcification. However, studies in this field have been limited by the difficulty of experimentally studying substrate site specificity in gamma-glutamyl carboxylation. In silico investigations have the potential for characterizing carboxylated sites before experiments are carried out.
   Results: Because of the importance of gamma-glutamyl carboxylation in biological mechanisms, this study investigates the substrate site specificity in carboxylation sites. It considers not only the composition of amino acids that surround carboxylation sites, but also the structural characteristics of these sites, including secondary structure and solvent-accessible surface area (ASA). The explored features are used to establish a predictive model for differentiating between carboxylation sites and non-carboxylation sites. A support vector machine (SVM) is employed to establish a predictive model with various features. A five-fold cross-validation evaluation reveals that the SVM model, trained with the combined features of positional weighted matrix (PWM), amino acid composition (AAC), and ASA, yields the highest accuracy (0.892). Furthermore, an independent testing set is constructed to evaluate whether the predictive model is over-fitted to the training set.
   Conclusions: Independent testing data that did not undergo the cross-validation process shows that the proposed model can differentiate between carboxylation sites and non-carboxylation sites. This investigation is the first to study carboxylation sites and to develop a system for identifying them. The proposed method is a practical means of preliminary analysis and greatly diminishes the total number of potential carboxylation sites requiring further experimental confirmation.
OI Bretana, Neil Arvin/0000-0003-4743-348X
SN 1471-2105
PD NOV 30
PY 2011
VL 12
SU 13
AR S10
DI 10.1186/1471-2105-12-S13-S10
UT WOS:000303935200010
PM 22372765
ER

PT J
AU Capo, JT
   Hastings, H
   Choung, E
   Kinchelow, T
   Rossy, W
   Steinberg, B
AF Capo, John T.
   Hastings, Hill, II
   Choung, Edward
   Kinchelow, Tosca
   Rossy, William
   Steinberg, Bruce
TI Hemicondylar hamate replacement arthroplasty for proximal
   interphalangeal joint fracture dislocations: An assessment of graft
   suitability
SO JOURNAL OF HAND SURGERY-AMERICAN VOLUME
AB Purpose Proximal interphalangeal (PIP) joint fracture-dislocations are complex injuries, and successful surgical treatment can be challenging. The hamate appears to be an appropriate graft based on its general shape and dimensions. The purpose of this study was to evaluate the rationale and suitability of the hamate as an autograft for proximal interphalangeal joint fracture-dislocations and to determine the inherent stability of the donor site after graft harvesting.
   Methods Fresh-frozen cadaveric hand specimens were used to evaluate the hamate as a suitable graft source for defects of the middle phalanx based on macroscopic, radiographic, and biomechanical properties. Radiographic measurements were made of the articular contours of the hamate and the base of middle phalanx of digits 2 through 5. Hemicondylar hamate replacement arthroplasty (HHRA) was performed in cadavers for defects created in the middle phalanges. Biomechanical stability testing of the hamate-metacarpal joint was then assessed in additional specimens before and after HHRA. Fluoroscopic examination with a 22.2-N load applied in a 45 degrees dorsal-proximal direction was used to assess stability of the carpometacarpal joints. A servohydraulic testing machine was then used to determine the amount of translation induced with a similarly directed force before and after harvesting of the hamate graft.
   Results The cadaveric HHRA reconstructions restored joint stability with no tendency to subluxate. Radiographic measurement showed that the hamate has a central ridge and bicondylar facet with articular contours that are similar to the base of the middle phalanx. The removal of a central portion of the hamate did not induce dislocation or create obvious clinical instability of the carpometacarpal joint.
   Conclusions The HHRA technique is used for treatment of fracture-dislocations of the proximal interphalangeal joint. This study demonstrated the suitability of using the dorsal portion of the hamate as an osteochondral autograft for middle phalangeal base fractures; the technique creates minimal donor site morbidity.
SN 0363-5023
PD MAY-JUN
PY 2008
VL 33A
IS 5
BP 733
EP 739
DI 10.1016/j.jhsa.2008.01.012
UT WOS:000257542200014
PM 18590857
ER

PT C
AU Re, C
AF Re, Christopher
GP ACM
TI Software 2.0 and Snorkel: Beyond Hand-Labeled Data
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
SP Assoc Comp Machinery, Assoc Comp Machinery SIGKDD, Assoc Comp Machinery SIGMOD
AB In the last few years, deep learning models have simultaneously achieved high quality on conventionally challenging tasks and become easy-to-use commodity tools. These factors, combined with the ease of deployment compared to traditional software, have led to deep learning models replacing production software stacks in not only traditional machine learning-driven products including translation and search, but also in many previously heuristic-based applications. This new mode of software construction and deployment has been called Software 2.0 [2].
   A key bottleneck in the construction of Software 2.0 applications is the need for large, high-quality training sets for each task. This talk describes Snorkel, a system that enables users to help shape, create, and manage training data for Software 2.0 stacks. In Snorkel applications, instead of tediously hand-labeling individual data items, a user implicitly defines large training sets by writing programs, called labeling functions, that assign labels to subsets of data points, albeit noisily. This idea of using multiple, imperfect sources of labels builds on work in distant supervision. However, if ignored, the uneven (and unknown) accuracies and coverages of the user-provided labeling functions can easily lead to suboptimal results:
   Example. Suppose we have two training sets, T-1 and T-2, which are produced by two processes (or labeling functions). T-1 has high accuracy say 90% but low yield, labeling 10k points while T-2 has lower accuracy, 60%, but higher yield, 1M points. If we put the training sets together, we have a set T of 1.01M points with overall accuracy 60.3%. This could be distressing for a user: a model trained on T-1 seems to lose quality when trained on all of T. Naively combining the training sets fails to account for the different origins of T-1 and T-2.
   Snorkel addresses this challenge of uneven training source quality by automatically learning a statistical model of the labeling functions' accuracies and correlation structure. The lack of hand-labeled data when learning this model raises several statistical challenges including estimating accuracies, learning correlations, and selecting features that refine labeling function quality [1,3,4]. Snorkel then uses this model to combine and reweight the labeling functions' labels, producing a set of probabilistic training labels, thus effectively passing along provenance information about the training. Our experimental results and theory show that estimating and accounting for the quality of the labeling functions in this way can lead to improved training set labels and boost downstream application quality-potentially by large margins, e.g., more than ten points of F1 score in NLP applications.
   Exploiting the varied quality of supervision is a key building block to help manage the software 2.0 stack-but it's far from the only technique. Indeed, recent extensions of these core themes have led to projects automatically generating data augmentations, synthesizing labeling functions, and programmatically defining multi-task supervision. This does not even touch the many new opportunities for deployment and systems in Software 2.0. Hence, we contend there is a broad research motivated by Software 2.0.
   Although only two years old, the Snorkel project powers applications in major tech companies and scientific efforts. It is used in applications in traditional machine learning applications like natural language processing, medical imaging, and prediction. Perhaps more excitingly for the Software 2.0 vision, it's also used in traditional enterprise applications like data cleaning, data integration, and semi-structured extraction-areas that have traditionally been difficult to deploy machine learning for.
   For more information about the formal underpinnings and applications of Snorkel, we refer to Snorkel. stanford.edu for open source code, tutorial, and links to technical papers.
BN 978-1-4503-5552-0
PY 2018
BP 2876
EP 2876
DI 10.1145/3219819.3219937
UT WOS:000455346400302
ER

PT J
AU Sun, YY
   Gao, DP
   Shen, XF
   Li, MT
   Nan, JL
   Zhang, WN
AF Sun, Yuanyuan
   Gao, Dongping
   Shen, Xifeng
   Li, Meiting
   Nan, Jiale
   Zhang, Weining
TI Multi-Label Classification in Patient-Doctor Dialogues With the
   RoBERTa-WWM-ext plus CNN (Robustly Optimized Bidirectional Encoder
   Representations From Transformers Pretraining Approach With Whole Word
   Masking Extended Combining a Convolutional Neural Network) Model: Named
   Entity Study
SO JMIR MEDICAL INFORMATICS
AB Background: With the prevalence of online consultation, many patient-doctor dialogues have accumulated, which, in an authentic language environment, are of significant value to the research and development of intelligent question answering and automated triage in recent natural language processing studies.
   Objective: The purpose of this study was to design a front-end task module for the network inquiry of intelligent medical services. Through the study of automatic labeling of real doctor-patient dialogue text on the internet, a method of identifying the negative and positive entities of dialogues with higher accuracy has been explored.
   Methods: The data set used for this study was from the Spring Rain Doctor internet online consultation, which was downloaded from the official data set of Alibaba Tianchi Lab. We proposed a composite abutting joint model, which was able to automatically classify the types of clinical finding entities into the following 4 attributes: positive, negative, other, and empty. We adapted a downstream architecture in Chinese Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach (RoBERTa) with whole word masking (WWM) extended (RoBERTa-WWM-ext) combining a text convolutional neural network (CNN). We used RoBERTa-WWM-ext to express sentence semantics as a text vector and then extracted the local features of the sentence through the CNN, which was our new fusion model. To verify its knowledge learning ability, we chose Enhanced Representation through Knowledge Integration (ERNIE), original Bidirectional Encoder Representations from Transformers (BERT), and Chinese BERT with WWM to perform the same task, and then compared the results. Precision, recall, and macro-F1 were used to evaluate the performance of the methods.
   Results: We found that the ERNIE model, which was trained with a large Chinese corpus, had a total score (macro-F1) of 65.78290014, while BERT and BERT-WWM had scores of 53.18247117 and 69.2795315, respectively. Our composite abutting joint model (RoBERTa-WWM-ext + CNN) had a macro-F1 value of 70.55936311, showing that our model outperformed the other models in the task.
   Conclusions: The accuracy of the original model can be greatly improved by giving priority to WWM and replacing the word-based mask with unit to classify and label medical entities. Better results can be obtained by effectively optimizing the downstream tasks of the model and the integration of multiple models later on. The study findings contribute to the translation of online consultation information into machine-readable information.
OI Shen, Xifeng/0000-0003-3446-6741; Zhang, Weining/0000-0003-2326-9400;
   Sun, Yuanyuan/0000-0001-7894-865X
EI 2291-9694
PD APR
PY 2022
VL 10
IS 4
BP 142
EP 152
AR e35606
DI 10.2196/35606
UT WOS:000832007700011
PM 35451969
ER

PT J
AU Revilla-Leon, M
   Att, W
   Ozcan, M
   Rubenstein, J
AF Revilla-Leon, Marta
   Att, Wael
   Ozcan, Mutlu
   Rubenstein, Jeffrey
TI Comparison of conventional, photogrammetry, and intraoral scanning
   accuracy of complete-arch implant impression procedures evaluated with a
   coordinate measuring machine
SO JOURNAL OF PROSTHETIC DENTISTRY
AB Statement of problem. Conventional implant impressions by using elastomeric impression material have been reported as a more reliable technique for a complete-arch implant record compared with intraoral scanner procedures. Photogrammetry technology may provide a reliable alternative to digital scanning or a conventional impression; however, its accuracy remains unclear.
   Purpose. The purpose of this in vitro study was to measure and compare the implant abutment replica positions of the definitive cast with the implant abutment replica positions obtained by the conventional technique, photogrammetry, and 2 intraoral scanners.
   Material and methods. An edentulous maxillary cast with 6 implant abutment replicas (RC analog for screw-retained abutment straight) was prepared. Three impression techniques were performed: the conventional impression technique (CNV group) by using a custom tray elastomeric impression procedure after splinting the impression copings at room temperature (23 degrees C), photogrammetry (PG group) technology (Icam4D), digital scans by using 2 different IOSs following the manufacturer's recommended scanning protocol, namely IOS-1 (iTero Element) and IOS-2 (TRIOS 3) groups (n=10). A coordinate measuring machine (CMM Contura G2 10/16/06 RDS) was used to measure the implant abutment replica positions of the definitive casts and to compare the linear discrepancies at the x-, y-, and z-axes and the angular distortion of each implant abutment replica position by using a computer aided-design software program (Geomagic) and the best fit technique. The 3D linear gap discrepancy was calculated. Measurements were repeated 3 times. The Shapiro-Wilk test revealed that the data were not normally distributed; therefore, the Kruskal-Wallis test was used to analyze the data, followed by pairwise Mann-Whitney U tests (alpha=.05).
   Results. Significant y-axis linear and XY and YZ angular discrepancies were found among the CNV, PG, IOS-1, and IOS-2 groups (P<.05). The PG group obtained a significantly higher distortion on the y-axis and 3D gap compared with all the remaining groups (P=.004). The 3D discrepancy of the CNV group was 11.7 mu m, of the IOS-1 group was 18.4 mu m, of the IOS-2 was 21.1 mu m, and of the PG group was 77.6 mu m. In all groups, the interquartile range was higher than the median errors from the discrepancies measured from the definitive cast, indicating that the relative precision was low.
   Conclusions. The conventional technique reported the lowest 3D discrepancy for the implant abutment position translation capabilities of all the implant techniques evaluated. The intraoral scanners tested provided no significant differences in linear distortion compared with the conventional method. However, the photogrammetry system tested provided the least accurate values, with the highest 3D discrepancy for the implant abutment positions among all the groups.
RI Revilla-León, Marta/AAH-2117-2019; Özcan, Mutlu/B-2862-2013
OI Revilla-León, Marta/0000-0003-2854-1135; Özcan,
   Mutlu/0000-0002-9623-6098
SN 0022-3913
EI 1097-6841
PD MAR
PY 2021
VL 125
IS 3
BP 470
EP 478
DI 10.1016/j.prosdent.2020.03.005
EA MAR 2021
UT WOS:000631831400016
PM 32386912
ER

PT J
AU Macleod, CN
   Dobie, G
   Pierce, SG
   Summan, R
   Morozov, M
AF Macleod, Charles Norman
   Dobie, Gordon
   Pierce, Stephen Gareth
   Summan, Rahul
   Morozov, Maxim
TI Machining-Based Coverage Path Planning for Automated Structural
   Inspection
SO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
AB The automation of robotically delivered nondestructive evaluation inspection shares many aims with traditional manufacture machining. This paper presents a new hardware and software system for automated thickness mapping of large-scale areas, with multiple obstacles, by employing computer-aided drawing (CAD)/computer-aided manufacturing (CAM)-inspired path planning to implement control of a novel mobile robotic thickness mapping inspection vehicle. A custom postprocessor provides the necessary translation from CAM numeric code through robotic kinematic control to combine and automate the overall process. The generalized steps to implement this approach for any mobile robotic platform are presented herein and applied, in this instance, to a novel thickness mapping crawler. The inspection capabilities of the system were evaluated on an indoor mock-inspection scenario, within a motion tracking cell, to provide quantitative performance figures for positional accuracy. Multiple thickness defects simulating corrosion features on a steel sample plate were combined with obstacles to be avoided during the inspection. A minimum thickness mapping error of 0.21 mm and a mean path error of 4.41 mm were observed for a 2 m(2) carbon steel sample of 10-mm nominal thickness. The potential of this automated approach has benefits in terms of repeatability of area coverage, obstacle avoidance, and reduced path overlap, all of which directly lead to increased task efficiency and reduced inspection time of large structural assets.
   Note to Practitioners-Current industrial robotic inspection approaches largely consist of a manual control of robotic platform motion to desired points, with the aim of producing a number of straight scans for larger areas, often spaced meters apart. The structures featuring large surface area and multiple obstacles are routinely inspected with such manual approaches, which are both labor intensive and error prone, and do not guarantee acquisition of full area coverage. The presented system addresses these limitations through a combined hardware and software approach. Core to the operation of the system is a fully wireless, differential drive crawler with integrated active ultrasonic wheel probe, to provide remote thickness mapping. Automation of the path generation algorithms is produced using the commercial CAD/CAM software algorithms, and this paper sets out an adaptable methodology for producing a custom postprocessor to convert the exported G-codes to suitable kinematic commands for mobile robotic platforms. The differential drive crawler is used in this paper to demonstrate the process. This approach has benefits in terms of improved industrial standardization and operational repeatability. The inspection capabilities of the system were documented on an indoor mock-inspection scenario, within a motion tracking cell to provide quantitative performance figures for the approach. Future work is required to integrate the onboard positioning strategies, removing the dependence on global systems, for full automated deployment capability.
RI Pierce, Stephen/AAM-7176-2020; Dobie, Gordon/AAC-5702-2020; Macleod,
   Charles/I-6125-2017
OI Dobie, Gordon/0000-0003-3972-5917; Pierce, Stephen/0000-0003-0312-8766;
   Macleod, Charles/0000-0003-4364-9769
SN 1545-5955
EI 1558-3783
PD JAN
PY 2018
VL 15
IS 1
BP 202
EP 213
DI 10.1109/TASE.2016.2601880
UT WOS:000419498100016
ER

PT J
AU Olberg, S
   Zhang, H
   Kennedy, WR
   Chun, J
   Rodriguez, V
   Zoberi, I
   Thomas, MA
   Kim, JS
   Mutic, S
   Green, OL
   Park, JC
AF Olberg, Sven
   Zhang, Hao
   Kennedy, William R.
   Chun, Jaehee
   Rodriguez, Vivian
   Zoberi, Imran
   Thomas, Maria A.
   Kim, Jin Sung
   Mutic, Sasa
   Green, Olga L.
   Park, Justin C.
TI Synthetic CT reconstruction using a deep spatial pyramid convolutional
   framework for MR-only breast radiotherapy
SO MEDICAL PHYSICS
AB Purpose The superior soft-tissue contrast achieved using magnetic resonance imaging (MRI) compared to x-ray computed tomography (CT) has led to the popularization of MRI-guided radiation therapy (MR-IGRT), especially in recent years with the advent of first and second generation MRI-based therapy delivery systems for MR-IGRT. The expanding use of these systems is driving interest in MRI-only RT workflows in which MRI is the sole imaging modality used for treatment planning and dose calculations. To enable such a workflow, synthetic CT (sCT) data must be generated based on a patient's MRI data so that dose calculations may be performed using the electron density information derived from CT images. In this study, we propose a novel deep spatial pyramid convolutional framework for the MRI-to-CT image-to-image translation task and compare its performance to the well established U-Net architecture in a generative adversarial network (GAN) framework. Methods Our proposed framework utilizes atrous convolution in a method named atrous spatial pyramid pooling (ASPP) to significantly reduce the total number of parameters required to describe the model while effectively capturing rich, multi-scale structural information in a manner that is not possible in the conventional framework. The proposed framework consists of a generative model composed of stacked encoders and decoders separated by the ASPP module, where atrous convolution is applied at increasing rates in parallel to encode large-scale features. The performance of the proposed method is compared to that of the conventional GAN framework in terms of the time required to train the model and the image quality of the generated sCT as measured by the root mean square error (RMSE), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR) depending on the size of the training data set. Dose calculations based on sCT data generated using the proposed architecture are also compared to clinical plans to evaluate the dosimetric accuracy of the method. Results Significant reductions in training time and improvements in image quality are observed at every training data set size when the proposed framework is adopted instead of the conventional framework. Over 1042 test images, values of 17.7 +/- 4.3 HU, 0.9995 +/- 0.0003, and 71.7 +/- 2.3 are observed for the RMSE, SSIM, and PSNR metrics, respectively. Dose distributions calculated based on sCT data generated using the proposed framework demonstrate passing rates equal to or greater than 98% using the 3D gamma index with a 2%/2 mm criterion. Conclusions The deep spatial pyramid convolutional framework proposed here demonstrates improved performance compared to the conventional GAN framework that has been applied to the image-to-image translation task of sCT generation. Adopting the method is a first step toward an MRI-only RT workflow that enables widespread clinical applications for MR-IGRT including online adaptive therapy.
OI Kennedy, William/0000-0001-8583-1345; Kim, Jin Sung/0000-0003-1415-6471
SN 0094-2405
EI 2473-4209
PD SEP
PY 2019
VL 46
IS 9
BP 4135
EP 4147
DI 10.1002/mp.13716
EA AUG 2019
UT WOS:000480178100001
PM 31309586
ER

PT J
AU Khushaba, RN
   Scheme, E
   Al-Timemy, AH
   Phinyomark, A
   Al-Taee, A
   Al-Jumaily, A
AF Khushaba, Rami N.
   Scheme, Erik
   Al-Timemy, Ali H.
   Phinyomark, Angkoon
   Al-Taee, Ahmed
   Al-Jumaily, Adel
TI A long short-term recurrent spatial-temporal fusion for myoelectric
   pattern recognition
SO EXPERT SYSTEMS WITH APPLICATIONS
AB Current state-of-the-art myoelectric interfaces employ traditional pattern recognition (PR) algorithms to decode the Electromyogram (EMG) signals into hand movements for controlling artificial limbs. Recently, deep learning (DL) models have also been exploited for EMG feature learning/extraction. Models like Convolutional Neural Networks (CNN), which capture the spatial correlations, and Long Short-Term Memory (LSTM), which capture the non-linear temporal dynamics of EMG time-series data, have been shown to outperform traditional EMG PR systems. Nevertheless, the large number of model parameters, long training times, and large amounts of data required to train these DL models remain limiting factors that may hinder their translation into clinically viable prostheses. Consequently, rather than applying DL directly, this paper leverages concepts derived from these models to build upon our proposed concept of a Fusion of Time Domain Descriptors (FTDD). The FTDD are augmented with Range Spatial Filtering (RSF) to capture the spatial correlations and combined into an LSTMstyle framework. This process, denoted as Recurrent Spatial-Temporal Fusion (RSTF), can be applied in combination with any traditional feature extraction method to exploit temporal and spatial correlations, with the potential for bi-directional applications. The advantages of the proposed RSTF method include (1) the memory concept, capturing long-and short-term spatial and temporal dependencies of the EMG signals, (2) significantly improved performance outperforming other state-of-the-art models and (3) the simplicity and the fairly low computational costs for feature extraction. Results are bench-marked against several feature extraction methods, proving the power of the RSTF using data from 82 subjects from five EMG databases with varying recording characteristics. The proposed method significantly outperforms all other methods tested for EMG pattern recognition, including a deep LSTM and other CNN methods previously reported in the literature and at a fracture of the computational cost. On the most challenging dataset, improvements of as much as 15% were found.
RI Al-Timemy, Ali H/A-6974-2011; Khushaba, Rami/O-1038-2015
OI Khushaba, Rami/0000-0001-8528-8979; H. Al-Timemy,
   Ali/0000-0003-2738-8896
SN 0957-4174
EI 1873-6793
PD SEP 15
PY 2021
VL 178
AR 114977
DI 10.1016/j.eswa.2021.114977
EA APR 2021
UT WOS:000696800700013
ER

PT J
AU Pratte, D
   Singh, U
   Murat, G
   Kressler, D
AF Pratte, Dagmar
   Singh, Ujjwala
   Murat, Guillaume
   Kressler, Dieter
TI Mak5 and Ebp2 Act Together on Early Pre-60S Particles and Their Reduced
   Functionality Bypasses the Requirement for the Essential Pre-60S Factor
   Nsa1
SO PLOS ONE
AB Ribosomes are the molecular machines that translate mRNAs into proteins. The synthesis of ribosomes is therefore a fundamental cellular process and consists in the ordered assembly of 79 ribosomal proteins (r-proteins) and four ribosomal RNAs (rRNAs) into a small 40S and a large 60S ribosomal subunit that form the translating 80S ribosomes. Most of our knowledge concerning this dynamic multi-step process comes from studies with the yeast Saccharomyces cerevisiae, which have shown that assembly and maturation of pre-ribosomal particles, as they travel from the nucleolus to the cytoplasm, relies on a multitude (>200) of biogenesis factors. Amongst these are many energy-consuming enzymes, including 19 ATP-dependent RNA helicases and three AAA-ATPases. We have previously shown that the AAA-ATPase Rix7 promotes the release of the essential biogenesis factor Nsa1 from late nucleolar pre-60S particles. Here we show that mutant alleles of genes encoding the DEAD-box RNA helicase Mak5, the C/D-box snoRNP component Nop1 and the rRNA-binding protein Nop4 bypass the requirement for Nsa1. Interestingly, dominant-negative alleles of RIX7 retain their phenotype in the absence of Nsa1, suggesting that Rix7 may have additional nuclear substrates besides Nsa1. Mak5 is associated with the Nsa1 pre-60S particle and synthetic lethal screens with mak5 alleles identified the r-protein Rpl14 and the 60S biogenesis factors Ebp2, Nop16 and Rpf1, which are genetically linked amongst each other. We propose that these 'Mak5 cluster' factors orchestrate the structural arrangement of a eukaryote-specific 60S subunit surface composed of Rpl6, Rpl14 and Rpl16 and rRNA expansion segments ES7L and ES39L. Finally, over-expression of Rix7 negatively affects growth of mak5 and ebp2 mutant cells both in the absence and presence of Nsa1, suggesting that Rix7, at least when excessively abundant, may act on structurally defective pre-60S subunits and may subject these to degradation.
RI Kressler, Dieter/C-2218-2012
OI Kressler, Dieter/0000-0003-4855-3563; Murat,
   Guillaume/0000-0003-1288-984X
SN 1932-6203
PD DEC 2
PY 2013
VL 8
IS 12
AR e82741
DI 10.1371/journal.pone.0082741
UT WOS:000327944500144
PM 24312670
ER

PT J
AU Lata, K
   Singh, P
   Dutta, K
AF Lata, Kusum
   Singh, Pardeep
   Dutta, Kamlesh
TI A comprehensive review on feature set used for anaphora resolution
SO ARTIFICIAL INTELLIGENCE REVIEW
AB In linguistics, the Anaphora Resolution (AR) is the method of identifying the antecedent for anaphora. In simple terms, this is the problem that helps to solve what the expression referring to a referent refers to. It is considered to be one of the tedious tasks in Natural Language Processing (NLP). AR's burgeoning popularity among researchers is attributable to its strong relevance to machine translation, text summarization, chatbot, question answering, and many others. This paper presents a review of AR approaches based on significant features utilized to perform this task and presents the evaluation metrics for this field. The feature is a relevant term related to AR that provides vital information regarding anaphor, antecedent, and relation between them. In this context, features represent the lexical, syntactical, semantical, and positional relationship between anaphor and its possible candidate antecedent. The performance of the Anaphora resolution system is profoundly dependent on the features used in the AR system. Hence, the selection of features for the AR system is highly significant. The main emphasis is to provide an overview of the various features needed to extract both the Anaphora and the Antecedent, respectively, used in different AR systems, present in literature. It is observed that syntactical information enhances the correctness of determining the properties for the existence of an anaphor and antecedent identification. Nowadays the trend is changing from hand-crafted feature dependent methods to deep learning approaches which try to learn feature representation. The performance of deep learning is progressing due to the accessibility of additional data and more powerful computing resources. This survey will provide the state-of art for the better understanding of solving AR problem from the feature selection perspective. The findings of this survey are useful to provide valuable insight into present trends and are helpful for researchers who are looking for developing AR system within given constraints.
RI Dutta, Kamlesh/ABD-3968-2021; Singh, Pardeep/AAZ-9884-2021; Dutta,
   Kamlesh/C-1633-2012
OI Singh, Pardeep/0000-0002-4019-604X; 
SN 0269-2821
EI 1573-7462
PD APR
PY 2021
VL 54
IS 4
BP 2917
EP 3006
DI 10.1007/s10462-020-09917-3
EA OCT 2020
UT WOS:000577226200001
ER

PT J
AU Wang, R
   Cai, YX
   Zhang, BP
   Wu, ZX
AF Wang, Rui
   Cai, Yuxing
   Zhang, Baoping
   Wu, Zhengxia
TI A 16-gene expression signature to distinguish stage I from stage II lung
   squamous carcinoma
SO INTERNATIONAL JOURNAL OF MOLECULAR MEDICINE
AB The present study aimed to perform screening of a gene signature for the discrimination and prognostic prediction of stage I and II lung squamous carcinoma. A microarray meta-analysis was performed to identify differentially expressed genes (DEGs) between stage I and II lung squamous carcinoma samples in seven microarray datasets collected from the Gene Expression Omnibus database via the MetaQC and MetaDE package in R. The important DEGs were selected according to the betweenness centrality value of the protein-protein interaction (PPI) network. Support vector machine (SVM) analysis was performed to screen the feature genes for discrimination and prognosis. One independent dataset downloaded from The Cancer Genome Atlas was used to validate the reliability. Pathway enrichment analysis was also performed for the feature genes. A total of 924 D EGs were identified to construct a PPI network consisting of 392 nodes and 686 edges. The top 100 of the 392 nodes were selected as crucial genes to construct an SVM classifier, and a 16-gene signature (caveolin 1, eukaryotic translation elongation factor 1 gamma, casein kinase 2 alpha 1, tyrosine 3-monooxygenase/tryptophan 5-monooxygenase activation gamma, tyrosine 3-monooxygenase/tryptophan 5-monooxygenase activation gamma, pleiotrophin, insulin receptor, insulin receptor substrate 1, 3-phosphoinositide-dependent protein kinase-1, specificity protein 1, COP9 signalosome subunit 6, N-myc downstream regulated gene 1, retinoid X receptor alpha, heat shock protein 90 alpha A1, karyopherin subunit beta 1 and erythrocyte membrane protein band 4.1) with high discrimination accuracy was identified. This 16-gene signature had significant prognostic value, and patients with stage II lung squamous carcinoma exhibited shorter survival rates, compared with those with stage I disease. Seven DEGs of the 16-gene signature were significantly involved in the phosphoinositide 3-kinase-Akt signaling pathway. The 16-gene signature identified in the present study may be useful for stratifying the patients with stage I or II lung squamous carcinoma and predicting prognosis.
SN 1107-3756
EI 1791-244X
PD MAR
PY 2018
VL 41
IS 3
BP 1377
EP 1384
DI 10.3892/ijmm.2017.3332
UT WOS:000424918000021
PM 29286069
ER

PT J
AU Daemen, A
   Gevaert, O
   Ojeda, F
   Debucquoy, A
   Suykens, JAK
   Sempoux, C
   Machiels, JP
   Haustermans, K
   De Moor, B
AF Daemen, Anneleen
   Gevaert, Olivier
   Ojeda, Fabian
   Debucquoy, Annelies
   Suykens, Johan A. K.
   Sempoux, Christine
   Machiels, Jean-Pascal
   Haustermans, Karin
   De Moor, Bart
TI A kernel-based integration of genome-wide data for clinical decision
   support
SO GENOME MEDICINE
AB Background: Although microarray technology allows the investigation of the transcriptomic make-up of a tumor in one experiment, the transcriptome does not completely reflect the underlying biology due to alternative splicing, post-translational modifications, as well as the influence of pathological conditions (for example, cancer) on transcription and translation. This increases the importance of fusing more than one source of genome-wide data, such as the genome, transcriptome, proteome, and epigenome. The current increase in the amount of available omics data emphasizes the need for a methodological integration framework.
   Methods: We propose a kernel-based approach for clinical decision support in which many genome-wide data sources are combined. Integration occurs within the patient domain at the level of kernel matrices before building the classifier. As supervised classification algorithm, a weighted least squares support vector machine is used. We apply this framework to two cancer cases, namely, a rectal cancer data set containing microarray and proteomics data and a prostate cancer data set containing microarray and genomics data. For both cases, multiple outcomes are predicted.
   Results: For the rectal cancer outcomes, the highest leave-one-out (LOO) areas under the receiver operating characteristic curves (AUC) were obtained when combining microarray and proteomics data gathered during therapy and ranged from 0.927 to 0.987. For prostate cancer, all four outcomes had a better LOO AUC when combining microarray and genomics data, ranging from 0.786 for recurrence to 0.987 for metastasis.
   Conclusions: For both cancer sites the prediction of all outcomes improved when more than one genome-wide data set was considered. This suggests that integrating multiple genome-wide data sources increases the predictive performance of clinical decision support models. This emphasizes the need for comprehensive multi-modal data. We acknowledge that, in a first phase, this will substantially increase costs; however, this is a necessary investment to ultimately obtain cost-efficient models usable in patient tailored therapy.
RI Gevaert, Olivier/H-3413-2019; Suykens, Johan A.K./C-9781-2014;
   Haustermans, Karin/AAB-5912-2021
OI Gevaert, Olivier/0000-0002-9965-5466; Suykens, Johan
   A.K./0000-0002-8846-6352; Haustermans, Karin/0000-0003-0364-682X;
   Sempoux, Christine/0000-0003-1375-3979
SN 1756-994X
PY 2009
VL 1
AR 39
DI 10.1186/gm39
UT WOS:000208627000039
PM 19356222
ER

PT J
AU Paranavithana, IR
   Stirling, D
   Ros, M
   Field, M
AF Paranavithana, Iromi R.
   Stirling, David
   Ros, Montserrat
   Field, Matthew
TI Systematic Review of Tumor Segmentation Strategies for Bone Metastases
SO CANCERS
AB Simple Summary With recent progress in radiation therapy, patients with bone metastases can be treated curatively, provided precise delineation of metastatic lesions is adequately identified. Tumor segmentation is a highly active area of research, but only limited studies have been on bone metastasis. This review aims to investigate methods for differentiating benign from malignant bone lesions and characterizing malignant bone lesions specifically in the context of bone metastases. While computer vision techniques have opened new opportunities for quantifying cancer growth with minimal expert supervision, fully automatic segmentation algorithms still require improvement. This is partly due to limited contrast between tumors and surrounding tissue and the lack of a widely agreed upon "gold standard" for defining these boundaries. Additionally, many studies do not provide evidence that their proposed methods are suitable for use in clinical practice. Purpose: To investigate the segmentation approaches for bone metastases in differentiating benign from malignant bone lesions and characterizing malignant bone lesions. Method: The literature search was conducted in Scopus, PubMed, IEEE and MedLine, and Web of Science electronic databases following the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). A total of 77 original articles, 24 review articles, and 1 comparison paper published between January 2010 and March 2022 were included in the review. Results: The results showed that most studies used neural network-based approaches (58.44%) and CT-based imaging (50.65%) out of 77 original articles. However, the review highlights the lack of a gold standard for tumor boundaries and the need for manual correction of the segmentation output, which largely explains the absence of clinical translation studies. Moreover, only 19 studies (24.67%) specifically mentioned the feasibility of their proposed methods for use in clinical practice. Conclusion: Development of tumor segmentation techniques that combine anatomical information and metabolic activities is encouraging despite not having an optimal tumor segmentation method for all applications or can compensate for all the difficulties built into data limitations.
EI 2072-6694
PD MAR
PY 2023
VL 15
IS 6
AR 1750
DI 10.3390/cancers15061750
UT WOS:000957384200001
PM 36980636
ER

PT C
AU Bauereiss, T
   Campbell, B
   Sewell, T
   Armstrong, A
   Esswood, L
   Stark, I
   Barnes, G
   Watson, RNM
   Sewell, P
AF Bauereiss, Thomas
   Campbell, Brian
   Sewell, Thomas
   Armstrong, Alasdair
   Esswood, Lawrence
   Stark, Ian
   Barnes, Graeme
   Watson, Robert N. M.
   Sewell, Peter
BE Sergey, I
TI Verified Security for the Morello Capability-enhanced Prototype Arm
   Architecture
SO PROGRAMMING LANGUAGES AND SYSTEMS, ESOP 2022
SE Lecture Notes in Computer Science
CT 31st European Symposium on Programming (ESOP) Held as Part of the 25th
   European Joint Conferences on Theory and Practice of Software (ETAPS)
CY APR 02-07, 2022
CL Munich, GERMANY
SP Tech Univ Munich, LMU Munich
AB Memory safety bugs continue to be a major source of security vulnerabilities in our critical infrastructure. The CHERI project has proposed extending conventional architectures with hardware-supported capabilities to enable fine-grained memory protection and scalable compartmentalisation, allowing historically memory-unsafe C and C++ to be adapted to deterministically mitigate large classes of vulnerabilities, while requiring only minor changes to existing system software sources. Arm is currently designing and building Morello, a CHERI-enabled prototype architecture, processor, SoC, and board, extending the high-performance Neoverse N1, to enable industrial evaluation of CHERI and pave the way for potential mass-market adoption. However, for such a major new security-oriented architecture feature, it is important to establish high confidence that it does provide the intended protections, and that cannot be done with conventional engineering techniques.
   In this paper we put the Morello architecture on a solid mathematical footing from the outset. We define the fundamental security property that Morello aims to provide, reachable capability monotonicity, and prove that the architecture definition satisfies it. This proof is mechanised in Isabelle/HOL, and applies to a translation of the official Arm specification of the Morello instruction-set architecture (ISA) into Isabelle. The main challenge is handling the complexity and scale of a production architecture: 62,000 lines of specification, translated to 210,000 lines of Isabelle. We do so by factoring the proof via a narrow abstraction capturing essential properties of arbitrary CHERI ISAs, expressed above a monadic intra-instruction semantics. We also develop a model-based test generator, which generates instruction-sequence tests that give good specification coverage, used in early testing of the Morello implementation and in Morello QEMU development, and we use Arm's internal test suite to validate our model.
   This gives us machine-checked mathematical proofs of whole-ISA security properties of a full-scale industry architecture, at design-time. To the best of our knowledge, this is the first demonstration that that is feasible, and it significantly increases confidence in Morello.
OI Campbell, Brian/0000-0001-6941-5034
SN 0302-9743
EI 1611-3349
BN 978-3-030-99336-8; 978-3-030-99335-1
PY 2022
VL 13240
BP 174
EP 203
DI 10.1007/978-3-030-99336-8_7
UT WOS:000783774400007
ER

PT J
AU Chandio, AA
   Asikuzzamana, M
   Pickering, M
   Leghari, M
AF Chandio, Asghar Ali
   Asikuzzamana, Md.
   Pickering, Mark
   Leghari, Mehwish
TI Cursive-Text: A Comprehensive Dataset for End-to-End Urdu Text
   Recognition in Natural Scene Images
SO DATA IN BRIEF
AB Reading text in natural scene images is an active research area in the fields of computer vision and pattern recognition as text detection, text recognition and script identification are required. In this data article, a comprehensive dataset for Urdu text detection and recognition in natural scene images is presented and analysed. To develop the dataset, more than 2500 natural scene images were captured using a digital camera and a built-in mobile phone camera. Three separate datasets for isolated Urdu character images, cropped word images and end-to-end text spotting were developed. The isolated Urdu character and cropped word images dataset contain a much larger number of samples than existing Arabic natural scene text datasets. The Urdu text spotting dataset contains images with Urdu, English and Sindhi text instances. However, the focus has been given to the Urdu text instances. The ground truths for each image in the isolated character, cropped word or text spotting datasets are provided separately. The proposed datasets can be used to perform Urdu text detection and recognition or end-to-end recognition in natural scenes. These datasets can also be helpful to develop Arabic and Persian natural scene text detection and recognition systems, as Urdu is a derived language of these scripts and has many similar letters. The datasets can also be helpful to develop multi-language translation systems, which can facilitate foreign tourists to read and translate multilingual text in natural scene images. To evaluate the datasets, state-of-the-art machine learning and deep neural networks were used to build the text detection and recognition models, where the best classification accura-cies are achieved. To the best of the authors' knowledge, this is the first dataset proposed for Urdu text detection, recognition or end-to-end text recognition in natural scene images. The aim of this data article is to present a benchmark work in the field of document analysis and recognition. (C) 2020 The Authors. Published by Elsevier Inc.
SN 2352-3409
PD AUG
PY 2020
VL 31
AR 105749
DI 10.1016/j.dib.2020.105749
UT WOS:000569217100004
PM 32490098
ER

PT J
AU de la Iglesia, D
   Garcia-Remesal, M
   Anguita, A
   Munoz-Marmol, M
   Kulikowski, C
   Maojo, V
AF de la Iglesia, Diana
   Garcia-Remesal, Miguel
   Anguita, Alberto
   Munoz-Marmol, Miguel
   Kulikowski, Casimir
   Maojo, Victor
TI A Machine Learning Approach to Identify Clinical Trials Involving
   Nanodrugs and Nanodevices from ClinicalTrials.gov
SO PLOS ONE
AB Background: Clinical Trials (CTs) are essential for bridging the gap between experimental research on new drugs and their clinical application. Just like CTs for traditional drugs and biologics have helped accelerate the translation of biomedical findings into medical practice, CTs for nanodrugs and nanodevices could advance novel nanomaterials as agents for diagnosis and therapy. Although there is publicly available information about nanomedicine-related CTs, the online archiving of this information is carried out without adhering to criteria that discriminate between studies involving nanomaterials or nanotechnology-based processes (nano), and CTs that do not involve nanotechnology (non-nano). Finding out whether nanodrugs and nanodevices were involved in a study from CT summaries alone is a challenging task. At the time of writing, CTs archived in the well-known online registry ClinicalTrials.gov are not easily told apart as to whether they are nano or non-nano CTs-even when performed by domain experts, due to the lack of both a common definition for nanotechnology and of standards for reporting nanomedical experiments and results.
   Methods: We propose a supervised learning approach for classifying CT summaries from ClinicalTrials.gov according to whether they fall into the nano or the non-nano categories. Our method involves several stages: i) extraction and manual annotation of CTs as nano vs. non-nano, ii) pre-processing and automatic classification, and iii) performance evaluation using several state-of-the-art classifiers under different transformations of the original dataset.
   Results and Conclusions: The performance of the best automated classifier closely matches that of experts (AUC over 0.95), suggesting that it is feasible to automatically detect the presence of nanotechnology products in CT summaries with a high degree of accuracy. This can significantly speed up the process of finding whether reports on ClinicalTrials.gov might be relevant to a particular nanoparticle or nanodevice, which is essential to discover any precedents for nanotoxicity events or advantages for targeted drug therapy.
RI Garcia-Remesal, Miguel/ABG-6470-2020
SN 1932-6203
PD OCT 27
PY 2014
VL 9
IS 10
AR e110331
DI 10.1371/journal.pone.0110331
UT WOS:000347994900021
PM 25347075
ER

PT C
AU Yonath, A
AF Yonath, Ada
BE Puglisi, JD
TI RIBOSOME: AN ANCIENT CELLULAR NANO-MACHINE FOR GENETIC CODE TRANSLATION
SO BIOPHYSICS AND THE CHALLENGES OF EMERGING THREATS
SE NATO Science for Peace and Security Series B-Physics and Biophysics
CT NATO Advanced Study Institute on Biophysics and the Challenges of
   Emerging Threats
CY JUN 19-30, 2007
CL Erice, ITALY
SP NATO
AB The ribosome is a ribozyme whose active site, the peptidyl transferase center (PTC), is situated within a highly conserved universal symmetrical re.-lion that connects all ribosomal functional centers involved in amino-acid polymerization. The linkage between this elaborate architecture and A-site tRNA position revealed that the A- > P-site passage of the tRNA terminus in the peptidyl-transferase center is performed by a rotatory motion, synchronized with the overall tRNA/mRNA sideways movement. Guided by the PTC the rotatory motion leads to stereochemistry suitable for peptide bond fort-nation as well as for substrate mediated catalysis, consistent with quantum mechanical calculations illuminating the transition state mechanism for peptide bond formation and indicating that the peptide bond is being formed during the rotatory motion.
   Analysis of substrate binding modes to inactive and active ribosomes illuminated the significant of PTC mobility and supported the hypothesis that the ancient ribosome produced single peptides bonds and non-coded chains, utilizing nucleotide conjugated amino acids. Genetic control of the reaction evolved after polypeptides capable of enzymatic function were created, and an ancient stable RNA fold was converted into tRNA molecules. As the symmetry relates only the backbone fold and nucleotides orientations, but not nucleotide sequence, it emphasizes the superiority of functional requirement over sequence conservation, and indicates that the PTC has evolved by gene fusion, presumably by taking advantage of similar RNA fold structures.
   The increase in antibiotic resistance among pathogenic bacterial strains poses a significant health threat. Therefore, improvement of existing antibiotics and the design of advance drugs are urgently needed. Ribosomes provide binding sited for many antibiotic families, utilizing their inherent functional flexibility, which triggers induced fit mechanism by remote interactions, and facilitates antibiotics synergism as well as reshaping less suitable binding pockets, leading to clinical usefulness even for antibiotics that bind to conserved functional regions. Exploitation of the diverse properties of antibiotics binding and benefiting from the detailed structural information that keeps emerging, should result in significant antibiotics improvement.
SN 1871-465X
BN 978-90-481-2367-4; 978-90-481-2366-7
PY 2009
BP 121
EP 155
DI 10.1007/978-90-481-2368-1_8
UT WOS:000267755300008
ER

PT J
AU Reynolds, A
   Vranic-Peters, M
   Lai, AL
   Grayden, DB
   Cook, MJ
   Peterson, A
AF Reynolds, Ashley
   Vranic-Peters, Michaela
   Lai, Alan
   Grayden, David B.
   Cook, Mark J.
   Peterson, Andre
TI Prognostic interictal electroencephalographic biomarkers and models to
   assess antiseizure medication efficacy for clinical practice: A scoping
   review
SO EPILEPSIA
AB Antiseizure medication (ASM) is the primary treatment for epilepsy. In clinical practice, methods to assess ASM efficacy (predict seizure freedom or seizure reduction), during any phase of the drug treatment lifecycle, are limited. This scoping review identifies and appraises prognostic electroencephalographic (EEG) biomarkers and prognostic models that use EEG features, which are associated with seizure outcomes following ASM initiation, dose adjustment, or withdrawal. We also aim to summarize the population and context in which these biomarkers and models were identified and described, to understand how they could be used in clinical practice. Between January 2021 and October 2022, four databases, references, and citations were systematically searched for ASM studies investigating changes to interictal EEG or prognostic models using EEG features and seizure outcomes. Study bias was appraised using modified Quality in Prognosis Studies criteria. Results were synthesized into a qualitative review. Of 875 studies identified, 93 were included. Biomarkers identified were classed as qualitative (visually identified by wave morphology) or quantitative. Qualitative biomarkers include identifying hypsarrhythmia, centrotemporal spikes, interictal epileptiform discharges (IED), classifying the EEG as normal/abnormal/epileptiform, and photoparoxysmal response. Quantitative biomarkers were statistics applied to IED, high-frequency activity, frequency band power, current source density estimates, pairwise statistical interdependence between EEG channels, and measures of complexity. Prognostic models using EEG features were Cox proportional hazards models and machine learning models. There is promise that some quantitative EEG biomarkers could be used to assess ASM efficacy, but further research is required. There is insufficient evidence to conclude any specific biomarker can be used for a particular population or context to prognosticate ASM efficacy. We identified a potential battery of prognostic EEG biomarkers, which could be combined with prognostic models to assess ASM efficacy. However, many confounders need to be addressed for translation into clinical practice.
OI Lai, Alan/0000-0002-4825-0354; Reynolds, Ashley/0000-0003-3614-5300
SN 0013-9580
EI 1528-1167
DI 10.1111/epi.17548
EA MAR 2023
UT WOS:000950134400001
PM 36790369
ER

PT J
AU Pamungkas, EW
   Basile, V
   Patti, V
AF Pamungkas, Endang Wahyu
   Basile, Valerio
   Patti, Viviana
TI A joint learning approach with knowledge injection for zero-shot
   cross-lingual hate speech detection
SO INFORMATION PROCESSING & MANAGEMENT
AB Hate speech is an increasingly important societal issue in the era of digital communication. Hateful expressions often make use of figurative language and, although they represent, in some sense, the dark side of language, they are also often prime examples of creative use of language. While hate speech is a global phenomenon, current studies on automatic hate speech detection are typically framed in a monolingual setting. In this work, we explore hate speech detection in low-resource languages by transferring knowledge from a resource-rich language, English, in a zero-shot learning fashion. We experiment with traditional and recent neural architectures, and propose two joint-learning models, using different multilingual language representations to transfer knowledge between pairs of languages. We also evaluate the impact of additional knowledge in our experiment, by incorporating information from a multilingual lexicon of abusive words. The results show that our joint-learning models achieve the best performance on most languages. However, a simple approach that uses machine translation and a pre-trained English language model achieves a robust performance. In contrast, Multilingual BERT fails to obtain a good performance in cross-lingual hate speech detection. We also experimentally found that the external knowledge from a multilingual abusive lexicon is able to improve the models' performance, specifically in detecting the positive class. The results of our experimental evaluation highlight a number of challenges and issues in this particular task. One of the main challenges is related to the issue of current benchmarks for hate speech detection, in particular how bias related to the topical focus in the datasets influences the classification performance. The insufficient ability of current multilingual language models to transfer knowledge between languages in the specific hate speech detection task also remain an open problem. However, our experimental evaluation and our qualitative analysis show how the explicit integration of linguistic knowledge from a structured abusive language lexicon helps to alleviate this issue.
OI Pamungkas, Endang Wahyu/0000-0003-0156-6754
SN 0306-4573
EI 1873-5371
PD JUL
PY 2021
VL 58
IS 4
AR 102544
DI 10.1016/j.ipm.2021.102544
EA MAR 2021
UT WOS:000658372100003
ER

PT S
AU Franz, M
AF Franz, M
BE Vitek, J
   Tschudin, C
TI Adaptive compression of syntax trees and iterative dynamic code
   optimization: Two basic technologies for mobile object systems
SO MOBILE OBJECT SYSTEMS: TOWARDS THE PROGRAMMABLE INTERNET
SE LECTURE NOTES IN COMPUTER SCIENCE
CT 2nd International Workshop on Mobile Object Systems (MOS 96)
CY JUL 08-09, 1996
CL LINZ, AUSTRIA
AB We are designing and implementing a flexible infrastructure for mobile-object systems. Two fundamental innovations distinguish our architecture from other proposed solutions. First, our representation of mobile code is based on adaptive compression of syntax trees. Not only is this representation more than twice as dense as Java byte-codes, but it also encodes semantic information on a much higher level than linear abstract-machine representations such as p-code or Java byte-codes. The extra structural information that is contained in our mobile-code format is directly beneficial for advanced code optimizations. Second, our architecture achieves superior run-time performance by integrating the activity of generating executable code into the operating system itself. Rather than being an auxiliary function performed off-line by a stand-alone compiler, code generation constitutes a central, indispensable service in our system. Our integral code generator has two distinct modes of operation: instantaneous load-time translation and continuous dynamic re-optimization, In contrast to just-in-time compilers that translate individual procedures on a call-by-call basis, our system's integral code-generator translates complete code-closures in a single burst during loading. This has the apparent disadvantage that it introduces a minor delay prior to the start of execution. As a consequence, to some extent we have to favor compilation speed over code quality at load time. But then, the second operation mode of our embedded code generator soon corrects this shortcoming. Central to our run-time architecture is a thread of activity that continually optimizes all of the already executing software in the background. Since this is strictly a re-compilation of already existing code, and since it occurs completely in the background, speed is not critical, so that aggressive, albeit slow, optimization techniques can be employed. Upon completion, the previously executing version of the same code is supplanted by the newly generated one and re-optimization starts over. By constructing globally optimized code-images from mobile software components, our architecture is able to reconcile dynamic composability with the run-time efficiency of monolithic applications.
SN 0302-9743
BN 3-540-62852-5
PY 1997
VL 1222
BP 263
EP 276
UT WOS:000074015900020
ER

PT J
AU Mangeot-Nagata, M
AF Mangeot-Nagata, Mathieu
TI COLLABORATIVE CONSTRUCTION OF A GOOD QUALITY, BROAD COVERAGE AND
   COPYRIGHT FREE JAPANESE-FRENCH DICTIONARY
SO INTERNATIONAL JOURNAL OF LEXICOGRAPHY
AB Although French and Japanese are regarded as well-resourced languages concerning tools and linguistic resources, the French-Japanese couple is considered an under-resourced language pair regarding its availability on the Web. Indeed, there are few bilingual electronic lexical resources of quality and which are both royalty and copyright free. French-Japanese bilingual aligned corpora and machine translation systems are logically equally rare.
   Fortunately, there are printed French-Japanese dictionaries of good quality and which are sufficiently old to be royalty-free. It should be possible to reuse these resources as part of our project to build a good quality and broad coverage dictionary available on the Web. In order to update this data whose vocabulary might be old, we could reuse existing electronic resources such as Wikipedia or Japanese-English electronic resources. The resulting resource could be then available on the Web for lookup and correction by voluntary contributors. This methodology could be applied to other language couples in a similar situation with good printed dictionaries but few electronic resources.
   We first conduct an inventory of Japanese bilingual dictionaries (printed or electronic) with their historical evolution. Then, we describe the resource we want to build. The next part concerns the conversion of three resources: the Cesselin Japanese-French printed dictionary, the language links between Japanese, French and English Wikipedia pages and the JMdict Japanese-English electronic dictionary. The Cesselin dictionary has been scanned, OCRized and parsed to detect headwords and entries. Then several error correction were performed on French and Japanese. New entries were created from Wikipedia links and finally, missing JMdict dictionary entries missing in the result resource were converted and added. Finally, we released the resource on a Web site built around the Jibiki platform allowing articles to be viewed and edited online. A French-Japanese bilingual corpus and an active reading moduel are also available. The resulting resources (dictionaries and corpora) are available for download on the project website. The data is released under public domain.
SN 0950-3846
EI 1477-4577
PD MAR
PY 2018
VL 31
IS 1
BP 78
EP 112
DI 10.1093/ijl/ecw035
UT WOS:000426865500004
ER

PT J
AU Andre, B
   Vercauteren, T
   Buchner, AM
   Wallace, MB
   Ayache, N
AF Andre, Barbara
   Vercauteren, Tom
   Buchner, Anna M.
   Wallace, Michael B.
   Ayache, Nicholas
TI Learning Semantic and Visual Similarity for Endomicroscopy Video
   Retrieval
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
AB Content-based image retrieval (CBIR) is a valuable computer vision technique which is increasingly being applied in the medical community for diagnosis support. However, traditional CBIR systems only deliver visual outputs, i.e., images having a similar appearance to the query, which is not directly interpretable by the physicians. Our objective is to provide a system for endomicroscopy video retrieval which delivers both visual and semantic outputs that are consistent with each other. In a previous study, we developed an adapted bag-of-visual-words method for endomicroscopy retrieval, called "Dense-Sift," that computes a visual signature for each video. In this paper, we present a novel approach to complement visual similarity learning with semantic knowledge extraction, in the field of in vivo endomicroscopy. We first leverage a semantic ground truth based on eight binary concepts, in order to transform these visual signatures into semantic signatures that reflect how much the presence of each semantic concept is expressed by the visual words describing the videos. Using cross-validation, we demonstrate that, in terms of semantic detection, our intuitive Fisher-based method transforming visual-word histograms into semantic estimations outperforms support vector machine (SVM) methods with statistical significance. In a second step, we propose to improve retrieval relevance by learning an adjusted similarity distance from a perceived similarity ground truth. As a result, our distance learning method allows to statistically improve the correlation with the perceived similarity. We also demonstrate that, in terms of perceived similarity, the recall performance of the semantic signatures is close to that of visual signatures and significantly better than those of several state-of-the-art CBIR methods. The semantic signatures are thus able to communicate high-level medical knowledge while being consistent with the low-level visual signatures and much shorter than them. In our resulting retrieval system, we decide to use visual signatures for perceived similarity learning and retrieval, and semantic signatures for the output of an additional information, expressed in the endoscopist own language, which provides a relevant semantic translation of the visual retrieval outputs.
RI Vercauteren, Tom K/I-7290-2013; Wallace, Michael/GZL-9731-2022
OI Vercauteren, Tom K/0000-0003-1794-0456; Wallace,
   Michael/0000-0002-6446-5785
SN 0278-0062
EI 1558-254X
PD JUN
PY 2012
VL 31
IS 6
BP 1276
EP 1288
DI 10.1109/TMI.2012.2188301
UT WOS:000304911300010
PM 22353403
ER

PT J
AU Lees, T
   Reece, S
   Kratzert, F
   Klotz, D
   Gauch, M
   De Bruijn, J
   Sahu, RK
   Greve, P
   Slater, L
   Dadson, SJ
AF Lees, Thomas
   Reece, Steven
   Kratzert, Frederik
   Klotz, Daniel
   Gauch, Martin
   De Bruijn, Jens
   Kumar Sahu, Reetik
   Greve, Peter
   Slater, Louise
   Dadson, Simon J.
TI Hydrological concept formation inside long short-term memory (LSTM)
   networks
SO HYDROLOGY AND EARTH SYSTEM SCIENCES
AB Neural networks have been shown to be extremely effective rainfall-runoff models, where the river discharge is predicted from meteorological inputs. However, the question remains: what have these models learned? Is it possible to extract information about the learned relationships that map inputs to outputs, and do these mappings represent known hydrological concepts? Small-scale experiments have demonstrated that the internal states of long short-term memory networks (LSTMs), a particular neural network architecture predisposed to hydrological modelling, can be interpreted. By extracting the tensors which represent the learned translation from inputs (precipitation, temperature, and potential evapotranspiration) to outputs (discharge), this research seeks to understand what information the LSTM captures about the hydrological system. We assess the hypothesis that the LSTM replicates real-world processes and that we can extract information about these processes from the internal states of the LSTM. We examine the cell-state vector, which represents the memory of the LSTM, and explore the ways in which the LSTM learns to reproduce stores of water, such as soil moisture and snow cover. We use a simple regression approach to map the LSTM state vector to our target stores (soil moisture and snow). Good correlations (R-2 > 0.8) between the probe outputs and the target variables of interest provide evidence that the LSTM contains information that reflects known hydrological processes comparable with the concept of variable-capacity soil moisture stores.
   The implications of this study are threefold: (1) LSTMs reproduce known hydrological processes. (2) While conceptual models have theoretical assumptions embedded in the model a priori, the LSTM derives these from the data. These learned representations are interpretable by scientists. (3) LSTMs can be used to gain an estimate of intermediate stores of water such as soil moisture. While machine learning interpretability is still a nascent field and our approach reflects a simple technique for exploring what the model has learned, the results are robust to different initial conditions and to a variety of benchmarking experiments. We therefore argue that deep learning approaches can be used to advance our scientific goals as well as our predictive goals.
RI Greve, Peter/AAD-4377-2019
OI Greve, Peter/0000-0002-9454-0125; Kratzert,
   Frederik/0000-0002-8897-7689; Slater, Louise/0000-0001-9416-488X; Sahu,
   Reetik Kumar/0000-0003-0681-0509; de Bruijn, Jens/0000-0003-3961-6382;
   Gauch, Martin/0000-0002-4587-898X
SN 1027-5606
EI 1607-7938
PD JUN 20
PY 2022
VL 26
IS 12
BP 3079
EP 3101
DI 10.5194/hess-26-3079-2022
UT WOS:000813039800001
ER

PT J
AU Meher, PK
   Rai, A
   Rao, AR
AF Meher, Prabina Kumar
   Rai, Anil
   Rao, Atmakuri Ramakrishna
TI mLoc-mRNA: predicting multiple sub-cellular localization of mRNAs using
   random forest algorithm coupled with feature selection via elastic net
SO BMC BIOINFORMATICS
AB Background: Localization of messenger RNAs (mRNAs) plays a crucial role in the growth and development of cells. Particularly, it plays a major role in regulating spatio-temporal gene expression. The in situ hybridization is a promising experimental technique used to determine the localization of mRNAs but it is costly and laborious. It is also a known fact that a single mRNA can be present in more than one location, whereas the existing computational tools are capable of predicting only a single location for such mRNAs. Thus, the development of high-end computational tool is required for reliable and timely prediction of multiple subcellular locations of mRNAs. Hence, we develop the present computational model to predict the multiple localizations of mRNAs.
   Results: The mRNA sequences from 9 different localizations were considered. Each sequence was first transformed to a numeric feature vector of size 5460, based on the k-mer features of sizes 1-6. Out of 5460 k-mer features, 1812 important features were selected by the Elastic Net statistical model. The Random Forest supervised learning algorithm was then employed for predicting the localizations with the selected features. Five-fold cross-validation accuracies of 70.87, 68.32, 68.36, 68.79, 96.46, 73.44, 70.94, 97.42 and 71.77% were obtained for the cytoplasm, cytosol, endoplasmic reticulum, exosome, mitochondrion, nucleus, pseudopodium, posterior and ribosome respectively. With an independent test set, accuracies of 65.33, 73.37, 75.86, 72.99, 94.26, 70.91, 65.53, 93.60 and 73.45% were obtained for the respective localizations. The developed approach also achieved higher accuracies than the existing localization prediction tools.
   Conclusions: This study presents a novel computational tool for predicting the multiple localization of mRNAs. Based on the proposed approach, an online prediction server "mLoc-mRNA" is accessible at http://cabgrid.res.in:8080/mlocmrna/. The developed approach is believed to supplement the existing tools and techniques for the localization prediction of mRNAs.
RI Meher, Prabina Kumar/V-9439-2019
OI Meher, Prabina Kumar/0000-0002-7098-8785
SN 1471-2105
PD JUN 24
PY 2021
VL 22
IS 1
AR 342
DI 10.1186/s12859-021-04264-8
UT WOS:000668581400002
PM 34167457
ER

PT J
AU Mura, C
   Pajarola, R
   Schindler, K
   Mitra, N
AF Mura, Claudio
   Pajarola, Renato
   Schindler, Konrad
   Mitra, Niloy
TI Walk2Map: Extracting Floor Plans from Indoor Walk Trajectories
SO COMPUTER GRAPHICS FORUM
AB Recent years have seen a proliferation of new digital products for the efficient management of indoor spaces, with important applications like emergency management, virtual property showcasing and interior design. While highly innovative and effective, these products rely on accurate 3D models of the environments considered, including information on both architectural and non-permanent elements. These models must be created from measured data such as RGB-D images or 3D point clouds, whose capture and consolidation involves lengthy data workflows. This strongly limits the rate at which 3D models can be produced, preventing the adoption of many digital services for indoor space management. We provide a radical alternative to such data-intensive procedures by presenting Walk2Map, a data-driven approach to generate floor plans only from trajectories of a person walking inside the rooms. Thanks to recent advances in data-driven inertial odometry, such minimalistic input data can be acquired from the IMU readings of consumer-level smartphones, which allows for an effortless and scalable mapping of real-world indoor spaces. Our work is based on learning the latent relation between an indoor walk trajectory and the information represented in a floor plan: interior space footprint, portals, and furniture. We distinguish between recovering area-related (interior footprint, furniture) and wall-related (doors) information and use two different neural architectures for the two tasks: an image-based Encoder-Decoder and a Graph Convolutional Network, respectively. We train our networks using scanned 3D indoor models and apply them in a cascaded fashion on an indoor walk trajectory at inference time. We perform a qualitative and quantitative evaluation using both trajectories simulated from scanned models of interiors and measured, real-world trajectories, and compare against a baseline method for image-to-image translation. The experiments confirm that our technique is viable and allows recovering reliable floor plans from minimal walk trajectory data.
SN 0167-7055
EI 1467-8659
PD MAY
PY 2021
VL 40
IS 2
BP 375
EP 388
DI 10.1111/cgf.142640
UT WOS:000657959600031
ER

PT J
AU Guarin, DL
   Yunusova, Y
   Taati, B
   Dusseldorp, JR
   Mohan, S
   Tavares, J
   van Veen, MM
   Fortier, E
   Hadlock, TA
   Jowett, N
AF Guarin, Diego L.
   Yunusova, Yana
   Taati, Babak
   Dusseldorp, Joseph R.
   Mohan, Suresh
   Tavares, Joana
   van Veen, Martinus M.
   Fortier, Emily
   Hadlock, Tessa A.
   Jowett, Nate
TI Toward an Automatic System for Computer-Aided Assessment in Facial Palsy
SO FACIAL PLASTIC SURGERY & AESTHETIC MEDICINE
AB Importance: Quantitative assessment of facial function is challenging, and subjective grading scales such as House-Brackmann, Sunnybrook, and eFACE have well-recognized limitations. Machine learning (ML) approaches to facial landmark localization carry great clinical potential as they enable high-throughput automated quantification of relevant facial metrics from photographs and videos. However, the translation from research settings to clinical application still requires important improvements.
   Objective: To develop a novel ML algorithm for fast and accurate localization of facial landmarks in photographs of facial palsy patients and utilize this technology as part of an automated computer-aided diagnosis system.
   Design, Setting, and Participants: Portrait photographs of 8 expressions obtained from 200 facial palsy patients and 10 healthy participants were manually annotated by localizing 68 facial landmarks in each photograph and by 3 trained clinicians using a custom graphical user interface. A novel ML model for automated facial landmark localization was trained using this disease-specific database. Algorithm accuracy was compared with manual markings and the output of a model trained using a larger database consisting only of healthy subjects.
   Main Outcomes and Measurements: Root mean square error normalized by the interocular distance (NRMSE) of facial landmark localization between prediction of ML algorithm and manually localized landmarks.
   Results: Publicly available algorithms for facial landmark localization provide poor localization accuracy when applied to photographs of patients compared with photographs of healthy controls (NRMSE, 8.56 +/- 2.16 vs. 7.09 +/- 2.34, p MUCH LESS-THAN 0.01). We found significant improvement in facial landmark localization accuracy for the facial palsy patient population when using a model trained with a relatively small number photographs (1440) of patients compared with a model trained using several thousand more images of healthy faces (NRMSE, 6.03 +/- 2.43 vs. 8.56 +/- 2.16, p MUCH LESS-THAN 0.01).
   Conclusions and Relevance: Retraining a computer vision facial landmark detection model with fewer than 1600 annotated images of patients significantly improved landmark detection performance in frontal view photographs of this population. The new annotated database and facial landmark localization model represent the first steps toward an automatic system for computer-aided assessment in facial palsy.
RI Yunusova, Yana/E-3428-2010
OI Yunusova, Yana/0000-0002-2353-2275; Guarin, Diego
   L./0000-0001-6077-7063; Dusseldorp, Joseph/0000-0002-6392-7423; Jowett,
   Nate/0000-0002-5242-0264
SN 2689-3614
EI 2689-3622
PD FEB 1
PY 2020
VL 22
IS 1
BP 42
EP 49
DI 10.1089/fpsam.2019.29000.gua
UT WOS:000525077900007
PM 32053425
ER

PT C
AU Zhou, SJ
   Kannan, R
   Prasanna, VK
AF Zhou, Shijie
   Kannan, Rajgopal
   Prasanna, Viktor K.
BE Athanas, P
   Cumplido, R
   Feregrino, C
   Sass, R
TI Accelerating Low Rank Matrix Completion on FPGA
SO 2017 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS
   (RECONFIG)
SE Proceedings International Conference on Reconfigurable Computing and
   FPGAs
CT International Conference on Reconfigurable Computing and FPGAs
   (ReConFig)
CY DEC 04-06, 2017
CL Cancun, MEXICO
SP Natl Inst Astrophy Opt & Elect Mexico, Virginia Tech, Univ N Carolina Charlotte, IEEE, IEEE Circuits & Syst Soc, XILINX
AB Low Rank Matrix Completion (LRMC) is widely used in the analysis of incomplete datasets. In this paper, we propose a novel FPGA-based accelerator to speedup a matrix-factorization-based LRMC algorithm that uses stochastic gradient descent. The accelerator is a multi-pipelined architecture with parallel pipelines processing distinct data from a shared on-chip buffer. We propose two distinct on-chip buffer architectures based on a design-space exploration of the performance tradeoffs offered by two competing design methodologies: memory-efficiency versus concurrent conflict-free accesses. Our first design (i.e., memory-efficient design) organizes the buffer into banks and maximally utilizes available on-chip memory for matrix chunk processing without requiring complex address translation tables for on-chip addressing; however, it could incur bank conflicts when concurrent accesses to the same bank occur. The second design (i.e., bank-conflict-free design) exploits parallel multiport memory access and completely eliminates bank conflicts by duplicating the stored data; however, it has much higher on-chip RAM consumption. Intuitively, design one enables (slower) acceleration of (larger) chunks of the input matrix whereas design two enables (faster) processing of (smaller) matrix chunks but requires more iterations for processing the complete matrix. We propose a simple but efficient partitioning approach for supporting large input matrices that do not fit in the on-chip memory of FPGA. We also develop algorithmic optimizations based on matching to reduce data dependencies for parallel pipeline execution. We implement our designs on a state-of-the-art UltraScale+FPGA device. We use real-life datasets for the evaluation and compare these two designs by varying the number of pipelines. The data dependency optimization results in at least 21.6x data dependency reduction and improves the execution time by up to 66.3x compared with non-optimized baseline designs. The memory-efficient design is also shown to be more scalable than the bank-conflict-free design. Compared with the state-of-the-art multi-core implementation and GPU implementation, the bank-conflict-free design achieves 5.4x and 5.2x speedup, respectively; the memory-efficient design achieves 16.7x and 16.2x speedup, respectively.
SN 2325-6532
BN 978-1-5386-3797-5
PY 2017
UT WOS:000426529700004
ER

PT J
AU Chinda, B
   Medvedev, G
   Siu, W
   Ester, M
   Arab, A
   Gu, T
   Moreno, S
   D'Arcy, RCN
   Song, XW
AF Chinda, Betty
   Medvedev, George
   Siu, William
   Ester, Martin
   Arab, Ali
   Gu, Tao
   Moreno, Sylvain
   D'Arcy, Ryan C. N.
   Song, Xiaowei
TI Automation of CT-based haemorrhagic stroke assessment for improved
   clinical outcomes: study protocol and design
SO BMJ OPEN
AB Introduction Haemorrhagic stroke is of significant healthcare concern due to its association with high mortality and lasting impact on the survivors' quality of life. Treatment decisions and clinical outcomes depend strongly on the size, spread and location of the haematoma. Non-contrast CT (NCCT) is the primary neuroimaging modality for haematoma assessment in haemorrhagic stroke diagnosis. Current procedures do not allow convenient NCCT-based haemorrhage volume calculation in clinical settings, while research-based approaches are yet to be tested for clinical utility; there is a demonstrated need for developing effective solutions. The project under review investigates the development of an automatic NCCT-based haematoma computation tool in support of accurate quantification of haematoma volumes.
   Methods and analysis Several existing research methods for haematoma volume estimation are studied. Selected methods are tested using NCCT images of patients diagnosed with acute haemorrhagic stroke. For inter-rater and intrarater reliability evaluation, different raters will analyse haemorrhage volumes independently. The efficiency with respect to time of haematoma volume assessments will be examined to compare with the results from routine clinical evaluations and planimetry assessment that are known to be more accurate. The project will target the development of an enhanced solution by adapting existing methods and integrating machine learning algorithms. NCCT-based information of brain haemorrhage (eg, size, volume, location) and other relevant information (eg, age, sex, risk factor, comorbidities) will be used in relation to clinical outcomes with future project development. Validity and reliability of the solution will be examined for potential clinical utility.
   Ethics and dissemination The project including procedures for deidentification of NCCT data has been ethically approved. The study involves secondary use of existing data and does not require new consent of participation. The team consists of clinical neuroimaging scientists, computing scientists and clinical professionals in neurology and neuroradiology and includes patient representatives. Research outputs will be disseminated following knowledge translation plans towards improving stroke patient care. Significant findings will be published in scientific journals. Anticipated deliverables include computer solutions for improved clinical assessment of haematoma using NCCT.
RI Gu, Tao/Z-2869-2019
OI Gu, Tao/0000-0002-1350-6639
SN 2044-6055
PD APR
PY 2018
VL 8
IS 4
AR e020260
DI 10.1136/bmjopen-2017-020260
UT WOS:000435176700148
PM 29674371
ER

PT J
AU Muriuki, MG
   Tuason, DA
   Tucker, BG
   Harner, CD
AF Muriuki, M. G.
   Tuason, D. A.
   Tucker, B. G.
   Harner, C. D.
TI Changes in Tibiofemoral Contact Mechanics Following Radial Split and
   Vertical Tears of the Medial Meniscus An in Vitro Investigation of the
   Efficacy of Arthroscopic Repair
SO JOURNAL OF BONE AND JOINT SURGERY-AMERICAN VOLUME
AB Background: The biomechanical effects of radial split tears and vertical tears of the medial meniscus are not well characterized. The goal of the present study was to determine the effects of these meniscal tears and meniscal repair on tibiofemoral joint contact pressure and area.
   Methods: Eleven fresh-frozen cadaveric knees were loaded to 1000 N of axial load at 0 degrees, 30 degrees, 60 degrees, and 90 degrees of flexion with use of a custom testing apparatus attached to a materials testing machine. Tibiofemoral translations and internal-external and varus-valgus rotations were unconstrained. The knees were tested under four conditions: intact, medial meniscal tear, repaired meniscal tear, and total medial meniscectomy. Radial split tears were created in six knees, and vertical tears were created in five knees. Pressure-sensitive film was used to measure tibiofemoral contact pressure and area.
   Results: Radial split tears of the medial meniscus did not cause significant changes in tibiofemoral joint contact pressure and area. Vertical tears of the medial meniscus caused increases in tibiofemoral joint contact pressure and reductions in contact area in the medial and lateral compartments that were not significantly different from those associated with total medial meniscectomy. The exception was at 90 degrees, where the lateral compartment pressure associated with the vertical tear of the medial meniscus was higher than that associated with total medial meniscectomy. In general, after repair of the vertical tear, contact pressure and area values were similar to those in the intact condition.
   Conclusions: Radial split tears of the medial meniscus that extend from the inner rim to the peripheral third of the meniscus do not cause significant changes in joint contact area and pressure. Vertical tears of the medial meniscus cause nonsignificant increases in joint contact pressure and reductions in contact area in the medial and lateral compartments. Repair of the vertical tear reverses these contact changes, resulting in contact pressure and area similar to the intact state.
RI Muriuki, Muturi/AAY-2764-2020
OI Muriuki, Muturi/0000-0002-0620-9169
SN 0021-9355
PD JUN 15
PY 2011
VL 93A
IS 12
BP 1089
EP 1095
DI 10.2106/JBJS.I.01241
UT WOS:000291623400001
PM 21571989
ER

PT J
AU Yan, SS
   Bowsher, J
   Yin, FF
AF Yan, Susu
   Bowsher, James
   Yin, Fang-Fang
TI A line-source method for aligning on-board and other pinhole SPECT
   systems
SO MEDICAL PHYSICS
AB Purpose: In order to achieve functional and molecular imaging as patients are in position for radiation therapy, a robotic multipinhole SPECT system is being developed. Alignment of the SPECT system-to the linear accelerator (LINAC) coordinate frame and to the coordinate frames of other on-board imaging systems such as cone-beam CT (CBCT)-is essential for target localization and image reconstruction. An alignment method that utilizes line sources and one pinhole projection is proposed and investigated to achieve this goal. Potentially, this method could also be applied to the calibration of the other pinhole SPECT systems.
   Methods: An alignment model consisting of multiple alignment parameters was developed which maps line sources in three-dimensional (3D) space to their two-dimensional (2D) projections on the SPECT detector. In a computer-simulation study, 3D coordinates of line-sources were defined in a reference room coordinate frame, such as the LINAC coordinate frame. Corresponding 2D line-source projections were generated by computer simulation that included SPECT blurring and noise effects. The Radon transform was utilized to detect angles (alpha) and offsets (rho) of the line-source projections. Alignment parameters were then estimated by a nonlinear least squares method, based on the alpha and rho values and the alignment model. Alignment performance was evaluated as a function of number of line sources, Radon transform accuracy, finite line-source width, intrinsic camera resolution, Poisson noise, and acquisition geometry. Experimental evaluations were performed using a physical linesource phantom and a pinhole-collimated gamma camera attached to a robot.
   Results: In computer-simulation studies, when there was no error in determining angles (alpha) and offsets (rho) of the measured projections, six alignment parameters (three translational and three rotational) were estimated perfectly using three line sources. When angles (alpha) and offsets (rho) were provided by the Radon transform, estimation accuracy was reduced. The estimation error was associated with rounding errors of Radon transform, finite line-source width, Poisson noise, number of line sources, intrinsic camera resolution, and detector acquisition geometry. Statistically, the estimation accuracy was significantly improved by using four line sources rather than three and by thinner linesource projections (obtained by better intrinsic detector resolution). With five line sources, median errors were 0.2 mm for the detector translations, 0.7 mm for the detector radius of rotation, and less than 0.5 degrees for detector rotation, tilt, and twist. In experimental evaluations, average errors relative to a different, independent registration technique were about 1.8 mm for detector translations, 1.1 mm for the detector radius of rotation (ROR), 0.5 degrees and 0.4 degrees for detector rotation and tilt, respectively, and 1.2. for detector twist.
   Conclusions: Alignment parameters can be estimated using one pinhole projection of line sources. Alignment errors are largely associated with limited accuracy of the Radon transform in determining angles (alpha) and offsets (rho) of the line-source projections. This alignment method may be important for multipinhole SPECT, where relative pinhole alignment may vary during rotation. For pinhole and multipinhole SPECT imaging on-board radiation therapy machines, the method could provide alignment of SPECT coordinates with those of CBCT and the LINAC. (c) 2013 American Association of Physicists in Medicine.
OI Yin, Fang-Fang/0000-0002-2025-4740
SN 0094-2405
PD DEC
PY 2013
VL 40
IS 12
DI 10.1118/1.4828776
UT WOS:000328031300053
PM 24320537
ER

PT J
AU Wu, J
   Wang, LR
   Huang, MY
   Peng, ZY
AF Wu Jun
   Wang Lingrong
   Huang Mingyi
   Peng Zhiyong
TI High Precision Calibration of Fisheye Camera with Single Image under
   Multiple Geometric Constraints
SO ACTA OPTICA SINICA
AB By taking the single checkerboard image of fisheye cameras to characterize as targets and comprehensively using multiple geometric constraints, the initial values of fisheye camera parameters arc solved in different steps and the global optimization is simultaneously implemented. The exact principal point location (u(0), v(0)) of the camera is obtained by means of fisheye image contours and their symmetry, in which the difficulty in detecting contour points under a black background is skillfully avoided through scanning the bounding box of fisheye image contours. Two group of projection ellipses on the fisheye checkerboard images arc accurately fitted whose intersection points arc back-projected to the unit sphere to obtain vanishing points of the parallel lines, and the initial value (f(x), f(y)) of the equivalent focal length of camera and the initial angle of rotation matrix arc deduced from the orthogonal constraints of two vanishing points. With the radial alignment constraint and the checkerboard corner point information, the initial values of the translation vectors (t(x), t(y)) arc first solved linearly, then that of t(z) is obtained after the establishment of a quadratic equation with one unknown, and finally, through minimizing the re-projection error of the checkerboard corner point, all the camera parameters except for the principal point arc globally optimized and these optimized parameters arc used for the correction of fisheye images. Two focus-fixed Hikvision fisheye cameras with different fields of view arc selected for the calibration and image correction tests. The results show that the reprojection root mean square error (RMSE) of each camera for the proposed method is less than 1/3 pixel, and the calibration parameters can keep a good stability in the planar perspective correction effect at different areas of the fisheye image with the correction effect in the central region slightly better than that at the edge. The RMSE in the checkerboard corner point line fitting of the corrected fisheye images is less than 0.7 pixel which is obviously superior to that from the online calibration toolbox, and thus it has a relatively good application value.
RI Wu, Jun/HKW-0177-2023
SN 0253-2239
PD NOV 10
PY 2018
VL 38
IS 11
AR 1115001
DI 10.3788/AOS201838.1115001
UT WOS:000619507400021
ER

PT J
AU Bai, O
   Lin, P
   Huang, DD
   Fei, DY
   Floeter, MK
AF Bai, Ou
   Lin, Peter
   Huang, Dandan
   Fei, Ding-Yu
   Floeter, Mary Kay
TI Towards a user-friendly brain-computer interface: Initial tests in ALS
   and PLS patients
SO CLINICAL NEUROPHYSIOLOGY
AB Objective: Patients usually require long-term training for effective EEG-based brain-computer interface (BCI) control due to fatigue caused by the demands for focused attention during prolonged BCI operation. We intended to develop a user-friendly BCI requiring minimal training and less mental load.
   Methods: Testing of BCI performance was investigated in three patients with amyotrophic lateral sclerosis (ALS) and three patients with primary lateral sclerosis (PLS), who had no previous BCI experience. All patients performed binary control of cursor movement. One ALS patient and one PLS patient performed four-directional cursor control in a two-dimensional domain under a BCI paradigm associated with human natural motor behavior using motor execution and motor imagery. Subjects practiced for 510 min and then participated in a multi-session study of either binary control or four-directional control including online BCI game over 1.5-2 h in a single visit.
   Results: Event-related desynchronization and event-related synchronization in the beta band were observed in all patients during the production of voluntary movement either by motor execution or motor imagery. The online binary control of cursor movement was achieved with an average accuracy about 82.1 +/- 8.2% with motor execution and about 80% with motor imagery, whereas offline accuracy was achieved with 91.4 +/- 3.4% with motor execution and 83.3 +/- 8.9% with motor imagery after optimization. In addition, four-directional cursor control was achieved with an accuracy of 50-60% with motor execution and motor imagery.
   Conclusion: Patients with ALS or PLS may achieve BCI control without extended training, and fatigue might be reduced during operation of a BCI associated with human natural motor behavior.
   Significance: The development of a user-friendly BCI will promote practical BCI applications in paralyzed patients. (C) 2010 International Federation of Clinical Neurophysiology. Published by Elsevier Ireland Ltd. All rights reserved.
RI Huang, Dandan/I-3108-2013
OI Lin, Peter/0000-0003-1198-7763
SN 1388-2457
PD AUG
PY 2010
VL 121
IS 8
BP 1293
EP 1303
DI 10.1016/j.clinph.2010.02.157
UT WOS:000279068200017
PM 20347612
ER

PT J
AU Burger, A
   Baldock, RA
   Adams, DJ
   Din, S
   Papatheodorou, I
   Glinka, M
   Hill, B
   Houghton, D
   Sharghi, M
   Wicks, M
   Arends, MJ
AF Burger, Albert
   Baldock, Richard A.
   Adams, David J.
   Din, Shahida
   Papatheodorou, Irene
   Glinka, Michael
   Hill, Bill
   Houghton, Derek
   Sharghi, Mehran
   Wicks, Michael
   Arends, Mark J.
TI Towards a clinically-based common coordinate framework for the human gut
   cell atlas: the gut models
SO BMC MEDICAL INFORMATICS AND DECISION MAKING
AB BackgroundThe Human Cell Atlas resource will deliver single cell transcriptome data spatially organised in terms of gross anatomy, tissue location and with images of cellular histology. This will enable the application of bioinformatics analysis, machine learning and data mining revealing an atlas of cell types, sub-types, varying states and ultimately cellular changes related to disease conditions. To further develop the understanding of specific pathological and histopathological phenotypes with their spatial relationships and dependencies, a more sophisticated spatial descriptive framework is required to enable integration and analysis in spatial terms.MethodsWe describe a conceptual coordinate model for the Gut Cell Atlas (small and large intestines). Here, we focus on a Gut Linear Model (1-dimensional representation based on the centreline of the gut) that represents the location semantics as typically used by clinicians and pathologists when describing location in the gut. This knowledge representation is based on a set of standardised gut anatomy ontology terms describing regions in situ, such as ileum or transverse colon, and landmarks, such as ileo-caecal valve or hepatic flexure, together with relative or absolute distance measures. We show how locations in the 1D model can be mapped to and from points and regions in both a 2D model and 3D models, such as a patient's CT scan where the gut has been segmented.ResultsThe outputs of this work include 1D, 2D and 3D models of the human gut, delivered through publicly accessible Json and image files. We also illustrate the mappings between models using a demonstrator tool that allows the user to explore the anatomical space of the gut. All data and software is fully open-source and available online.ConclusionsSmall and large intestines have a natural "gut coordinate" system best represented as a 1D centreline through the gut tube, reflecting functional differences. Such a 1D centreline model with landmarks, visualised using viewer software allows interoperable translation to both a 2D anatomogram model and multiple 3D models of the intestines. This permits users to accurately locate samples for data comparison.
OI Adams, David/0000-0001-9490-0306; Papatheodorou,
   Irene/0000-0001-7270-5470
EI 1472-6947
PD FEB 15
PY 2023
VL 23
IS 1
AR 36
DI 10.1186/s12911-023-02111-9
UT WOS:000937162500001
PM 36793076
ER

PT J
AU Fernandes, JB
   Ramos, C
   Domingos, J
   Castro, C
   Simoes, A
   Bernardes, C
   Fonseca, J
   Proenca, L
   Grunho, M
   Moleirinho-Alves, P
   Simoes, S
   Sousa-Catita, D
   Vareta, DA
   Godinho, C
AF Fernandes, Julio Belo
   Ramos, Catarina
   Domingos, Josefa
   Castro, Cidalia
   Simoes, Aida
   Bernardes, Catarina
   Fonseca, Jorge
   Proenca, Luis
   Grunho, Miguel
   Moleirinho-Alves, Paula
   Simoes, Sergio
   Sousa-Catita, Diogo
   Vareta, Diana Alves
   Godinho, Catarina
TI Addressing Ageism-Be Active in Aging: Study Protocol
SO JOURNAL OF PERSONALIZED MEDICINE
AB Ageism refers to stereotyping (how we think), prejudice (how we feel), and discrimination (how we act) against people based on their age. It is a serious public health issue that can negatively impact older people's health and quality of life. The present protocol has several goals: (1) adapt the Ambivalent Ageism Scale for the general Portuguese population and healthcare professionals; (2) assess the factorial invariance of the questionnaire between general population vs. healthcare professionals; (3) evaluate the level of ageism and its predictors in the general population and evaluate the level of ageism and its predictors in healthcare professionals; (4) compare the levels of ageism between groups and the invariance between groups regarding the explanatory model of predictors of ageism. This quantitative, cross-sectional, descriptive, observational study will be developed in partnership with several Healthcare Professional Boards/Associations, National Geriatrics and Gerontology Associations, and the Universities of the Third Age Network Association. The web-based survey will be conducted on a convenience sample recruited via various social media and institutional channels. The survey consists of three questionnaires: (1) Demographic data; (2) Ambivalent Ageism Scale; (3) Palmore-Neri and Cachioni questionnaire. The methodology of this study will include translation, pilot testing, semantic adjustment, exploratory and confirmatory factor analysis, and multigroup analysis of the Ambivalent Ageism Scale. Data will be treated using International Business Machines Corporation (IBM (R)) Statistical Package for the Social Sciences (SPSS) software and Analysis of Moment Structures (AMOS). Descriptive analysis will be conducted to assess the level of ageism in the study sample. The ageism levels between the two groups will be compared using the t-student test, and two Structural Equation Modeling will be developed to evaluate the predictors of ageism. Assessing ageism is necessary to allow healthcare professionals and policymakers to design and implement strategies to solve or reduce this issue. Findings from this study will generate knowledge relevant to healthcare and medical courses along with anti-ageism education for the Portuguese population.
RI Fernandes, Júlio/AAQ-3458-2021; Domingos, Josefa Maria/ABA-9148-2021;
   Proença, Luis F A/N-6604-2013; Godinho, Catarina/ABF-6650-2020
OI Fernandes, Júlio/0000-0003-4613-7339; Domingos, Josefa
   Maria/0000-0001-9390-6183; Proença, Luis F A/0000-0002-8482-5936;
   Godinho, Catarina/0000-0003-2304-3129; Grunho,
   Miguel/0000-0002-5529-7695; Castro, Cidalia/0000-0002-7402-9442; Sousa
   Paraiso Bernardes, Catarina Isabel/0000-0002-3475-1983; Ramos,
   Catarina/0000-0003-2867-1466; Fonseca, Jorge/0000-0001-6477-7028; Sousa
   Catita, Diogo/0000-0001-7998-6199; Vareta, Diana/0000-0002-6368-891X;
   Correia Simoes, Aida de Jesus/0000-0001-5410-0590
EI 2075-4426
PD MAR
PY 2022
VL 12
IS 3
AR 354
DI 10.3390/jpm12030354
UT WOS:000776412400001
PM 35330354
ER

PT J
AU Zhang, XY
   Wu, YW
   Zhou, PP
   Tang, XL
   Hu, JT
AF Zhang, Xinyi
   Wu, Yawen
   Zhou, Peipei
   Tang, Xulong
   Hu, Jingtong
TI Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices
SO ACM TRANSACTIONS ON EMBEDDED COMPUTING SYSTEMS
CT International Conference on Hardware/Software Codesign and System
   Synthesis (CODES plus ISSS)
CY OCT 10-15, 2021
CL ELECTR NETWORK
AB Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA.
RI Zhou, Peipei/ABZ-3578-2022; Zhang, Xinyi/ADO-0487-2022; Wu,
   Yawen/HKF-7274-2023
OI Zhou, Peipei/0000-0002-0493-1844; Zhang, Xinyi/0000-0002-9307-1654; 
SN 1539-9087
EI 1558-3465
PD OCT
PY 2021
VL 20
IS 5
SU S
AR 71
DI 10.1145/3477002
UT WOS:000699999600022
ER

PT J
AU Rafaely, B
   Tourbabin, V
   Habets, E
   Ben-Hur, Z
   Lee, H
   Gamper, H
   Arbel, L
   Birnie, L
   Abhayapala, T
   Samarasinghe, P
AF Rafaely, Boaz
   Tourbabin, Vladimir
   Habets, Emanuel
   Ben-Hur, Zamir
   Lee, Hyunkook
   Gamper, Hannes
   Arbel, Lior
   Birnie, Lachlan
   Abhayapala, Thushara
   Samarasinghe, Prasanga
TI Spatial audio signal processing for binaural reproduction of recorded
   acoustic scenes - review and challenges
SO ACTA ACUSTICA
AB Spatial audio has been studied for several decades, but has seen much renewed interest recently due to advances in both software and hardware for capture and playback, and the emergence of applications such as virtual reality and augmented reality. This renewed interest has led to the investment of increasing efforts in developing signal processing algorithms for spatial audio, both for capture and for playback. In particular, due to the popularity of headphones and earphones, many spatial audio signal processing methods have dealt with binaural reproduction based on headphone listening. Among these new developments, processing spatial audio signals recorded in real environments using microphone arrays plays an important role. Following this emerging activity, this paper aims to provide a scientific review of recent developments and an outlook for future challenges. This review also proposes a generalized framework for describing spatial audio signal processing for the binaural reproduction of recorded sound. This framework helps to understand the collective progress of the research community, and to identify gaps for future research. It is composed of five main blocks, namely: the acoustic scene, recording, processing, reproduction, and perception and evaluation. First, each block is briefly presented, and then, a comprehensive review of the processing block is provided. This includes topics from simple binaural recording to Ambisonics and perceptually motivated approaches, which focus on careful array configuration and design. Beamforming and parametric-based processing afford more flexible designs and shift the focus to processing and modeling of the sound field. Then, emerging machine- and deep-learning approaches, which take a further step towards flexibility in design, are described. Finally, specific methods for signal transformations such as rotation, translation and enhancement, enabling additional flexibility in reproduction and improvement in the quality of the binaural signal, are presented. The review concludes by highlighting directions for future research.
RI Habets, Emanuel/F-7298-2011
OI Habets, Emanuel/0000-0002-2613-8046
EI 2681-4617
PD OCT 12
PY 2022
VL 6
AR 47
DI 10.1051/aacus/2022040
UT WOS:000866173100001
ER

PT J
AU Iyer, V
   Yang, ZJ
   Ko, JN
   Weissleder, R
   Issadore, D
AF Iyer, Vasant
   Yang, Zijian
   Ko, Jina
   Weissleder, Ralph
   Issadore, David
TI Advancing microfluidic diagnostic chips into clinical use: a review of
   current challenges and opportunities
SO LAB ON A CHIP
AB Microfluidic diagnostic (mu DX) technologies miniaturize sensors and actuators to the length-scales that are relevant to biology: the micrometer scale to interact with cells and the nanometer scale to interrogate biology's molecular machinery. This miniaturization allows measurements of biomarkers of disease (cells, nanoscale vesicles, molecules) in clinical samples that are not detectable using conventional technologies. There has been steady progress in the field over the last three decades, and a recent burst of activity catalyzed by the COVID-19 pandemic. In this time, an impressive and ever-growing set of technologies have been successfully validated in their ability to measure biomarkers in clinical samples, such as blood and urine, with sensitivity and specificity not possible using conventional tests. Despite our field's many accomplishments to date, very few of these technologies have been successfully commercialized and brought to clinical use where they can fulfill their promise to improve medical care. In this paper, we identify three major technological trends in our field that we believe will allow the next generation of mu Dx to have a major impact on the practice of medicine, and which present major opportunities for those entering the field from outside disciplines: 1. the combination of next generation, highly multiplexed mu Dx technologies with machine learning to allow complex patterns of multiple biomarkers to be decoded to inform clinical decision points, for which conventional biomarkers do not necessarily exist. 2. The use of micro/nano devices to overcome the limits of binding affinity in complex backgrounds in both the detection of sparse soluble proteins and nucleic acids in blood and rare circulating extracellular vesicles. 3. A suite of recent technologies that obviate the manual pre-processing and post-processing of samples before they are measured on a mu DX chip. Additionally, we discuss economic and regulatory challenges that have stymied mu Dx translation to the clinic, and highlight strategies for successfully navigating this challenging space.
RI YANG, ZIJIAN/GRS-4433-2022
OI Issadore, David/0000-0002-5461-8653
SN 1473-0197
EI 1473-0189
PD AUG 23
PY 2022
VL 22
IS 17
BP 3110
EP 3121
DI 10.1039/d2lc00024e
EA JUN 2022
UT WOS:000807548700001
PM 35674283
ER

PT J
AU Hogstedt, K
   Kimelman, D
   Rajan, VT
   Roth, T
   Wegman, M
   Wang, N
AF Hogstedt, K
   Kimelman, D
   Rajan, VT
   Roth, T
   Wegman, M
   Wang, N
TI Optimizing component interaction
SO ACM SIGPLAN NOTICES
CT 1st Workshop on Optimization of Middleware and Distributed Systems
CY JUN   18, 2001
CL SNOWBIRD, UTAH
AB Interactions among two or more components may consist of passing of (large) data, a continuation, or a remote method call. Optimizing or minimizing the cost of such interactions is a fundamental problem in generating efficient code for component-oriented programs. This paper lays out a foundation for modeling and optimizing the interaction between components, with the goal of optimizing the entire assembled system. Our approach consists of automatically choosing, among a number of potential candidate, implementation strategies for choice of data structures, communication mechanisms, data placement, etc., components may use during their life time.
   By building a high-level optimizer that has dynamic knowledge of how components will be used in a system, we may achieve performance improvements of an order of magnitude or more. In our approach, a component writer provides the alternative implementation strategies, and our analysis will recommend or use the best strategies among the alternative during execution. One way to specify different alternatives is to recognize the characterization of problem domain. A more efficient code may be generated if we recognize the dynamic nature of the component usage. For instance, at one point during execution a component may choose one alternative strategy that is different from the choice it makes at another program point. In general, information needed to decide when to use one implementation over another may not be available to a component writer, but it is available when components are put to use in a system (for instance, the knowledge about objects are allocated, say for locality analysis, is only available at runtime).
   Sometimes choosing among the alternative implementations can especially tricky because the different choices are not independent of each other. This means an implementation choice that works well for one component may hurt the performance of another. This problem is particularly troublesome for component writers, since they have an extremely clear view of one component but cannot see the rest. For example, programs which work on the net will generally do better with ASCII strings, whereas programs that work heavily with databases might do better with EBCDIC -- and programs which do both (or systems in which the two kinds of program coexist) have to choose between them, or pay the cost of translation. Similarly, distributed objects that interact extensively should be on the same machine, although it does not matter on which machine they reside. Many of these optimizations can be reduced to a graph-cutting problem in which nodes in the graph correspond to objects and edges and its weights correspond to their interact. To optimize the interaction, we need to find a min-cut on the graph, and in general the min-cut problem is NP hard.
   We give heuristics that often give an almost linear time performance on actual instances of the problem. We present a vision for how future high-level optimizations that can optimize programs as they are written today, eliminating some of the inefficiencies caused by the high level component based programming. In the full paper we present theoretical analysis and case studies that show the feasibility of this approach, however much research remains to be done to achieve the promise.
SN 0362-1340
PD AUG
PY 2001
VL 36
IS 8
BP 181
EP 181
DI 10.1145/384196.384221
UT WOS:000171010100023
ER

PT J
AU Houborg, R
   McCabe, MF
AF Houborg, Rasmus
   McCabe, Matthew F.
TI Adapting a regularized canopy reflectance model (REGFLEC) for the
   retrieval challenges of dryland agricultural systems
SO REMOTE SENSING OF ENVIRONMENT
AB A regularized canopy reflectance model (REGFLEC) is applied over a dryland irrigated agricultural system in Saudi Arabia for the purpose of retrieving leaf area index (LAI) and leaf chlorophyll content (Ch11). To improve the robustness of the retrieved properties, REGFLEC was modified to 1) correct for aerosol and adjacency effects, 2) consider foliar dust effects on modeled canopy reflectances, 3) include spectral information in the red-edge wavelength region, and 4) exploit empirical LAI estimates in the model inversion. Using multi-spectral RapidEye imagery allowed Chl(1) to be retrieved with a Mean Absolute Deviation (MAD) of 7.9 mu g cm(-2) (16%), based upon in situ measurements conducted in fields of alfalfa, Rhodes grass and maize over the course of a growing season. LAI and Chl1 compensation effects on canopy reflectance were largely avoided by informing the inversion process with ancillary LAI inputs established empirically on the basis of a statistical machine learning technique. As a result, LAI was reproduced with good accuracy, with an overall MAD of 0.42 m(2) ITI-2 (12.5%). Results highlighted the considerable challenges associated with the translation of at-sensor radiance observations to surface bidirectional reflectances in dryland environments, where issues such as high aerosol loadings and large spatial gradients in surface reflectance from bright desert soils to dark vegetated fields are often present. Indeed, surface reflectances in the visible bands were reduced by up to 60% after correction for such adjacency effects. In addition, dust deposition on leaves required explicit modification of the reflectance sub-model to account for its influence. By implementing these model refinements, REGFLEC demonstrated its utility for within-field characterization of vegetation conditions over the challenging landscapes typical of dryland agricultural regions, offering a means through which improvements can be made in the management of these globally important systems. (C) 2016 Elsevier Inc. All rights reserved.
RI McCabe, Matthew F/G-5194-2011
OI McCabe, Matthew F/0000-0002-1279-5272; Houborg,
   Rasmus/0000-0002-3604-0747
SN 0034-4257
EI 1879-0704
PD DEC 1
PY 2016
VL 186
BP 105
EP 120
DI 10.1016/j.rse.2016.08.017
UT WOS:000396382500008
ER

PT J
AU Griffin, J
   Chen, X
AF Griffin, James
   Chen, Xun
TI Real-time simulation of neural network classifications from
   characteristics emitted by acoustic emission during horizontal single
   grit scratch tests
SO JOURNAL OF INTELLIGENT MANUFACTURING
AB With an increase of exotic materials and the machine process often associated with obtaining tight tolerances is that of grinding. There is an increased need in understanding the fundamental mechanics to be able to accurately model such material and grit interactions. For this reason the unit event of material interaction in grinding are investigated. Where three phenomenon's are involved namely: rubbing, ploughing and cutting. Ploughing and rubbing essentially mean the energy is being applied less efficiently in terms of material removal. Such phenomenon usually occurs before or after cutting. Based on this distinction it is important to identify the effects of these different phenomena experienced during grinding. Two acoustic emission (AE) sensors were used to extract the very fast, transient material and grit interaction to correlate with the measured material surface. Accurate material surface profile measurements of the cut groove were made using the Fogale Photomap Profiler which enables the comparison between the corresponding AE signal scratch data. Short-time Fourier transforms and filtration techniques ensured the translation and salient components for identification were ready for classification ensuring the distinct levels between the three different Grit (SG) phenomenons. Neural networks (NNs) was used to classify and verify the demarcation of SG phenomena. After the cutting, ploughing and rubbing gave a high confidence in terms of classification accuracy. To map the results from the unit/micro to the multi/macro event both 1 mu m and 0.1mm grinding test data were applied to the NN for classification. Interesting output results correlated for both classifiers signifying a distinction that there is more cutting utilisation than both ploughing and rubbing as the interaction between grit and workpiece become more involved (measured depth of cut increases). Such findings were then realised into a Simulink model as a potential control system for industrial purposes and a potential model for mapping the micro and macro mechanics seen in grinding technologies.
RI Chen, Xun/AAI-8314-2020; Griffin, James/H-9512-2013
OI Chen, Xun/0000-0003-2547-9022; Griffin, James/0000-0002-9179-5130
SN 0956-5515
EI 1572-8145
PD JUN
PY 2016
VL 27
IS 3
BP 507
EP 523
DI 10.1007/s10845-014-0883-x
UT WOS:000379231600002
ER

PT J
AU Sharadgah, TA
   Sa'di, RA
AF Sharadgah, Talha A.
   Sa'di, Rami A.
TI A SYSTEMATIC REVIEW OF RESEARCH ON THE USE OF ARTIFICIAL INTELLIGENCE IN
   ENGLISH LANGUAGE TEACHING AND LEARNING (2015-2021): WHAT ARE THE CURRENT
   EFFECTS?
SO JOURNAL OF INFORMATION TECHNOLOGY EDUCATION-RESEARCH
AB Aim/Purpose This study carried out a systematic review of the literature on artificial intelli-gence (AI) in English language teaching (ELT). The objective was to delineate the current research progress in the field and to further understand the chal-lenges.
   Background The study analyzed articles published between 2015 and 2021.
   Methodology The qualitative research method was employed. Five steps were taken to steer the review. 200 articles were scrutinized; 64 were retained.
   Contribution Prior research on AI in ELT has not investigated how the literature is progress-ing or what areas of AI are being covered. Without a holistic picture, some im-portant research findings could be missed. Understanding how studies on AI in ELT are designed and implemented will contribute to a greater understanding of the existing state of research.
   Findings Findings show that there is a promising future for AI in ELT. AI in ELT yielded positive results in terms of optimizing the English language skills, translation, assessment, recognition, attitude, satisfaction, etc. It was also found out that more and more articles on the topic are being published; the mixed research method is the most commonly used, higher education level is the most sampled, students as participants are the most sampled, and most studies developed novel AI-based systems. Various AI approaches have been identified in the re-viewed studies, including machine learning, neural network, support vector ma-chine, genetic algorithms, deep learning, decision tree, expert system, natural language processing, data mining, cloud computing, and edge computing. How-ever, AI in ELT is still in its infancy, where little research has been conducted and gaps in the literature are still present, especially in terms of inherent issues related to body language, gestures, expressions, emotions, translation, lack of elaborate description of teaching material used for learning driven by AI, uncer-tainties and vagueness with regards to what can be considered under the realm of AI, and most authors being outside of the ELT discipline.
   Recommendations for Practitioners Recommendations for Researchers This literature review is likely to provide practitioners with an overview of the current adopted technology, research method, instruments and/or tools, educa-tional level, language skill, and the effects reported by the AI-based studies for designing effective systems for the use of AI in their ELT classrooms. Researchers need to conduct research on AI in ELT along with a detailed in-depth description of the methodology, research design, and the proposed sys-tems used to achieve AI in ELT. Furthermore, it is recommended that research-ers explore the efficiency of AI-based systems used in previous research and ensure their relevance and functionality. They are also required to provide in-depth analysis of the challenges inherent to systems that have been highlighted in the literature, which will maximize the potentials of these AI-based technolo-gies.
   Impact on Society The findings of this paper can provide visualization of research findings that could particularly benefit researchers, educators, and AI specialists who are in-volved in the study of the applications of AI in ELT.
   Future Research Future AI research needs to seriously include more detailed descriptions of the method in further research.
RI Al-Sadi, Rami/GXV-8469-2022
OI Al-Sadi, Rami/0000-0002-6205-6144
SN 1547-9714
EI 1539-3585
PY 2022
VL 21
BP 337
EP 377
DI 10.28945/4999
UT WOS:000830043300001
ER

PT J
AU Rashid, M
   Bari, BS
   Hasan, MJ
   Razman, MAM
   Musa, RM
   Ab Nasir, AF
   Majeed, APPA
AF Rashid, Mamunur
   Bari, Bifta Sama
   Hasan, Md Jahid
   Razman, Mohd Azraai Mohd
   Musa, Rabiu Muazu
   Ab Nasir, Ahmad Fakhri
   Majeed, Anwar P. P. Abdul
TI The classification of motor imagery response: an accuracy enhancement
   through the ensemble of random subspace k-NN
SO PEERJ COMPUTER SCIENCE
AB Brain-computer interface (BCI) is a viable alternative communication strategy for patients of neurological disorders as it facilitates the translation of human intent into device commands. The performance of BCIs primarily depends on the efficacy of the feature extraction and feature selection techniques, as well as the classification algorithms employed. More often than not, high dimensional feature set contains redundant features that may degrade a given classifier's performance. In the present investigation, an ensemble learning-based classification algorithm, namely random subspace k-nearest neighbour (k-NN) has been proposed to classify the motor imagery (MI) data. The common spatial pattern (CSP) has been applied to extract the features from the MI response, and the effectiveness of random forest (RF)-based feature selection algorithm has also been investigated. In order to evaluate the efficacy of the proposed method, an experimental study has been implemented using four publicly available MI dataset (BCI Competition III dataset 1 (data-1), dataset IIIA (data-2), dataset IVA (data-3) and BCI Competition IV dataset II (data-4)). It was shown that the ensemble-based random subspace k-NN approach achieved the superior classification accuracy (CA) of 99.21%, 93.19%, 93.57% and 90.32% for data-1, data-2, data-3 and data-4, respectively against other models evaluated, namely linear discriminant analysis, support vector machine, random forest, Naive Bayes and the conventional k-NN. In comparison with other classification approaches reported in the recent studies, the proposed method enhanced the accuracy by 2.09% for data-1, 1.29% for data-2, 4.95% for data-3 and 5.71% for data-4, respectively. Moreover, it is worth highlighting that the RF feature selection technique employed in the present study was able to significantly reduce the feature dimension without compromising the overall CA. The outcome from the present study implies that the proposed method may significantly enhance the accuracy of MI data classification.
RI Rashid, Mamunur/AHA-5219-2022; Mohd Razman, Mohd Azraai/GRE-7620-2022;
   Musa, Rabiu Muazu/AAG-1683-2020
OI Rashid, Mamunur/0000-0003-4958-6041; Musa, Rabiu
   Muazu/0000-0001-5332-1770; Mohd Razman, Mohd Azraai/0000-0002-3045-2804;
   Bari, Bifta Sama/0000-0001-6102-1823
EI 2376-5992
PD MAR 2
PY 2021
AR e374
DI 10.7717/peerj-cs.374
UT WOS:000624304100001
PM 33817022
ER

PT J
AU Naiemi, F
   Ghods, V
   Khalesi, H
AF Naiemi, Fatemeh
   Ghods, Vahid
   Khalesi, Hassan
TI An efficient character recognition method using enhanced HOG for spam
   image detection
SO SOFT COMPUTING
AB Generally, a spam image is an unsolicited message electronically sent to a wide group of arbitrary addresses. Due to attractiveness and more difficult detection, spam images are the most complicated type of spam. One of the ways to encounter the spam images is an optical character recognition, OCR, method. In this paper, the proposed enhanced HOG feature extraction method has been used so that the optical character recognition system of spam has been enhanced by using the HOG feature extraction method in such a way to be both resistant against the character variations on scale and translation and to be computationally cost-effective. For these purposes, two steps of the cropped image and input image size normalization have been added to pre-processing stages. Support vector machine, SVM, was employed for classification. Two heuristic modifications including thickening of the thin characters in the pre-processing stage and non-discrimination in detecting the uppercase and lowercase letters with the same shapes in the classification stage have been also proposed to increase the system recognition accuracy. In the first heuristic modification, when all pixels of the output image are empty (the character is eliminated), the original image was made thicker by one layer. In the second modification, when recognizing the letters, no differentiation was considered between the uppercase and lowercase letters with the same shapes. An average recognition accuracy of the modified HOG method with two heuristic modifications equals 91.61% on Char74K database. Then, an optimum threshold for classification was investigated by ROC curve. The optimal cutoff point was 0.736 with the highest average accuracy, 94.20%, and AUC, area under curve, for ROC and precision-recall, PR, curves were 0.96 and 0.73, respectively. The proposed method was also examined on ICDAR2003 database, and the average accuracy and its optimum using ROC curve were 82.73% and 86.01%, respectively. These results of recognition accuracy and AUC for ROC and PR curve showed an outstanding enhancement in comparison with the best recognition rate of the previous methods.
RI Naiemi, fatemeh/AAR-7469-2020
OI Naiemi, fatemeh/0000-0001-5571-8560; Khalesi, Hassan/0000-0002-2684-1890
SN 1432-7643
EI 1433-7479
PD NOV
PY 2019
VL 23
IS 22
BP 11759
EP 11774
DI 10.1007/s00500-018-03728-z
UT WOS:000490958700025
ER

PT J
AU Kismet, Y
   Wagner, MH
AF Kismet, Y.
   Wagner, M. H.
TI Utilizing hydrolyzed powder recyclates as filler in polystyrene
SO MATERIALWISSENSCHAFT UND WERKSTOFFTECHNIK
AB In the present study, hydrolyzed powder coating wastes were used as filler materials in polystyrene compounds, and the effect on the mechanical properties, melt flow index and density of polystyrene compounds were investigated. Epoxy/polyester, epoxy and polyurethane systems with thermoset structure were used as powder coating recyclates. After separately hydrolyzing these wastes, 5%, 10%, 20% and 30% by weight were homogeneously mixed with polystyrene, initially mechanically and subsequently with an extruder in the melt at 180 degrees C. The resulting compounds were cooled and granulated, and by use of an injection molding machine at 180 degrees C to 200 degrees C, standard tensile test bars were produced. Tensile strength, bending strength and izod impact strength of the test bars were measured and the variations of the mechanical properties of the polystyrene compounds as a function of the type and amount of filler were examined. In addition, melt flow index and density of the compounds were determined. Furthermore, the bonding mechanism of filler and matrix material was examined by electron microscopy.
   Translation abstract In der vorliegenden Arbeit wurden hydrolysierte Pulverlackrecyclate als Fullstoffe in Polystyrol eingesetzt und die mechanischen Eigenschaften, der Schmelzindex und die Dichte der hergestellten Zugproben untersucht. Als Pulverlackrecyclate wurden Epoxid/Polyester-, Epoxid- und Polyurethansysteme mit duroplastischer Struktur verwendet. Nachdem diese Restmaterialen getrennt voneinander hydrolysiert wurden, wurden jeweils 5Gew.-%, 10Gew.-%, 20Gew.-% und 30Gew.-% mit Polystyrol zunachst mechanisch und anschlie ss end mit einem Extruder bei 180 degrees C in der Schmelze homogen gemischt. Die mit dem Extruder hergestellten Mischungen wurden abgegekuhlt und dann granuliert. Die Granulate wurden in einer Spritzgie ss maschine bei 180 degrees C bis 200 degrees C zu Standard-Zugprufstaben verarbeitet. Zugfestigkeit, Biegefestigkeit und Izod-Schlagzahigkeit der Prufstabe wurden bestimmt und die Variation der mechanischen Eigenschaften der gefullten Polystyrolproben in Abhangigkeit von der Art und der Menge des Fullstoffs ermittelt. Zusatzlich wurde die Abhangigkeit des Schmelzindexes und der Dichte der Mischungen vom Fullstoffgehalt bestimmt. Daruber hinaus wurde die Einbindung des Fullstoffs in die Polymermatrix mit Hilfe der Elektronenmikroskopie untersucht.
RI Wagner, Manfred H./AAM-6055-2020
OI Wagner, Manfred H./0000-0002-1815-7060
SN 0933-5137
EI 1521-4052
PD JAN
PY 2019
VL 50
IS 1
BP 25
EP 32
DI 10.1002/mawe.201700270
UT WOS:000459320200003
ER

PT C
AU Hummel, J
   Figl, M
   Schmidbauer, J
   Tinzl, M
   Bergmann, H
   Birkfellner, W
AF Hummel, Johann
   Figl, Michael
   Schmidbauer, Joerg
   Tinzl, Martina
   Bergmann, Helmar
   Birkfellner, Wolfgang
BE Cleary, KR
   Miga, MI
TI Motion correction for radiation therapy of prostate using B-mode
   Ultrasound
SO MEDICAL IMAGING 2007: VISUALIZATION AND IMAGE-GUIDED PROCEDURES, PTS 1
   AND 2
SE Proceedings of SPIE
CT Medical Imaging 2007 Conference
CY FEB 18-20, 2007
CL San Diego, CA
SP SPIE, Amer Assoc Physicists, Amer Physiol Soc, Comp Assisted Radiol & Surg, Soc Imaging Sci & Technol, Med Image Percept Soc, Radiol Soc N Amer, Soc Imaging Informat Med, Soc Mol Imaging, DICOM Standards Comm
AB The use of intensity modulated radiation therapy promises to spare organs at risk by applying better dose distribution on the tumor. The specific challenge of this methods is the exact positioning of the patient and the localization of the exposured organ. With respect to the filling of rectum and bladder the prostate can move several millimeters up to centimeters. Therfore, the position of the prostate should be determinated and corrected daily before irradiation. We used a B-mode US machine ( Ultramark 9, advanced Technology Laboratories, USA) which was calibrated using an optical tracking system (Polaris, NDI, Can). After correct positioning of the patient in the simulation room three anatomical markers (apex prostate, prostate lateral sinister/dexter) were identified and their positions calculated with respect to the coordinate system of the simulator. The same situation is given in the treatment room. Both, simulator and accelerator are registered by a simple point-to-point registration using a block with five drilled holes with known coordinates in the block coordinate system. The block is aligned by means of laser markers. When the patient is placed on the treatment table, the three anatomical landmarks are located on the US images and their positions are calculated with respect to the coordinate system of the treatment room. Applying a point-to-point registration results in a rotation matrix and a translation vector in the desired coordinate system which can be used for repositioning by translating and rotating the patient table. Additionally, a fiducial registration error (FRE) is calculated which gives a dimension of the accuracy the three points were identified. We found an fiducial registration error (FRE) of 2.4 mm +/- 1.2 mm. for the point-to-point registration of the anatomical landmarks. The FRE for the point-to-point registration between the block and the optical tracking system was 0.5 mm +/- 0.2 mm. According to the US calibration we found an error of 0.8 mm +/- 0.2 mm.
RI Birkfellner, Wolfgang/ABH-8946-2020
OI Birkfellner, Wolfgang/0000-0002-1470-2041; Figl,
   Michael/0000-0002-1745-0871
SN 0277-786X
BN 978-0-8194-6627-3
PY 2007
VL 6509
AR 65092F
DI 10.1117/12.708190
PN 1-2
UT WOS:000247294800086
ER

PT C
AU Stromberg, N
   Massana, JF
AF Stromberg, N
   Massana, JF
BE DeHosson, JTM
   Brebbia, CA
   Nishida, SI
TI A soft contact formulation for modelling thin coatings
SO Computer Methods and Experimental Measurements for Surface Effects and
   Contact Mechanics VII
SE WIT TRANSACTIONS ON ENGINEERING SCIENCES
CT 7th International Conference on Computer Methods and Experimental
   Measurements for Surface Effects and Contact Mechanics
CY SEP, 2005
CL Bologna, ITALY
SP Wessex Inst Technol, Univ Groningen, WIT Transact Engn Sci
AB Today, high-performance machine components are often improved by coatings. For instance, thin layers can increase the life-time by reducing friction and wear. An example of this is the spline joint in the suspension system of a truck. This joint, which is placed at the output of the gearbox, allows relative translation in the axial direction. In order to improve performance, this spline can be coated with a thin layer of polyamide. In such a manner, the life-time of the joint is increased by decreasing friction and wear. This type of coatings may be optimized by performing finite element contact analysis. However, when performing such an analysis, it can be difficult to obtain a good mesh due to the very thin layer. An approach to avoid this difficulty is to instead include the elastic properties of the coating in the contact formulation. Such a soft contact formulation is suggested and solved in this paper. The formulation is obtained by adding an elastic part to the free energy corresponding to Signorini's contact conditions. In such a manner a new soft contact law is derived by taking the subdifferential of the free energy. In this law two new constitutive parameters appear. The first parameter describes the elastic response of the thin layer and the second one is taken to be equal to the thickness of the layer. In the event, when the layer is completely penetrated, hard contact is developed following a classical Lagrange formulation of Signorini. A numerical method for solving the new contact formulation is also developed. The method is obtained by following the augmented Lagrangian approach. In such a way an equivalent setting of equations is derived which in turn is solved by using a non-smooth Newton method. The method is implemented in a Matlab toolbox. The method is robust and produces accurate results. This is shown by comparing numerical results with solutions obtained by using a penalty formulation in Abaqus.
SN 1746-4471
BN 1-84564-022-5
PY 2005
VL 49
BP 211
EP 218
UT WOS:000235463100021
ER

PT J
AU Ali, L
   Jassmi, HA
   Khan, W
   Alnajjar, F
AF Ali, Luqman
   Jassmi, Hamad Al
   Khan, Wasif
   Alnajjar, Fady
TI Crack45K: Integration of Vision Transformer with Tubularity Flow Field
   (TuFF) and Sliding-Window Approach for Crack-Segmentation in Pavement
   Structures
SO BUILDINGS
AB Recently, deep-learning (DL)-based crack-detection systems have proven to be the method of choice for image processing-based inspection systems. However, human-like generalization remains challenging, owing to a wide variety of factors such as crack type and size. Additionally, because of their localized receptive fields, CNNs have a high false-detection rate and perform poorly when attempting to capture the relevant areas of an image. This study aims to propose a vision-transformer-based crack-detection framework that treats image data as a succession of small patches, to retrieve global contextual information (GCI) through self-attention (SA) methods, and which addresses the CNNs' problem of inductive biases, including the locally constrained receptive-fields and translation-invariance. The vision-transformer (ViT) classifier was tested to enhance crack classification, localization, and segmentation performance by blending with a sliding-window and tubularity-flow-field (TuFF) algorithm. Firstly, the ViT framework was trained on a custom dataset consisting of 45K images with 224 x 224 pixels resolution, and achieved accuracy, precision, recall, and F1 scores of 0.960, 0.971, 0.950, and 0.960, respectively. Secondly, the trained ViT was integrated with the sliding-window (SW) approach, to obtain a crack-localization map from large images. The SW-based ViT classifier was then merged with the TuFF algorithm, to acquire efficient crack-mapping by suppressing the unwanted regions in the last step. The robustness and adaptability of the proposed integrated-architecture were tested on new data acquired under different conditions and which were not utilized during the training and validation of the model. The proposed ViT-architecture performance was evaluated and compared with that of various state-of-the-art (SOTA) deep-learning approaches. The experimental results show that ViT equipped with a sliding-window and the TuFF algorithm can enhance real-world crack classification, localization, and segmentation performance.
RI Alnajjar, Fady/CAF-5791-2022
OI Alnajjar, Fady/0000-0001-6102-3765; Ali, Luqman/0000-0001-5996-7804
EI 2075-5309
PD JAN
PY 2023
VL 13
IS 1
AR 55
DI 10.3390/buildings13010055
UT WOS:000914586700001
ER

PT J
AU Song, WY
   Robar, JL
   Moren, B
   Larsson, T
   Tedgren, AC
   Jia, X
AF Song, William Y.
   Robar, James L.
   Moren, Bjorn
   Larsson, Torbjern
   Tedgren, Asa Carlsson
   Jia, Xun
TI Emerging technologies in brachytherapy
SO PHYSICS IN MEDICINE AND BIOLOGY
AB Brachytherapy is a mature treatment modality. The literature is abundant in terms of review articles and comprehensive books on the latest established as well as evolving clinical practices. The intent of this article is to part ways and look beyond the current state-of-the-art and review emerging technologies that are noteworthy and perhaps may drive the future innovations in the field. There are plenty of candidate topics that deserve a deeper look, of course, but with practical limits in this communicative platform, we explore four topics that perhaps is worthwhile to review in detail at this time. First, intensity modulated brachytherapy (IMBT) is reviewed. The IMBT takes advantage of anisotropic radiation profile generated through intelligent high-density shielding designs incorporated onto sources and applicators such to achieve high quality plans. Second, emerging applications of 3D printing (i.e. additive manufacturing) in brachytherapy are reviewed. With the advent of 3D printing, interest in this technology in brachytherapy has been immense and translation swift due to their potential to tailor applicators and treatments customizable to each individual patient. This is followed by, in third, innovations in treatment planning concerning catheter placement and dwell times where new modelling approaches, solution algorithms, and technological advances are reviewed. And, fourth and lastly, applications of a new machine learning technique, called deep learning, which has the potential to improve and automate all aspects of brachytherapy workflow, are reviewed. We do not expect that all ideas and innovations reviewed in this article will ultimately reach clinic but, nonetheless, this review provides a decent glimpse of what is to come. It would be exciting to monitor as IMBT, 3D printing, novel optimization algorithms, and deep learning technologies evolve over time and translate into pilot testing and sensibly phased clinical trials, and ultimately make a difference for cancer patients. Today's fancy is tomorrow's reality. The future is bright for brachytherapy.
OI Moren, Bjorn/0000-0001-7191-5206; Carlsson Tedgren,
   Asa/0000-0002-4549-8303
SN 0031-9155
EI 1361-6560
PD DEC 7
PY 2021
VL 66
IS 23
AR 23TR01
DI 10.1088/1361-6560/ac344d
UT WOS:000721060300001
PM 34710856
ER

PT J
AU Nearchou, IP
   Ueno, H
   Kajiwara, Y
   Lillard, K
   Mochizuki, S
   Takeuchi, K
   Harrison, DJ
   Caie, PD
AF Nearchou, Ines P.
   Ueno, Hideki
   Kajiwara, Yoshiki
   Lillard, Kate
   Mochizuki, Satsuki
   Takeuchi, Kengo
   Harrison, David J.
   Caie, Peter D.
TI Automated Detection and Classification of Desmoplastic Reaction at the
   Colorectal Tumour Front Using Deep Learning
SO CANCERS
AB Simple Summary
   Desmoplastic reaction (DR) has previously been shown to be a promising prognostic factor in colorectal cancer (CRC). However, its manual reporting can be subjective and consequently consistency of reporting might be affected. The aim of our study was to develop a deep learning algorithm that would facilitate the objective and standardised DR assessment. By applying this algorithm on a CRC cohort of 528 patients, we demonstrate how deep learning methodologies can be used for the accurate and reproducible reporting of DR. Furthermore, this study showed that the prognostic significance of DR was superior when assessed through the use of the deep learning classifier than when assessed manually. In this study, we demonstrate how the application of machine learning approaches can help by not only identifying complex patterns present within histopathological images in a standardised and reproducible manner, but also report a more accurate patient stratification.
   The categorisation of desmoplastic reaction (DR) present at the colorectal cancer (CRC) invasive front into mature, intermediate or immature type has been previously shown to have high prognostic significance. However, the lack of an objective and reproducible assessment methodology for the assessment of DR has been a major hurdle to its clinical translation. In this study, a deep learning algorithm was trained to automatically classify immature DR on haematoxylin and eosin digitised slides of stage II and III CRC cases (n = 41). When assessing the classifier's performance on a test set of patient samples (n = 40), a Dice score of 0.87 for the segmentation of myxoid stroma was reported. The classifier was then applied to the full cohort of 528 stage II and III CRC cases, which was then divided into a training (n = 396) and a test set (n = 132). Automatically classed DR was shown to have superior prognostic significance over the manually classed DR in both the training and test cohorts. The findings demonstrated that deep learning algorithms could be applied to assist pathologists in the detection and classification of DR in CRC in an objective, standardised and reproducible manner.
RI ; Takeuchi, Kengo/C-3614-2008
OI Nearchou, Ines P./0000-0002-1863-5413; Caie, Peter/0000-0002-0031-9850;
   Takeuchi, Kengo/0000-0002-1599-5800; harrison, david/0000-0001-9041-9988
EI 2072-6694
PD APR
PY 2021
VL 13
IS 7
AR 1615
DI 10.3390/cancers13071615
UT WOS:000638359100001
PM 33807394
ER

PT C
AU Ebrahimipour, V
AF Ebrahimipour, Vahid
GP IEEE
TI Lexical Semantic Analysis to support Ontology Maintenance Modeling of
   FMEA
SO 2021 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING AND
   ENGINEERING MANAGEMENT (IEEE IEEM21)
SE International Conference on Industrial Engineering and Engineering
   Management IEEM
CT IEEE International Conference on Industrial Engineering and Engineering
   Management (IEEM)
CY DEC 13-16, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Singapore Sect, IEEE TEMS Singapore Chapter, IEEE TEMS Hong Kong Chapter
AB For text-based documents, word representations and meaning extraction play essential roles in knowledge modeling and presentation. A maintenance procedure comprises consecutive logical arguments for determining step-by-step cause-effect events, ideally resulting in mitigative tasks. Therefore, the context and meaning representation when mimicking the purpose of a maintenance procedure are highly dependent on the word sense, syntax-semantic interface, and semantic features of the argument. This paper proposes an event-based ontology approach for supporting a failure-mode-effect analysis (FMEA), based on a lexical semantic analysis and on identifying the meaning(s) of contextual text. Our approach constructs a straightforward lexical semantic approach for analyzing the semantic and syntactic features of the contextual structures of maintenance reports, so as to facilitate translation and interpretation for knowledge-based reasoning in the format of an FMEA. Then, the knowledge is converted into a computer- understandable representation with less heterogeneity and ambiguity. The methodology enables users to obtain a representation format that maximizes shareability and accessibility, allowing for multi-purpose usage. First, it maps the argument structure into a causal event structure, in which an event is represented as a group of highly frequent contextual features or words logically linked together to shape structured arguments. Then, Dowty and Van Valin's decomposition model is employed in the format of [Event-State-Activity-Accomplishment-Result] to determine the syntax-sematic interface(s) and linking rules in the causal chain. In addition, Van Valin's model is used to differentiate between active and causative accomplishments for punctual/non-punctual changes of states in the causal chain. Finally, the metadata and/or hypernyms of causal events are represented, to accommodate ontology modeling for semantic extraction and cause-effect interpretation. We show how easily the result is converted to a computer-understandable document using Web Ontology Language & Resource Description Framework (W3C), thereby enriching interoperability and data exchange, explicit wording, multidimensional word representation, and contextual meaning extraction in machine processing.
SN 2157-3611
BN 978-1-6654-3771-4
PY 2021
BP 377
EP 382
DI 10.1109/IEEM50564.2021.9672817
UT WOS:000821855600074
ER

PT J
AU Wattarujeekrit, T
   Shah, PK
   Collier, N
AF Wattarujeekrit, T
   Shah, PK
   Collier, N
TI PASBio: predicate-argument structures for event extraction in molecular
   biology
SO BMC BIOINFORMATICS
AB Background: The exploitation of information extraction (IE), a technology aiming to provide instances of structured representations from free-form text, has been rapidly growing within the molecular biology (MB) research community to keep track of the latest results reported in literature. IE systems have traditionally used shallow syntactic patterns for matching facts in sentences but such approaches appear inadequate to achieve high accuracy in MB event extraction due to complex sentence structure. A consensus in the IE community is emerging on the necessity for exploiting deeper knowledge structures such as through the relations between a verb and its arguments shown by predicate-argument structure (PAS). PAS is of interest as structures typically correspond to events of interest and their participating entities. For this to be realized within IE a key knowledge component is the definition of PAS frames. PAS frames for non-technical domains such as newswire are already being constructed in several projects such as PropBank, VerbNet, and FrameNet. Knowledge from PAS should enable more accurate applications in several areas where sentence understanding is required like machine translation and text summarization. In this article, we explore the need to adapt PAS for the MB domain and specify PAS frames to support IE, as well as outlining the major issues that require consideration in their construction.
   Results: We introduce PASBio by extending a model based on PropBank to the MB domain. The hypothesis we explore is that PAS holds the key for understanding relationships describing the roles of genes and gene products in mediating their biological functions. We chose predicates describing gene expression, molecular interactions and signal transduction events with the aim of covering a number of research areas in MB. Analysis was performed on sentences containing a set of verbal predicates from MEDLINE and full text journals. Results confirm the necessity to analyze PAS specifically for MB domain.
   Conclusions: At present PASBio contains the analyzed PAS of over 30 verbs, publicly available on the Internet for use in advanced applications. In the future we aim to expand the knowledge base to cover more verbs and the nominal form of each predicate.
OI Collier, Nigel/0000-0002-7230-4164
SN 1471-2105
PD OCT 19
PY 2004
VL 5
AR 155
DI 10.1186/1471-2105-5-155
UT WOS:000225770100001
PM 15494078
ER

PT J
AU Ma, WQ
AF Ma, Wenqing
TI Artificial Intelligence-Assisted Decision-Making Method for Legal
   Judgment Based on Deep Neural Network
SO MOBILE INFORMATION SYSTEMS
AB With the arrival of the third revolution of artificial intelligence, the applications of artificial intelligence in the fields of automatic driving, image recognition, smart home, machine translation, medical services, e-sports, and so on can be seen everywhere, and topics about artificial intelligence are constantly emerging. Since 2017, the discussion on artificial intelligence in the field of law has become more and more active. In this context, the application of artificial intelligence in the field of legal judgment and the hypothetical system based on this technology in court judgment has also become the object of discussion from time to time. In this paper, based on the artificial intelligence decision-making method of the deep neural network, aiming at the three subtasks of legal judgment prediction, namely, crime prediction, law recommendation, and sentence prediction, a multi-task judgment prediction model BERT12multi and a sentence interval prediction model BERT-Text CNN are proposed, which improve the prediction accuracy and adopt the knowledge distillation strategy to compress the model parameters and improve the reasoning speed of the judgment model. Experiments on the CAIL2018 data set show that the performance of the deep neural network model in crime prediction and law recommendation tasks can be significantly improved by adopting the pre training model adaptive training, grouping focus loss, and gradient confrontation training strategies. Using a step-by-step sentence prediction strategy can realize the weight sharing of pre training model and make use of the prediction results of charges and laws in sentence prediction. The recall training-prediction strategy can avoid error accumulation and improve the accuracy of sentence prediction. By integrating the artificial intelligence decision-making method, the case reasoning speed can be greatly improved, the highest compressible model volume can be about 11% of the original one, and the reasoning speed can be increased by about 8 times. At the same time, performance close to that of the deep neural model can be obtained, which is superior to other legal decision prediction models based on word embedding.
SN 1574-017X
EI 1875-905X
PD OCT 11
PY 2022
VL 2022
AR 4636485
DI 10.1155/2022/4636485
UT WOS:000876512300024
ER

PT J
AU Andreozzi, JM
   Bruza, P
   Cammin, J
   Alexander, DA
   Pogue, BW
   Green, O
   Gladstone, DJ
AF Andreozzi, Jacqueline M.
   Bruza, Petr
   Cammin, Jochen
   Alexander, Daniel A.
   Pogue, Brian W.
   Green, Olga
   Gladstone, David J.
TI Optical emission-based phantom to verify coincidence of radiotherapy and
   imaging isocenters on an MR-linac
SO JOURNAL OF APPLIED CLINICAL MEDICAL PHYSICS
AB Purpose Demonstrate a novel phantom design using a remote camera imaging method capable of concurrently measuring the position of the x-ray isocenter and the magnetic resonance imaging (MRI) isocenter on an MR-linac. Methods A conical frustum with distinct geometric features was machined out of plastic. The phantom was submerged in a small water tank, and aligned using room lasers on a MRIdian MR-linac (ViewRay Inc., Cleveland, OH). The phantom physical isocenter was visualized in the MR images and related to the DICOM coordinate isocenter. To view the x-ray isocenter, an intensified CMOS camera system (DoseOptics LLC., Hanover, NH) was placed at the foot of the treatment couch, and centered such that the optical axis of the camera was coincident with the central axis of the treatment bore. Two or four 8.3mm x 24.1cm beams irradiated the phantom from cardinal directions, producing an optical ring on the conical surface of the phantom. The diameter of the ring, measured at the peak intensity, was compared to the known diameter at the position of irradiation to determine the Z-direction offset of the beam. A star-shot method was employed on the front face of the frustum to determine X-Y alignment of the MV beam. Known shifts were applied to the phantom to establish the sensitivity of the method. Results Couch translations, demonstrative of possible isocenter misalignments, on the order of 1mm were detectable for both the radiotherapy and MRI isocenters. Data acquired on the MR-linac demonstrated an average error of 0.28mm(N=10, R-2=0.997, sigma=0.37mm) in established Z displacement, and 0.10mm(N=5, sigma=0.34mm) in XY directions of the radiotherapy isocenter. Conclusions The phantom was capable of measuring both the MRI and radiotherapy treatment isocenters. This method has the potential to be of use in MR-linac commissioning, and could be streamlined to be valuable in daily constancy checks of isocenter coincidence.
OI Gladstone, David/0000-0002-4086-0297; Alexander,
   Daniel/0000-0003-3831-7479
SN 1526-9914
PD SEP
PY 2021
VL 22
IS 9
BP 252
EP 261
DI 10.1002/acm2.13377
EA AUG 2021
UT WOS:000687051900001
PM 34409766
ER

PT J
AU Malovani, C
   Friedman, N
   Ben-Eliezer, N
   Tavor, I
AF Malovani, Cfir
   Friedman, Naama
   Ben-Eliezer, Noam
   Tavor, Ido
TI Tissue Probability Based Registration of Diffusion-Weighted Magnetic
   Resonance Imaging
SO JOURNAL OF MAGNETIC RESONANCE IMAGING
AB Background Current registration methods for diffusion-MRI (dMRI) data mostly focus on white matter (WM) areas. Recently, dMRI has been employed for the characterization of gray matter (GM) microstructure, emphasizing the need for registration methods that consider all tissue types.
   Purpose To develop a dMRI registration method based on GM, WM, and cerebrospinal fluid (CSF) tissue probability maps (TPMs).
   Study Type Retrospective longitudinal study.
   Population Thirty-two healthy participants were scanned twice (legacy data), divided into a training-set (n = 16) and a test-set (n = 16), and 35 randomly-selected participants from the Human Connectome Project.
   Field Strength/Sequence 3.0T, diffusion-weighted spin-echo echo-planar sequence; T1-weighted spoiled gradient-recalled echo (SPGR) sequence.
   Assessment A joint segmentation-registration approach was implemented: Diffusion tensor imaging (DTI) maps were classified into TPMs using machine-learning approaches. The resulting GM, WM, and CSF probability maps were employed as features for image alignment. Validation was performed on the test dataset and the HCP dataset. Registration performance was compared with current mainstream registration tools.
   Statistical Tests Classifiers used for segmentation were evaluated using leave-one-out cross-validation and scored using Dice-index. Registration success was evaluated by voxel-wise variance, normalized cross-correlation of registered DTI maps, intra- and inter-subject similarity of the registered TPMs, and region-based intra-subject similarity using an anatomical atlas. One-way ANOVAs were performed to compare between our method and other registration tools.
   Results The proposed method outperformed mainstream registration tools as indicated by lower voxel-wise variance of registered DTI maps (SD decrease of 10%) and higher similarity between registered TPMs within and across participants, for all tissue types (Dice increase of 0.1-0.2; P < 0.05).
   Data Conclusion A joint segmentation-registration approach based on diffusion-driven TPMs provides a more accurate registration of dMRI data, outperforming other registration tools. Our method offers a "translation" of diffusion data into structural information in the form of TPMs, allowing to directly align diffusion and structural images.
   Technical Efficacy Stage: 1
RI Tavor, Ido/AAB-1345-2019; Ben-Eliezer, Noam/AAZ-8033-2021
OI Tavor, Ido/0000-0002-9117-4449; Ben-Eliezer, Noam/0000-0003-2944-6412
SN 1053-1807
EI 1522-2586
PD OCT
PY 2021
VL 54
IS 4
BP 1066
EP 1076
DI 10.1002/jmri.27654
EA APR 2021
UT WOS:000643146300001
PM 33894095
ER

PT J
AU Dappiaggi, C
   Moretti, V
   Pinamonti, N
AF Dappiaggi, Claudio
   Moretti, Valter
   Pinamonti, Nicola
TI Rigorous steps towards holography in asymptotically flat spacetimes
SO REVIEWS IN MATHEMATICAL PHYSICS
AB Scalar QFT on the boundary i+ at future null infinity of a general asymptotically flat 4D spacetime is constructed using the algebraic approach based on Weyl algebra associated to a BMS-invariant symplectic form. The constructed theory turns out to be invariant under a suitable strongly-continuous unitary representation of the BMS group with manifest meaning when the fields are interpreted as suitable extensions to i+ of massless minimally coupled fields propagating in the bulk. The group theoretical analysis of the found unitary BMS representation proves that such a field on i+ coincides with the natural wave function constructed out of the unitary BMS irreducible representation induced from the little group Delta, the semidirect product between SO(2) and the two-dimensional translations group. This wave function is massless with respect to the notion of mass for BMS representation theory. The presented result proposes a natural criterion to solve the long-standing problem of the topology of BMS group. Indeed the found natural correspondence of quantum field theories holds only if the BMS group is equipped with the nuclear topology rejecting instead the Hilbert one. Eventually, some theorems towards a holographic description on i+ of QFT in the bulk are established at level of C*-algebras of fields for asymptotically flat at null infinity spacetimes. It is proved that preservation of a certain symplectic form implies the existence of an injective *-homomorphism from the Weyl algebra of fields of the bulk into that associated with the boundary i+. Those results are, in particular, applied to 4D Minkowski spacetime where a nice interplay between Poincare invariance in the bulk and BMS invariance on the boundary at null infinity is established at the level of QFT. It arises that, in this case, the *-homomorphism admits unitary implementation and Minkowski vacuum is mapped into the BMS invariant vacuum on i+.
RI Pinamonti, Nicola/AAX-5377-2021; Moretti, Valter/J-5844-2012
OI Pinamonti, Nicola/0000-0002-6752-9799; moretti,
   Valter/0000-0001-6182-6613
SN 0129-055X
EI 1793-6659
PD MAY
PY 2006
VL 18
IS 4
BP 349
EP 415
DI 10.1142/S0129055X0600270X
UT WOS:000239260500001
ER

PT J
AU Wu, SH
   Huang, QH
   Zhao, L
AF Wu, Sihong
   Huang, Qinghua
   Zhao, Li
TI A deep learning-based network for the simulation of airborne
   electromagnetic responses
SO GEOPHYSICAL JOURNAL INTERNATIONAL
AB Airborne electromagnetic (AEM) method detects the subsurface electrical resistivity structure by inverting the measured electromagnetic field. AEM data inversion is extremely time-consuming when huge volumes of observational data are involved. Forward modelling is an essential part and represents a large proportion of computational cost in the inversion process. In this study, we develop an AEM simulator using deep learning as a computationally efficient alternative to accelerate 1-D forward modelling. Inspired by Google's neural machine translation, our AEM simulator adopts the long short-term memory (LSTM) modules with an encoder-decoder structure, combining the advantages in time-series regression and feature extraction. The well-trained LSTM network describes directly the mapping relationship between resistivity models with transceiver altitudes and time-domain AEM signals. The prediction results of the test set show that 95 per cent of the relative errors at most sampling points fall in the range of +/- 5 per cent, with average values within the range of +/- 0.5 per cent, indicating an overall prediction accuracy. We investigate the effects of the distributions of both resistivity and transceiver altitude in the training set on the prediction accuracy. The LSTM-based AEM simulator can effectively handle the resistivity characteristics involved in the training set and yields great sensitivity to the variations of transceiver altitudes. We also examine the adaptability of our AEM simulator for discontinuous resistivity variations. Synthetic tests indicate that the application effect of the AEM simulator relies on the completeness of the training samples and suggest that enriching the sample diversity is necessary to ensure the prediction accuracy, in cases of observation environments dominated by extreme transceiver altitudes or under-represented geological features. Furthermore, we discuss the influence of network configuration on its accuracy and computational efficiency. Our simulator can deliver similar to 13 600 1-D forward modelling calculations within 1 s, which significantly improves the simulation efficiency of AEM data.
OI Wu, Sihong/0000-0002-5584-664X; Zhao, Li/0000-0002-0950-6863
SN 0956-540X
EI 1365-246X
PD DEC 8
PY 2023
VL 233
IS 1
BP 253
EP 263
DI 10.1093/gji/ggac463
UT WOS:000894238900011
ER

PT J
AU Ding, YB
AF Ding, Yanbin
TI Effectiveness Evaluation Model of Students' English Listening Ability
   Based on Immersive Computing
SO MOBILE INFORMATION SYSTEMS
AB In the context of the integration of China's international economic development, the country has an increasing demand for high-level English professionals. Therefore, schools need to strengthen the teaching of students' English learning and improve students' comprehensive English ability. College English listening course is an important course to cultivate students' English skills. Improving the teaching effect of college English listening course has become an urgent problem in English teaching. With the development of China's network technology and education reform, a brand-new three-dimensional virtual immersive environment teaching model has emerged. This teaching model is of great help to students in learning English. This paper applies immersion theory to college English listening teaching, using virtual reality technology to create a three-dimensional virtual immersive environment for English listening teaching and using this new environment to promote English listening teaching, stimulate students' interest in English learning, and improve students' English. The ability of listening comprehensively improves the efficiency of English listening teaching, thereby making college English listening lessons lively and interesting and rich in information, and helps students have an immersive experience, so as to cultivate students' interest in learning, form learning motivation, and ultimately achieve the goal of improving their English proficiency. The method proposed in this article organically integrates the teaching, learning, and evaluation of English listening. First of all, it can evaluate the attribute grasping mode and its advantages and disadvantages of groups and individuals in the process of listening and cognition. Relying on the network, it can also provide real-time networked diagnosis reports and targeted guidance. Secondly, teachers can discover the characteristics and needs of individuals and groups based on the diagnosis results, adjust and optimize their own teaching methods, and truly teach students in accordance with their aptitude. More importantly, students can find problems through self-diagnosis and obtain effective guidance and intervention, which finally help them to move towards autonomous learning. This is also the ultimate goal of modern English education. From this perspective, this research also helps to open up new directions for future English research.
SN 1574-017X
EI 1875-905X
PD APR 23
PY 2022
VL 2022
AR 4302175
DI 10.1155/2022/4302175
UT WOS:000793372400001
ER

PT J
AU Saadi, A
   Belhadef, H
AF Saadi, Abdelhalim
   Belhadef, Hacene
TI Deep neural networks for Arabic information extraction
SO SMART AND SUSTAINABLE BUILT ENVIRONMENT
AB Purpose The purpose of this paper is to present a system based on deep neural networks to extract particular entities from natural language text, knowing that a massive amount of textual information is electronically available at present. Notably, a large amount of electronic text data indicates great difficulty in finding or extracting relevant information from them. Design/methodology/approach This study presents an original system to extract Arabic-named entities by combining a deep neural network-based part-of-speech tagger and a neural network-based named entity extractor. Firstly, the system extracts the grammatical classes of the words with high precision depending on the context of the word. This module plays the role of the disambiguation process. Then, a second module is used to extract the named entities. Findings Using deep neural networks in natural language processing, requires tuning many hyperparameters, which is a time-consuming process. To deal with this problem, applying statistical methods like the Taguchi method is much requested. In this study, the system is successfully applied to the Arabic-named entities recognition, where accuracy of 96.81 per cent was reported, which is better than the state-of-the-art results. Research limitations/implications - The system is designed and trained for the Arabic language, but the architecture can be used for other languages. Practical implications - Information extraction systems are developed for different applications, such as analysing newspaper articles and databases for commercial, political and social objectives. Information extraction systems also can be built over an information retrieval (IR) system. The IR system eliminates irrelevant documents and paragraphs.
   Originality/value The proposed system can be regarded as the first attempt to use double deep neural networks to increase the accuracy. It also can be built over an IR system. The IR system eliminates irrelevant documents and paragraphs. This process reduces the mass number of documents from which the authors wish to extract the relevant information using an information extraction system.
RI Saadi, Abdelahlim/AAQ-6375-2020
OI BELHADEF, HACENE/0000-0001-7313-5406
SN 2046-6099
EI 2046-6102
PD NOV 16
PY 2020
VL 9
IS 4
BP 467
EP 482
DI 10.1108/SASBE-03-2019-0031
EA APR 2020
UT WOS:000523369500001
ER

PT J
AU Rabbi, M
   Pfammatter, A
   Zhang, M
   Spring, B
   Choudhury, T
AF Rabbi, Mashfiqui
   Pfammatter, Angela
   Zhang, Mi
   Spring, Bonnie
   Choudhury, Tanzeem
TI Automated Personalized Feedback for Physical Activity and Dietary
   Behavior Change With Mobile Phones: A Randomized Controlled Trial on
   Adults
SO JMIR MHEALTH AND UHEALTH
AB Background: A dramatic rise in health-tracking apps for mobile phones has occurred recently. Rich user interfaces make manual logging of users'behaviors easier and more pleasant, and sensors make tracking effortless. To date, however, feedback technologies have been limited to providing overall statistics, attractive visualization of tracked data, or simple tailoring based on age, gender, and overall calorie or activity information. There are a lack of systems that can perform automated translation of behavioral data into specific actionable suggestions that promote healthier lifestyle without any human involvement.
   Objective: MyBehavior, a mobile phone app, was designed to process tracked physical activity and eating behavior data in order to provide personalized, actionable, low-effort suggestions that are contextualized to the user's environment and previous behavior. This study investigated the technical feasibility of implementing an automated feedback system, the impact of the suggestions on user physical activity and eating behavior, and user perceptions of the automatically generated suggestions.
   Methods: MyBehavior was designed to (1) use a combination of automatic and manual logging to track physical activity (eg, walking, running, gym), user location, and food, (2) automatically analyze activity and food logs to identify frequent and nonfrequent behaviors, and (3) use a standard machine-learning, decision-making algorithm, called multi-armed bandit (MAB), to generate personalized suggestions that ask users to either continue, avoid, or make small changes to existing behaviors to help users reach behavioral goals. We enrolled 17 participants, all motivated to self-monitor and improve their fitness, in a pilot study of MyBehavior. In a randomized two-group trial, investigators randomly assigned participants to receive either MyBehavior's personalized suggestions (n=9) or nonpersonalized suggestions (n=8), created by professionals, from a mobile phone app over 3 weeks. Daily activity level and dietary intake was monitored from logged data. At the end of the study, an in-person survey was conducted that asked users to subjectively rate their intention to follow MyBehavior suggestions.
   Results: In qualitative daily diary, interview, and survey data, users reported MyBehavior suggestions to be highly actionable and stated that they intended to follow the suggestions. MyBehavior users walked significantly more than the control group over the 3 weeks of the study (P=.05). Although some MyBehavior users chose lower-calorie foods, the between-group difference was not significant (P=.15). In a poststudy survey, users rated MyBehavior's personalized suggestions more positively than the nonpersonalized, generic suggestions created by professionals (P<.001).
   Conclusions: MyBehavior is a simple-to-use mobile phone app with preliminary evidence of efficacy. To the best of our knowledge, MyBehavior represents the first attempt to create personalized, contextualized, actionable suggestions automatically from self-tracked information (ie, manual food logging and automatic tracking of activity). Lessons learned about the difficulty of manual logging and usability concerns, as well as future directions, are discussed.
RI Pfammatter, Angela/AAZ-6036-2021
OI Pfammatter, Angela/0000-0003-0081-4090
SN 2291-5222
PD APR-JUN
PY 2015
VL 3
IS 2
AR e42
DI 10.2196/mhealth.4160
UT WOS:000359791800015
PM 25977197
ER

PT J
AU Lu, BZ
   Cheng, XL
   Huang, JF
   McCammon, JA
AF Lu, Benzhuo
   Cheng, Xiaolin
   Huang, Jingfang
   McCammon, J. Andrew
TI AFMPB: An adaptive fast multipole, Poisson-Boltzmann solver for
   calculating electrostatics in biomolecular systems
SO COMPUTER PHYSICS COMMUNICATIONS
AB A Fortran program package is introduced for rapid evaluation of the electrostatic potentials and forces in biomolecular systems modeled by the linearized Poisson-Boltzmann equation The numerical solver utilizes a well-conditioned boundary integral equation (BIE) formulation. a node-patch discretization scheme, a Krylov subspace iterative solver package with reverse communication protocols, and an adaptive new version of fast multipole method in which the exponential expansions are used to diagonalize the multipole-to-local translations The program and its full description, as well as several closely related libraries and utility tools are available at http //lsec cc ac cn/-lubz/afmpb html and a mirror site at http //mccammon ucsd edu/ This paper is a brief summary of the program the algorithms, the implementation and the usage
   Program summary
   Program title AFMPB Adaptive fast multipole Poisson-Boltzmann solver
   Catalogue identifier AEGB_v1_0
   Program summary URL http / lcpc cs qub ac uk/sunlmaries/AEGBvI.0 html
   Program obtainable from CPC Program Library. Queen's University, Belfast. N Ireland
   Licensing provisions GPL 2 0
   No of lines in distributed program, including test data, etc 453 649
   No of bytes in distributed program, including test data, etc 8 764 754
   Distribution format tar gz
   Programming language Fortran
   Computer Any
   Operating system Any
   RAM Depends on the size of the discretized biomolecular system
   Classification 3
   External routines Pre- and post-processing tools are required for generating the boundary elements and for visualization Users can use MSMS (hap //www scripps edu/-sanner/html/msms_home html) for preprocessing. and VMD (http //www ks unic eclu/Research/vmd/) for visualization S
   ub-programs included An iterative Krylov subspace solvers package from SPARSKIT by Yousef Saad (hap //www-users cs umn edu/-saad/software/SPARSKIT/sparskit html), and the fast multipole methods subroutines from FMMSuite (http //www.fastmultipole org/)
   Nature of problem Numerical solution of the linearized Poisson-Boltzmann equation that describes electrostatic interactions of molecular systems in ionic solutions
   Solution method A novel node-patch scheme is used to discretize the well-conditioned boundary integral equation formulation of the linearized Poisson-Boltzmann equation Various Krylov subspace solvers can be subsequently applied to solve the resulting linear system, with a bounded number of iterations independent of the number of discretized unknowns The matrix-vector multiplication at each iteration is accelerated by the adaptive new versions of fast multipole methods The AFMPB solver requires other stand-alone pre-processing tools for boundary mesh generation, post-processing tools for data analysis and visualization, and can be conveniently coupled with different time stepping methods for dynamics simulation
   Restrictions Only three or six significant digits options are provided in this version Unusual features Most of the codes are in Fortran77 style Memory allocation functions from Fortran90 and above are used in a few subroutines
   Additional comments The current version of the codes is designed and written for single core/processor desktop machines Check htip //lsec cc ac cn/-lubz/afmpb html and Imp //mccammon ucsd edu/ for updates and changes
   Running time The running time varies with the number of discretized elements (N) in the system and their distributions In most cases. it scales linearly as a function of N (C) 2010 Elsevier B V All rights reserved
RI McCammon, J. Andrew/AAG-1832-2021
OI McCammon, J. Andrew/0000-0003-3065-1456; Cheng,
   Xiaolin/0000-0002-7396-3225
SN 0010-4655
EI 1879-2944
PD JUN
PY 2010
VL 181
IS 6
BP 1150
EP 1160
DI 10.1016/j.cpc.2010.02.015
UT WOS:000277954400019
PM 20532187
ER

PT J
AU Ho, S
   Doig, GS
   Ly, A
AF Ho, Sharon
   Doig, Gordon S.
   Ly, Angelica
TI Attitudes of optometrists towards artificial intelligence for the
   diagnosis of retinal disease: A cross-sectional mail-out survey
SO OPHTHALMIC AND PHYSIOLOGICAL OPTICS
AB Purpose Artificial intelligence (AI)-based systems have demonstrated great potential in improving the diagnostic accuracy of retinal disease but are yet to achieve widespread acceptance in routine clinical practice. Clinician attitudes are known to influence implementation. Therefore, this study aimed to identify optometrists' attitudes towards the use of AI to assist in diagnosing retinal disease. Methods A paper-based survey was designed to assess general attitudes towards AI in diagnosing retinal disease and motivators/barriers for future use. Two clinical scenarios for using AI were evaluated: (1) at the point of care to obtain a diagnostic recommendation, versus (2) after the consultation to provide a second opinion. Relationships between participant characteristics and attitudes towards AI were explored. The survey was mailed to 252 randomly selected practising optometrists across Australia, with repeat mail-outs to non-respondents. Results The response rate was 53% (133/252). Respondents' mean (SD) age was 42.7 (13.3) years, and 44.4% (59/133) identified as female, whilst 1.5% (2/133) identified as gender diverse. The mean number of years practising in primary eye care was 18.8 (13.2) years with 64.7% (86/133) working in an independently owned practice. On average, responding optometrists reported positive attitudes (mean score 4.0 out of 5, SD 0.8) towards using AI as a tool to aid the diagnosis of retinal disease, and would be more likely to use AI if it is proven to increase patient access to healthcare (mean score 4.4 out of 5, SD 0.6). Furthermore, optometrists expressed a statistically significant preference for using AI after the consultation to provide a second opinion rather than during the consultation, at the point-of-care (+0.12, p = 0.01). Conclusions Optometrists have positive attitudes towards the future use of AI as an aid to diagnose retinal disease. Understanding clinician attitudes and preferences for using AI may help maximise its clinical potential and ensure its successful translation into practice.
OI Ho, Sharon/0000-0002-1975-828X; Ly, Angelica/0000-0001-7881-1522
SN 0275-5408
EI 1475-1313
PD NOV
PY 2022
VL 42
IS 6
BP 1170
EP 1179
DI 10.1111/opo.13034
EA AUG 2022
UT WOS:000835799400001
PM 35924658
ER

PT J
AU Waldmann, P
AF Waldmann, Patrik
TI A proximal LAVA method for genome-wide association and prediction of
   traits with mixed inheritance patterns
SO BMC BIOINFORMATICS
AB Background: The genetic basis of phenotypic traits is highly variable and usually divided into mono-, oligo- and polygenic inheritance classes. Relatively few traits are known to be monogenic or oligogeneic. The majority of traits are considered to have a polygenic background. To what extent there are mixtures between these classes is unknown. The rapid advancement of genomic techniques makes it possible to directly map large amounts of genomic markers (GWAS) and predict unknown phenotypes (GWP). Most of the multi-marker methods for GWAS and GWP falls into one of two regularization frameworks. The first framework is based on l(1)-norm regularization (e.g. the LASSO) and is suitable for mono- and oligogenic traits, whereas the second framework regularize with the l(2)-norm (e.g. ridge regression; RR) and thereby is favourable for polygenic traits. A general framework for mixed inheritance is lacking.
   Results: We have developed a proximal operator algorithm based on the recent LAVA regularization method that jointly performs l(1)- and l(2)-norm regularization. The algorithm is built on the alternating direction method of multipliers and proximal translation mapping (LAVA ADMM). When evaluated on the simulated QTLMAS2010 data, it is shown that the LAVA ADMM together with Bayesian optimization of the regularization parameters provides an efficient approach with lower test prediction mean-squared-error (65.89) than the LASSO (66.11), Ridge regression (83.41) and Elastic net (66.11). For the real pig data the test MSE of the LAVA ADMM is 0.850 compared to the LASSO, RR and EN with 0.875, 0.853 and 0.853, respectively.
   Conclusions: This study presents the LAVA ADMM that is capable of joint modelling of monogenic major genetic effects and polygenic minor genetic effects which can be used for both genome-wide assoiciation and prediction purposes. The statistical evaluations based on both simulated and real pig data set shows that the LAVA ADMM has better prediction properies than the LASSO, RR and EN. Julia code for the LAVA ADMM is available at: https://github.com/patwa67/LAVAADMM.
SN 1471-2105
PD OCT 26
PY 2021
VL 22
IS 1
AR 523
DI 10.1186/s12859-021-04436-6
UT WOS:000711433400001
PM 34702175
ER

PT J
AU Haggenmuller, S
   Maron, RC
   Hekler, A
   Utikal, JS
   Barata, C
   Barnhill, RL
   Beltraminelli, H
   Berking, C
   Betz-Stablein, B
   Blum, A
   Braun, SA
   Carr, R
   Combalia, M
   Fernandez-Figueras, MT
   Ferrara, G
   Fraitag, S
   French, LE
   Gellrich, FF
   Ghoreschi, K
   Goebeler, M
   Guitera, P
   Haenssle, HA
   Haferkamp, S
   Heinzerling, L
   Heppt, MV
   Hilke, FJ
   Hobelsberger, S
   Krahl, D
   Kutzner, H
   Lallas, A
   Liopyris, K
   Llamas-Velasco, M
   Malvehy, J
   Meier, F
   Muller, CSL
   Navarini, AA
   Navarrete-Dechent, C
   Perasole, A
   Poch, G
   Podlipnik, S
   Requena, L
   Rotemberg, VM
   Saggini, A
   Sangueza, OP
   Santonja, C
   Schadendorf, D
   Schilling, B
   Schlaak, M
   Schlager, JG
   Sergon, M
   Sondermann, W
   Soyer, HP
   Starz, H
   Stolz, W
   Vale, E
   Weyers, W
   Zink, A
   Krieghoff-Henning, E
   Kather, JN
   von Kalle, C
   Lipka, DB
   Frohling, S
   Hauschild, A
   Kittler, H
   Brinker, TJ
AF Haggenmueller, Sarah
   Maron, Roman C.
   Hekler, Achim
   Utikal, Jochen S.
   Barata, Catarina
   Barnhill, Raymond L.
   Beltraminelli, Helmut
   Berking, Carola
   Betz-Stablein, Brigid
   Blum, Andreas
   Braun, Stephan A.
   Carr, Richard
   Combalia, Marc
   Fernandez-Figueras, Maria-Teresa
   Ferrara, Gerardo
   Fraitag, Sylvie
   French, Lars E.
   Gellrich, Frank F.
   Ghoreschi, Kamran
   Goebeler, Matthias
   Guitera, Pascale
   Haenssle, Holger A.
   Haferkamp, Sebastian
   Heinzerling, Lucie
   V. Heppt, Markus
   Hilke, Franz J.
   Hobelsberger, Sarah
   Krahl, Dieter
   Kutzner, Heinz
   Lallas, Aimilios
   Liopyris, Konstantinos
   Llamas-Velasco, Mar
   Malvehy, Josep
   Meier, Friedegund
   Mueller, Cornelia S. L.
   Navarini, Alexander A.
   Navarrete-Dechent, Cristian
   Perasole, Antonio
   Poch, Gabriela
   Podlipnik, Sebastian
   Requena, Luis
   Rotemberg, Veronica M.
   Saggini, Andrea
   Sangueza, Omar P.
   Santonja, Carlos
   Schadendorf, Dirk
   Schilling, Bastian
   Schlaak, Max
   Schlager, Justin G.
   Sergon, Mildred
   Sondermann, Wiebke
   Soyer, H. Peter
   Starz, Hans
   Stolz, Wilhelm
   Vale, Esmeralda
   Weyers, Wolfgang
   Zink, Alexander
   Krieghoff-Henning, Eva
   Kather, Jakob N.
   von Kalle, Christof
   Lipka, Daniel B.
   Froehling, Stefan
   Hauschild, Axel
   Kittler, Harald
   Brinker, Titus J.
TI Original Research Skin cancer classification via convolutional neural
   networks: systematic review of studies involving human experts
SO EUROPEAN JOURNAL OF CANCER
AB Background: Multiple studies have compared the performance of artificial intelligence (AI) -based models for automated skin cancer classification to human experts, thus setting the cornerstone for a successful translation of AI-based tools into clinicopathological practice. Objective: The objective of the study was to systematically analyse the current state of research on reader studies involving melanoma and to assess their potential clinical relevance by evaluating three main aspects: test set characteristics (holdout/out-of-distribution data set, composition), test setting (experimental/clinical, inclusion of metadata) and representativeness of participating clini-cians. Methods: PubMed, Medline and ScienceDirect were screened for peer-reviewed studies published between 2017 and 2021 and dealing with AI-based skin cancer classification involving melanoma. The search terms skin cancer classification, deep learning, convolutional neural network (CNN), melanoma (detection), digital biomarkers, histopathology and whole slide imaging were com-bined. Based on the search results, only studies that considered direct comparison of AI results with clinicians and had a diagnostic classification as their main objective were included. Results: A total of 19 reader studies fulfilled the inclusion criteria. Of these, 11 CNN-based ap-proaches addressed the classification of dermoscopic images; 6 concentrated on the classification of clinical images, whereas 2 dermatopathological studies utilised digitised histopathological whole slide images. Conclusions: All 19 included studies demonstrated superior or at least equivalent performance of CNN-based classifiers compared with clinicians. However, almost all studies were conducted in highly artificial settings based exclusively on single images of the suspicious lesions. Moreover, test sets mainly consisted of holdout images and did not represent the full range of patient populations and melanoma subtypes encountered in clinical practice. (c) 2021 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
RI Betz-Stablein, Brigid/S-6925-2018; Barata, Catarina/R-6188-2018; Stolz,
   Wilhelm/M-5074-2019; Haferkamp, Sebastian/ABD-7390-2021; Meier,
   Friedegund/HTN-0745-2023; Podlipnik, Sebastian/U-3941-2019; Zink,
   Alexander/S-4445-2019; Navarini, Alexander/C-5504-2014
OI Betz-Stablein, Brigid/0000-0003-3876-501X; Barata,
   Catarina/0000-0002-2852-7723; Haferkamp, Sebastian/0000-0002-3894-8345;
   Meier, Friedegund/0000-0003-4340-9706; Podlipnik,
   Sebastian/0000-0003-4150-0522; Zink, Alexander/0000-0001-9313-6588;
   Brinker, Titus Josef/0000-0002-3620-5919; Poch,
   Gabriela/0000-0003-3948-2505; von Kalle, Christof/0000-0001-9221-3297;
   Navarini, Alexander/0000-0001-7059-632X; Schlaak,
   Max/0000-0002-1663-8098
SN 0959-8049
EI 1879-0852
PD OCT
PY 2021
VL 156
BP 202
EP 216
DI 10.1016/j.ejca.2021.06.049
EA SEP 2021
UT WOS:000703177400021
PM 34509059
ER

PT J
AU Westrate, L
   Brennan, S
   Carmany, EP
AF Westrate, Libby
   Brennan, Simone
   Carmany, Erin P.
TI Assessing the availability and quality of Spanish-language genetic
   resources for patients on the internet
SO JOURNAL OF GENETIC COUNSELING
AB In recent years, people increasingly are accessing health information on the Internet. A significant percentage of the United States (US) population has limited English proficiency with Spanish being the most common other language spoken. There is limited research on the presence or quality of Spanish-language health information, particularly in genetics, on the Internet overall. Therefore, we aimed to assess the availability and quality of patient-specific education resources in Spanish available on US-based support group websites for a wide range of genetic conditions. We assessed 630 websites through the Disease InfoSearch website (), created by Genetic Alliance, for the presence of Spanish genetic resources for patients with a new diagnosis of a genetic condition. Of these, 261 (41.4%) websites met study criteria for further evaluation. Of the 99 websites (37.9%) that had any Spanish content, 45 Spanish resources and a paired English resource from the same site met criteria for a quality assessment. Scoring was performed by two independent raters using Ensuring Quality Information for Patients (EQIP), a previously validated tool to assess the quality of written health information. The mean scores for Spanish and English resources were 57.3% and 58.4%, respectively, corresponding to a good quality score according to guidelines proposed by authors of EQIP. An independent two-sample t test showed no significant difference in the mean quality scores between Spanish and English resources (p-value = .506). Overall, we found limited availability of Spanish resources on the websites analyzed, but of those identified, there was no difference between the quality of Spanish resources and the paired English resources from the same site. These results highlight the need for genetics professionals to advocate for the creation of more Spanish patient resources. However, genetics professionals can have some reassurance that if a support group does produce a Spanish resource, it likely has comparable quality to its English equivalent.
SN 1059-7700
EI 1573-3599
PD JUN
PY 2020
VL 29
IS 3
SI SI
BP 381
EP 390
DI 10.1002/jgc4.1267
EA MAR 2020
PN 2
UT WOS:000522483100001
PM 32227560
ER

PT C
AU Oswald, M
   Powell, D
AF Oswald, Marion
   Powell, David
GP Assoc Comp Machinery
TI Can an algorithmic system be a 'friend' to a police officer's
   discretion? ACM FAT 2020 Translation Tutorial
SO FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS,
   ACCOUNTABILITY, AND TRANSPARENCY
CT ACM Conference on Fairness, Accountability, and Transparency (FAT)
CY JAN 27-30, 2020
CL Barcelona, SPAIN
SP Assoc Comp Machinery
AB This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.
   Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.
   Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).
   Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.
OI Oswald, Marion/0000-0003-1853-0002
BN 978-1-4503-6936-7
PY 2020
BP 698
EP 698
DI 10.1145/3351095.3375673
UT WOS:000620151400091
ER

PT C
AU Shao, MH
   Zuo, LR
   Carass, A
   Zhuo, JC
   Gullapalli, RP
   Prince, JL
AF Shao, Muhan
   Zuo, Lianrui
   Carass, Aaron
   Zhuo, Jiachen
   Gullapalli, Rao P.
   Prince, Jerry L.
BE Colliot, O
   Isgum, I
   Landman, BA
   Loew, MH
TI Evaluating the impact of MR image harmonization on thalamus deep network
   segmentation
SO MEDICAL IMAGING 2022: IMAGE PROCESSING
SE Proceedings of SPIE
CT Conference on Medical Imaging - Image Processing
CY FEB 20-MAR 27, 2022
CL ELECTR NETWORK
SP SPIE, Philips Healthcare
AB Medical image segmentation is one of the core tasks of medical image analysis. Automatic segmentation of brain magnetic resonance images (MRIs) can be used to visualize and track changes of the brain's anatomical structures that may occur due to normal aging or disease. Machine learning techniques are widely used in automatic structure segmentation. However, the contrast variation between the training and testing data makes it difficult for segmentation algorithms to generate consistent results. To address this problem, an image-to-image translation technique called MR image harmonization can be used to match the contrast between different data sets. It is important for the harmonization to transform image intensity while maintaining the underlying anatomy. In this paper, we present a 3D U-Net algorithm to segment the thalamus from multiple MR image modalities and investigate the impact of harmonization on the segmentation algorithm. Manual delineations of thalamic nuclei on two data sets are available. However, we aim to analyze the thalamus in another large data set where ground truth labels are lacking. We trained two segmentation networks, one with unharmonized images and the other with harmonized images, on one data set with manual labels, and compared their performances on the other data set with manual labels. These two data groups were diagnosed with two brain disorders and were acquired with similar imaging protocols. The harmonization target is the large data set without manual labels, which also has a different imaging protocol. The networks trained on unharmonized and harmonized data showed no significant difference when evaluating on the other data set; demonstrating that image harmonization can maintain the anatomy and does not affect the segmentation task. The two networks were evaluated on the harmonization target data set and the network trained on harmonized data showed significant improvement over the network trained on unharmonized data. Therefore, the network trained on harmonized data provides the potential to process large amounts of data from other sites, even in the absence of site-specific training data.
SN 0277-786X
EI 1996-756X
BN 978-1-5106-4940-8; 978-1-5106-4939-2
PY 2022
VL 12032
AR 120320H
DI 10.1117/12.2613159
UT WOS:000836295600015
PM 35514535
ER

PT J
AU Lendaro, E
   Middleton, A
   Brown, S
   Ortiz-Catalan, M
AF Lendaro, Eva
   Middleton, Alexandra
   Brown, Shannon
   Ortiz-Catalan, Max
TI Out of the Clinic, into the Home: The in-Home Use of Phantom Motor
   Execution Aided by Machine Learning and Augmented Reality for the
   Treatment of Phantom Limb Pain
SO JOURNAL OF PAIN RESEARCH
AB Purpose: Phantom motor execution (PME) facilitated by augmented/virtual reality (AR/VR) and serious gaming (SG) has been proposed as a treatment for phantom limb pain (PLP). Evidence of the efficacy of this approach was obtained through a clinical trial involving individuals with chronic intractable PLP affecting the upper limb, and further evidence is currently being sought with a multi-sited, international, double blind, randomized, controlled clinical trial in upper and lower limb amputees. All experiments have been conducted in a clinical setting supervised by a therapist. Here, we present a series of case studies (two upper and two lower limb amputees) on the use of PME as a self-treatment. We explore the benefits and the challenges encountered in translation from clinic to home use with a holistic, mixed-methods approach, employing both quantitative and qualitative methods from engineering, medical anthropology, and user interface design.
   Patients and Methods: All patients were provided with and trained to use a myoelectric pattern recognition and AR/VR device for PME. Patients took these devices home and used them independently over 12 months.
   Results: We found that patients were capable of conducting PME as a self-treatment and incorporated the device into their daily life routines. Use patterns and adherence to PME practice were not only driven by the presence of PLP but also influenced by patients' perceived need and social context. The main barriers to therapy adherence were time and availability of single-use electrodes, both of which could be resolved, or attenuated, by informed design considerations.
   Conclusion: Our findings suggest that adherence to treatment, and thus related outcomes, could be further improved by considering disparate user types and their utilization patterns. Our study highlights the importance of understanding, from multiple disciplinary angles, the tight coupling and interplay between pain, perceived need, and use of medical devices in patient-initiated therapy.
RI Lendaro, Eva/ABD-3925-2020
OI Lendaro, Eva/0000-0001-9545-7633; Middleton,
   Alexandra/0000-0002-1967-5994
SN 1178-7090
PY 2020
VL 13
BP 195
EP 209
DI 10.2147/JPR.S220160
UT WOS:000510700800001
PM 32021409
ER

PT J
AU Sadaoui, SE
   Mehdi-Souzani, C
   Lartigue, C
   Brahim, M
AF Sadaoui, Sif Eddine
   Mehdi-Souzani, Charyar
   Lartigue, Claire
   Brahim, Mahiddini
TI Automatic path planning for high performance measurement by laser plane
   sensors
SO OPTICS AND LASERS IN ENGINEERING
AB The Fourth Industrial Revolution (4IR) has led to a wide development of laser sensors and their use in inspection processes to achieve high performance is becoming more and more common. High measurement performance implies high measurement quality of surfaces in a short time. Generally, the quality is evaluated on the basis of indicators such as noise and trueness. Recently, it has been found that these indicators are mostly affected by the scanning strategy defined by two main parameters: the scanning angle and distance. Indeed, the path planning of laser sensors ensuring a high level of quality is based on the definition of the scanning strategy allowing scanning well-known surfaces with reduced noise and trueness. In this paper, an original approach for the generation of automatic path planning is proposed. The originality lies in the high performance obtained by optimizing the quality and the measurement time based on the optimal path planning. First, the laser sensor is qualified in order to evaluate the quality indicators as a function of the scanning distance and angle. Configurations that provide better quality are determined. By analyzing all laser sensor orientations held by the CMM's rotating head, a set of minimum orientations allowing high quality measurement is determined based on an optimization algorithm regarding quality, accessibility, visibility and collision detection criteria. The laser sensor path is based on the determination of the path levels in each sensor orientation. An original method is proposed to generate the optimal levels guaranteeing a high quality in terms of scanning distance. The laser sensor crossing points are then calculated in each level. An improvement of the method is proposed through the connection of levels using connection lines. Finally, collisions are studied and fixed by adding translation points. The proposed approach is applied to a part defined by its CAD model. The path is generated and executed on the CMM with a laser sensor. The measurement performance is compared to previous approaches in the literature. The methodology allows an adapted planning and can be used for automated dimensional inspection.
OI sadaoui, sif eddine/0000-0003-1570-5057
SN 0143-8166
EI 1873-0302
PD DEC
PY 2022
VL 159
AR 107194
DI 10.1016/j.optlaseng.2022.107194
EA JUL 2022
UT WOS:000858713400002
ER

PT J
AU Khan, S
   Nauman, M
   Alsaif, SA
   Syed, TA
   Eleraky, HA
AF Khan, Sohail
   Nauman, Mohammad
   Alsaif, Suleiman Ali
   Syed, Toqeer Ali
   Eleraky, Hassan Ahmad
TI Using Capsule Networks for Android Malware Detection Through
   Orientation-Based Features
SO CMC-COMPUTERS MATERIALS & CONTINUA
AB Mobile phones are an essential part of modern life. The two popular mobile phone platforms, Android and iPhone Operating System (iOS), have an immense impact on the lives of millions of people. Among these two, Android currently boasts more than 84% market share. Thus, any personal data put on it are at great risk if not properly protected. On the other hand, more than a million pieces of malware have been reported on Android in just 2021 till date. Detecting and mitigating all this malware is extremely difficult for any set of human experts. Due to this reason, machine learning-and specifically deep learning-has been utilized in the recent past to resolve this issue. How-ever, deep learning models have primarily been designed for image analysis. While this line of research has shown promising results, it has been difficult to really understand what the features extracted by deep learning models are in the domain of malware. Moreover, due to the translation invariance property of popular models based on Convolutional Neural Network (CNN), the true potential of deep learning for malware analysis is yet to be realized. To resolve this issue, we envision the use of Capsule Networks (CapsNets), a state-of-the-art model in deep learning. We argue that since CapsNets are orientation-based in terms of images, they can potentially be used to capture spatial relationships between different features at different locations within a sequence of opcodes. We design a deep learning-based architecture that efficiently and effectively handles very large scale malware datasets to detect Android malware without resorting to very deep networks. This leads to much faster detection as well as increased accuracy. We achieve state-of-the-art F1 score of 0.987 with an FPR of just 0.002 for three very large, real-world malware datasets. Our code is made available as open source and can be used to further enhance our work with minimal effort.
RI Alsaif, Dr. Suleiman/AAI-3128-2021; Eleraky, Hassan/ABB-8895-2021
OI Alsaif, Dr. Suleiman/0000-0003-4699-6432; Eleraky,
   Hassan/0000-0001-6485-3713
SN 1546-2218
EI 1546-2226
PY 2022
VL 70
IS 3
BP 5345
EP 5362
DI 10.32604/cmc.2022.021271
UT WOS:000707334500026
ER

PT J
AU Bonatti, AF
   Vozzi, G
   Chua, CK
   De Maria, C
AF Bonatti, Amedeo Franco
   Vozzi, Giovanni
   Chua, Chee Kai
   De Maria, Carmelo
TI A Deep Learning Quality Control Loop of the Extrusion-based Bioprinting
   Process
SO INTERNATIONAL JOURNAL OF BIOPRINTING
AB Extrusion-based bioprinting (EBB) represents one of the most used deposition technologies in the field of bioprinting, thanks to key advantages such as the easy-to-use hardware and the wide variety of materials that can be successfully printed. In recent years, research efforts have been focused on implementing a quality control loop for EBB, which can reduce the trial-and-error process necessary to optimize the printing parameters for a specific ink, standardize the results of a print across multiple laboratories, and so accelerate the translation of extrusion bioprinted products to more impactful clinical applications. Due to its capacity to acquire relevant features from a training dataset and generalize to unseen data, machine learning (ML) is currently being studied in literature as a relevant enabling technology for quality control in EBB. In this context, we propose a robust, deep learning-based control loop to automatically optimize the printing parameters and monitor the printing process online. We collected a comprehensive dataset of EBB prints by recording the process with a high-resolution webcam. To model multiple printing scenarios, each video represents a combination of multiple parameters, including printing set-up (either mechanical or pneumatic extrusion), material color, layer height, and infill density. After pre-processing, the collected dataset was used to thoroughly train and evaluate an ad hoc defined convolutional neural network by controlling over-fitting and optimizing the prediction time of the network. Finally, the ML model was used in a control loop to: (i) monitor the printing process and detect if a print with an error could be stopped before completion to save material and time and (ii) automatically optimize the printing parameters by combining the ML model with a previously published mathematical model of the EBB process. Altogether, we demonstrated for the first time how ML techniques can be used to automatize the EBB process, paving the way for a total quality control loop of the printing process.
OI Bonatti, Amedeo Franco/0000-0001-7177-5135; vozzi,
   giovanni/0000-0002-9414-9994; De Maria, Carmelo/0000-0002-1368-3571
SN 2424-7723
EI 2424-8002
PY 2022
VL 8
IS 4
BP 307
EP 320
DI 10.18063/ijb.v8i4.620
UT WOS:000869260200001
PM 36404777
ER

PT J
AU Martetschlager, F
   Reifenschneider, F
   Fischer, N
   Wijdicks, CA
   Millett, PJ
   Imhoff, AB
   Braun, S
AF Martetschlaeger, Frank
   Reifenschneider, Franziska
   Fischer, Nicole
   Wijdicks, Coen A.
   Millett, Peter J.
   Imhoff, Andreas B.
   Braun, Sepp
TI Sternoclavicular Joint Reconstruction Fracture Risk Is Reduced With
   Straight Drill Tunnels and Optimized With Tendon Graft Suture
   Augmentation
SO ORTHOPAEDIC JOURNAL OF SPORTS MEDICINE
AB Background: Despite the rare entity of sternoclavicular joint (SCJ) instability, a variety of different reconstruction techniques for SCJ dislocations have been described. A technique with oblique drilling has been proposed to reduce intraoperative risks. Purpose: To biomechanically investigate different cerclage reconstruction techniques and the benefit of additional reinforcement using suture tape. Study Design: Controlled laboratory study. Methods: Reconstructed artificial bone specimens were mounted on a mechanical testing machine. They were subjected to anterior and posterior translation, analyzing ultimate strength, displacement, stiffness, and elongation. For stage 1, different angulations of the drill tunnels through the sternum and clavicle were compared. Straight drill tunnels from anterior to posterior were compared with 45 degrees oblique drill tunnels. For stage 2, three different materials for cerclage reconstruction were compared: (1) suture tape alone (FT group), (2) tendon graft alone (tendon group), and (3) tendon graft with suture tape augmentation (tendon+FT group). Results: For the FT group, in the anterior and posterior directions, straight drill holes resulted in a significantly higher load to failure (936.9 +/- 122.5 N) compared with oblique ones (434.5 +/- 20.2 N) (P < .0001). During cyclic testing, all specimens with straight drill holes survived the 5- to 550-N step, while all specimens with oblique ones failed during the 5- to 450-N step. Analyzing the graft material choice, the mean load to failure was 556.6 +/- 174.3 N for the tendon group, 936.9 +/- 122.5 N for the FT group, and 767.0 +/- 110.7 N for the tendon+FT group (P = .089). The stiffness of the tendon+FT group was significantly lower than that of the FT group and significantly higher than that of the tendon group. Conclusion: Oblique tunnel placement during SCJ reconstruction, while reducing the intraoperative risk, results in decreased primary stability of the construct. Tendon graft reconstruction with suture tape augmentation leads to enhanced stability and optimizes biomechanical properties of the construct.
RI Wijdicks, Coen/M-7544-2019
OI Wijdicks, Coen/0000-0003-1831-6161
EI 2325-9671
PD APR 23
PY 2019
VL 7
IS 4
AR 2325967119838265
DI 10.1177/2325967119838265
UT WOS:000465380000001
PM 31041330
ER

PT J
AU Nguyen, TL
   Aung, YK
   Evans, CF
   Yoon-Ho, C
   Jenkins, MA
   Sung, J
   Hopper, JL
   Song, YM
AF Nguyen, Tuong Linh
   Aung, Ye Kyaw
   Evans, Christopher Francis
   Yoon-Ho, Choi
   Jenkins, Mark Anthony
   Sung, Joohon
   Hopper, John Llewelyn
   Song, Yun-Mi
TI Mammographic density defined by higher than conventional brightness
   threshold better predicts breast cancer risk for full-field digital
   mammograms
SO BREAST CANCER RESEARCH
AB Introduction: When measured using the computer-assisted method CUMULUS, mammographic density adjusted for age and body mass index predicts breast cancer risk. We asked if new mammographic density measures defined by higher brightness thresholds gave better risk predictions.
   Methods: The Korean Breast Cancer Study included 213 women diagnosed with invasive breast cancer and 630 controls matched for age at full-field digital mammogram and menopausal status. Mammographic density was measured using CUMULUS at the conventional threshold (Cumulus), and in effect at two increasingly higher thresholds, which we call Altocumulus and Cirrocumulus, respectively. All measures were Box-Cox transformed and adjusted for age, body mass index, menopausal status and machine. We used conditional logistic regression to estimate the change in Odds PER standard deviation of transformed and Adjusted density measures (OPERA). The area under the receiver operating characteristic curve (AUC) was estimated.
   Results: Corresponding Altocumulus and Cirrocumulus density measures were correlated with Cumulus measures (r approximately 0.8 and 0.6, respectively). Altocumulus and Cirrocumulus measures were on average 25 % and 80 % less, respectively, than the Cumulus measure. For dense area, the OPERA was 1.18 (95 % confidence interval: 1.01-1.39, P = 0.03) for Cumulus; 1.36 (1.15-1.62, P < 0.001) for Altocumulus; and 1.23 (1.04-1.45, P = 0.01) for Cirrocumulus. After fitting the Altocumulus measure, the Cumulus measure was no longer associated with risk. After fitting the Cumulus measure, the Altocumulus measure was still associated with risk (P = 0.001). The AUCs for dense area was 0.59 for the Altocumulus measure, greater than 0.55 and 0.57 for the Cumulus and Cirrocumulus measures, respectively (P = 0.001). Similar results were found for percentage dense area measures.
   Conclusions: Altocumulus measures perform better than Cumulus measures in predicting breast cancer risk, and Cumulus measures are confounded by Altocumulus measures. The mammographically bright regions might be more aetiologically important for breast cancer, with implications for biological, molecular, genetic and epidemiological research and clinical translation.
RI Nguyen, Tuong L./AAW-2704-2021; Jenkins, Mark/P-7803-2015; Aung, Ye
   Kyaw/AAC-7192-2022
OI Jenkins, Mark/0000-0002-8964-6160; Aung, Ye Kyaw/0000-0001-7488-6412;
   Nguyen, Tuong L./0000-0002-6597-8363
SN 1465-542X
EI 1465-5411
PD NOV 18
PY 2015
VL 17
AR 142
DI 10.1186/s13058-015-0654-4
UT WOS:000364910400001
PM 26581435
ER

PT J
AU Karvelis, P
   Charlton, CE
   Allohverdi, SG
   Bedford, P
   Hauke, DJ
   Diaconescu, AO
AF Karvelis, Povilas
   Charlton, Colleen E.
   Allohverdi, Shona G.
   Bedford, Peter
   Hauke, Daniel J.
   Diaconescu, Andreea O.
TI Computational approaches to treatment response prediction in major
   depression using brain activity and behavioral data: A systematic review
SO NETWORK NEUROSCIENCE
AB Major depressive disorder is a heterogeneous diagnostic category with multiple available treatments. With the goal of optimizing treatment selection, researchers are developing computational models that attempt to predict treatment response based on various pretreatment measures. In this paper, we review studies that use brain activity data to predict treatment response. Our aim is to highlight and clarify important methodological differences between various studies that relate to the incorporation of domain knowledge, specifically within two approaches delineated as data-driven and theory-driven. We argue that theory-driven generative modeling, which explicitly models information processing in the brain and thus can capture disease mechanisms, is a promising emerging approach that is only beginning to be utilized in treatment response prediction. The predictors extracted via such models could improve interpretability, which is critical for clinical decision-making. We also identify several methodological limitations across the reviewed studies and provide suggestions for addressing them. Namely, we consider problems with dichotomizing treatment outcomes, the importance of investigating more than one treatment in a given study for differential treatment response predictions, the need for a patient-centered approach for defining treatment outcomes, and finally, the use of internal and external validation methods for improving model generalizability.
   Author Summary Individuals with major depressive disorder (MDD) vary in their response to available treatments, rendering treatment selection a challenging task. In this paper, we review studies applying computational models for predicting treatment response in MDD based on measures of brain activity. We discuss methodological differences across studies, focusing on how they incorporate existing knowledge about MDD and how that affects interpretability of model predictions. In this context, we argue that theory-driven generative modeling, which explicitly models information processing in the brain and thus can capture disease mechanisms, is a promising emerging approach for treatment response prediction. Finally, we identify several other important limitations that are holding back the translation of these tools into clinical practice.
RI Hauke, Daniel J./ABC-8728-2021
OI Hauke, Daniel J./0000-0003-1772-9239; Karvelis,
   Povilas/0000-0001-7469-5624; Charlton, Colleen/0000-0002-9342-3533
SN 2472-1751
PD OCT 1
PY 2022
VL 6
IS 4
BP 1066
EP 1103
DI 10.1162/netn_a_00233
UT WOS:000876355600008
ER

PT J
AU Lee, MC
   Chang, JW
   Yeh, SC
   Chia, TL
   Liao, JS
   Chen, XM
AF Lee, Ming-Che
   Chang, Jia-Wei
   Yeh, Sheng-Cheng
   Chia, Tsorng-Lin
   Liao, Jie-Shan
   Chen, Xu-Ming
TI Applying attention-based BiLSTM and technical indicators in the design
   and performance analysis of stock trading strategies
SO NEURAL COMPUTING & APPLICATIONS
AB With the development of the Internet, information on the stock market has gradually become transparent, and stock information is easy to obtain. For investors, investment performance depends on the amount of capital and effective trading strategies. The analysis tool commonly used by investors and securities analysts is technical analysis (TA). Technical analysis is the study of past and current financial market information, and a large amount of statistical data is used to predict price trends and determine trading strategies. Technical indicators (TIs) are a type of technical analysis that summarizes possible future trends of stock prices based on historical statistical data to assist investors in making decisions. The stock price trend is a typical time series data with special characteristics such as trend, seasonality, and periodicity. In recent years, time series deep neural networks (DNNs) have demonstrated their powerful performance in machine translation, speech processing, and natural language processing fields. This research proposes the concept of attention-based BiLSTM (AttBiLSTM) applied to trading strategy design and verified the effectiveness of a variety of TIs, including stochastic oscillator, RSI, BIAS, W%R, and MACD. This research also proposes two trading strategies that suitable for DNN, combining with TIs and verifying their effectiveness. The main contributions of this research are as follows: (1) As our best knowledge, this is the first research to propose the concept of applying TIs to the LSTM-attention time series model for stock price prediction. (2) This study introduces five well-known TIs, which reached a maximum of 68.83% in the accuracy of stock trend prediction. (3) This research introduces the concept of exporting the probability of the deep model to the trading strategy. On the backtest of TPE0050, the experimental results reached the highest return on investment of 42.74%. (4) This research concludes from an empirical point of view that technical analysis combined with time series deep neural network has significant effects in stock price prediction and return on investment.
RI Kumar, Vijay/A-2782-2015
OI Kumar, Vijay/0000-0002-3460-6989; Chang, Jiawei/0000-0002-9321-6278;
   Lee, Ming Che/0000-0002-4400-9109
SN 0941-0643
EI 1433-3058
PD AUG
PY 2022
VL 34
IS 16
SI SI
BP 13267
EP 13279
DI 10.1007/s00521-021-06828-4
EA JAN 2022
UT WOS:000747641400001
PM 35106029
ER

PT J
AU Pasha, S
   Mac-Thiong, JM
AF Pasha, Saba
   Mac-Thiong, Jean-Marc
TI Defining criteria for optimal lumbar curve correction following the
   selective thoracic fusion surgery in Lenke 1 adolescent idiopathic
   scoliosis: developing a decision tree
SO EUROPEAN JOURNAL OF ORTHOPAEDIC SURGERY AND TRAUMATOLOGY
AB Objective The aim of this study was to identify the range of optimal versus suboptimal rates of spontaneous lumbar Cobb correction (SLCC%) and the factors predicting such outcomes in a cohort of Lenke 1 adolescent idiopathic scoliosis (AIS) after posterior spinal fusion surgery.
   Methods Seventy-one consecutive Lenke1 B and C AIS patients with a fusion level to L1 and higher with two-year follow-up were included. Thoracic kyphosis (T1-T4 and T4-T12 TK), lumbar lordosis (L1-S1 LL), thoracic and lumbar Cobb angles, thoracic and lumbar apical vertebral rotations and translations (AVR and AVT), pelvic incidence, sacral slope, and sagittal and frontal balances were measured at preoperative, early postoperative, and two-year follow-up. The SLCC% was calculated between preoperative and two-year follow-up. A clustering analysis determined the subgroups of patients with significantly higher and lower (optimal versus suboptimal) rate of SLCC% in the cohort at two-year follow-up. The cutoff values of the preoperative and early postoperative radiographic parameters that significantly predicted the optimal and suboptimal SLCC% were determined using a decision tree.
   Results The averages of the optimal versus suboptimal range of SLCC% in the cohort were 72% [55%, 105%] versus 39% [-7%, 42%]. Preoperative and early postoperative spinal parameters predicted the optimal versus suboptimal SLCC% with an accuracy of 82%, 95%CI [0.73-0.94]. Preoperative AVT(Lumbar)<10 mm was a predictor of optimal SLCC%. In patients with a preoperative AVT(Lumbar)>10 mm, early postoperative T4-T12 TK<24 degrees (but not less than 17 degrees) accompanied by -5 degrees<AVR(Thoracic)<5 degrees were the main predictors of optimal SLCC% in our cohort.
   Conclusion Quantitative clustering of the SLCC% into optimal and suboptimal groups allowed identifying the cutoff values of preoperative (AVT(Lumbar)) and early postoperative (T4-T12 TK and AVR(Thoracic)) spinal parameters that can predict the optimal range of SLCC% at two-year postoperative in our cohort of Lenke 1 AIS.Level of evidenceIV
SN 1633-8065
EI 1432-1068
PD APR
PY 2020
VL 30
IS 3
BP 513
EP 522
DI 10.1007/s00590-019-02596-z
UT WOS:000729759300017
PM 31760495
ER

PT J
AU Varatharajah, Y
   Berry, B
   Cimbalnik, J
   Kremen, V
   Van Gompel, J
   Stead, M
   Brinkmann, B
   Iyer, R
   Worrell, G
AF Varatharajah, Yogatheesan
   Berry, Brent
   Cimbalnik, Jan
   Kremen, Vaclav
   Van Gompel, Jamie
   Stead, Matt
   Brinkmann, Benjamin
   Iyer, Ravishankar
   Worrell, Gregory
TI Integrating artificial intelligence with real-time intracranial EEG
   monitoring to automate interictal identification of seizure onset zones
   in focal epilepsy
SO JOURNAL OF NEURAL ENGINEERING
AB Objective. An ability to map seizure-generating brain tissue, i.e. the seizure onset zone (SOZ), without recording actual seizures could reduce the duration of invasive EEG monitoring for patients with drug-resistant epilepsy. A widely-adopted practice in the literature is to compare the incidence (events/time) of putative pathological electrophysiological biomarkers associated with epileptic brain tissue with the SOZ determined from spontaneous seizures recorded with intracranial EEG, primarily using a single biomarker. Clinical translation of the previous efforts suffers from their inability to generalize across multiple patients because of (a) the inter-patient variability and (b) the temporal variability in the epileptogenic activity. Approach. Here, we report an artificial intelligence-based approach for combining multiple interictal electrophysiological biomarkers and their temporal characteristics as a way of accounting for the above barriers and show that it can reliably identify seizure onset zones in a study cohort of 82 patients who underwent evaluation for drug-resistant epilepsy. Main results. Our investigation provides evidence that utilizing the complementary information provided by multiple electrophysiological biomarkers and their temporal characteristics can significantly improve the localization potential compared to previously published singlebiomarker incidence-based approaches, resulting in an average area under ROC curve (AUC) value of 0.73 in a cohort of 82 patients. Our results also suggest that recording durations between 90 min and 2 h are sufficient to localize SOZs with accuracies that may prove clinically relevant. Significance. The successful validation of our approach on a large cohort of 82 patients warrants future investigation on the feasibility of utilizing intra-operative EEG monitoring and artificial intelligence to localize epileptogenic brain tissue. Broadly, our study demonstrates the use of artificial intelligence coupled with careful feature engineering in augmenting clinical decision making.
RI Kremen, Vaclav/G-7612-2017; Berger, Benjamin/HJA-7645-2022; Cimbalnik,
   Jan/H-6831-2016; Van Gompel, Jamie/AAW-4459-2020; Cimbalnik,
   Jan/N-5753-2019
OI Kremen, Vaclav/0000-0001-9844-7617; Berger,
   Benjamin/0000-0003-2654-2898; Cimbalnik, Jan/0000-0001-6670-6717; Van
   Gompel, Jamie/0000-0001-8087-7870; Cimbalnik, Jan/0000-0001-6670-6717;
   Brinkmann, Benjamin/0000-0002-2392-8608; Varatharajah,
   Yogatheesan/0000-0002-4547-0036
SN 1741-2560
EI 1741-2552
PD AUG
PY 2018
VL 15
IS 4
AR 046035
DI 10.1088/1741-2552/aac960
UT WOS:000436798200002
PM 29855436
ER

PT C
AU Jamjureeruk, V
AF Jamjureeruk, V
BE Aziz, OA
TI Evaluation of ventricular myocardial velocities and heart motions of the
   fetal heart by tissue Doppler imaging
SO PROCEEDINGS OF THE 4TH WORLD CONGRESS OF ECHOCARDIOGRAPHY AND VASCULAR
   ULTRASOUND
CT 4th World Congress of Echocardiography and Vascular Ultrasound
CY JAN 19-21, 2000
CL CAIRO, EGYPT
SP Int Soc Echocardiog & Vasc Ultrasound, Egyptian Soc Atherosclerosis
AB Background: Fetal echocardiogram has developed into a reliable tool for prenatal diagnosis of congenital heart disease. It is also used to evaluate ventricular function. Recently, Tissue Dopler Imagining (TDI) has been introduced to evaluate ventricular functions especially in ischaemic heart disease.
   Objective: To evaluate myocardial velocities and heart motions of the fetal heart by using TDI
   Method: TDI were preformed in 28 fetal hearts with gestational ages 20-35 wks. (Mean 29+/-3.7 wks) to evaluate myocardial velocities and heart motions. The Toshiba, Power Vision, machine and 3.75-5 MHz transducers were used with an appropriate setting of colour-coded tissue velocities. The epical four chambers and apical or parasternal long axis views were the standard planes for measuring myocardial velocities and evaluate the heart motions.
   Result: The myocardial velocities of the posterior wall of the left ventricle during the early, mid, late of systolic phase were 1.5+/-0,2.1+/- 0.9,1.0+/-0.5 cm/sec. and early, mid, late of diastolic phase were 1.3+/-0.7, 1.9+/-0.8,1.1+/-0.7 cm/sec, respectively. The myocardial velocity of the anterior wall of the right ventricle during the early, mid, late of systolic phase were 1.7+/-0.9,1.7+/-0.6,1.0+/-0.6 cm/sec. and early, mid, late of diastolic phase were 1.1+/-0.6,1.8+/-0.7,1.5+/-1.0 cm/sec. respectively. The myocardial velocity of the interventricular septum could not be measured due to the abnormal septal motion and the total fetal heart movement during the cardiac cycle. The fetal heart had anterior displacement during systole and posterior translation during diastole and also had counter-clockwise rotation during systolic phase.
   Conclusion: Using of the TDI to evaluate myocardial velocities of the fetal heart is limited due to the total fetal heart motions. The fetal heart movement is similar to the newborn or young adult heart.
BN 88-323-0119-9
PY 2000
BP 193
EP 200
UT WOS:000165663400030
ER

PT J
AU Wilkins, AA
   Whaley, P
   Persad, AS
   Druwe, IL
   Lee, JS
   Taylor, MM
   Shapiro, AJ
   Southard, NB
   Lemeris, C
   Thayer, KA
AF Wilkins, A. Amina
   Whaley, Paul
   Persad, Amanda S.
   Druwe, Ingrid L.
   Lee, Janice S.
   Taylor, Michele M.
   Shapiro, Andrew J.
   Southard, Natalie Blanton
   Lemeris, Courtney
   Thayer, Kristina A.
TI Assessing author willingness to enter study information into structured
   data templates as part of the manuscript submission process: A pilot
   study
SO HELIYON
AB Background: Environmental health and other researchers can benefit from automated or semi-automated summaries of data within published studies as summarizing study methods and results is time and resource intensive. Automated summaries can be designed to identify and extract details of interest pertaining to the study design, population, testing agent/intervention, or outcome (etc.). Much of the data reported across existing publications lack unified structure, standardization and machine-readable formats or may be presented in complex tables which serve as barriers that impede the development of automated data extraction methodologies. As full automation of data extraction seems unlikely soon, encouraging investigators to submit structured summaries of methods and results in standardized formats with meta-data tagging of content may be of value during the publication process. This would produce machine-readable content to facilitate automated data extraction, establish sharable data repositories, help make research data FAIR, and could improve reporting quality. Objectives: A pilot study was conducted to assess the feasibility of asking participants to summarize study methods and results using a structured, web-based data extraction model as a potential workflow that could be implemented during the manuscript submission process. Methods: Eight participants entered study details and data into the Health Assessment Workplace Collaborative (HAWC). Participants were surveyed after the extraction exercise to ascertain 1) whether this extraction exercise will impact their conducting and reporting of future research, 2) the ease of data extraction, including which fields were easiest and relatively more problematic to extract and 3) the amount of time taken to perform data extractions and other related tasks. Investigators then presented participants the potential benefits of providing structured data in the format they were extracting. After this, participants were surveyed about 1) their willingness to provide structured data during the publication process and 2) whether they felt the potential application of structured data entry approaches and their implementation during the journal submission process should continue to be further explored. Conclusions: Routine provision of structured data that summarizes key information from research studies could reduce the amount of effort required for reusing that data in the future, such as in systematic reviews or agency scientific assessments. Our pilot study suggests that directly asking authors to provide that data, via structured templates, may be a viable approach to achieving this: participants were willing to do so, and the overall process was not prohibitively arduous. We also found some support for the hypothesis that use of study templates may have halo benefits in improving the conduct and completeness of reporting of future research. While limitations in the generalizability of our findings mean that the conditions of success of templates cannot be assumed, further research into how such templates might be designed and implemented does seem to have enough chance of success that it ought to be undertaken.
OI Lee, Janice/0000-0002-4331-8452; Shapiro, Andy/0000-0002-5233-8092
EI 2405-8440
PD MAR
PY 2022
VL 8
IS 3
AR e09095
DI 10.1016/j.heliyon.2022.e09095
EA MAR 2022
UT WOS:000788362400004
PM 35846467
ER

PT J
AU Chen, LY
   Ribeiro, ALP
   Platonov, PG
   Cygankiewicz, I
   Soliman, EZ
   Gorenek, B
   Ikeda, T
   Vassilikos, VP
   Steinberg, JS
   Varma, N
   Bayes-de-Luna, A
   Baranchuk, A
AF Chen, Lin Yee
   Ribeiro, Antonio Luiz Pinho
   Platonov, Pyotr G.
   Cygankiewicz, Iwona
   Soliman, Elsayed Z.
   Gorenek, Bulent
   Ikeda, Takanori
   Vassilikos, Vassilios P.
   Steinberg, Jonathan S.
   Varma, Niraj
   Bayes-de-Luna, Antoni
   Baranchuk, Adrian
TI P Wave Parameters and Indices: A Critical Appraisal of Clinical Utility,
   Challenges, and Future Research-A Consensus Document Endorsed by the
   International Society of Electrocardiology and the International Society
   for Holter and Noninvasive Electrocardiology
SO CIRCULATION-ARRHYTHMIA AND ELECTROPHYSIOLOGY
AB Atrial cardiomyopathy, characterized by abnormalities in atrial structure and function, is associated with increased risk of adverse cardiovascular and neurocognitive outcomes, independent of atrial fibrillation. There exists a critical unmet need for a clinical tool that is cost-effective, easy to use, and that can diagnose atrial cardiomyopathy. P wave parameters (PWPs) reflect underlying atrial structure, size, and electrical activation; alterations in these factors manifest as abnormalities in PWPs that can be readily ascertained from a standard 12-lead ECG and potentially be used to aid clinical decision-making. PWPs include P wave duration, interatrial block, P wave terminal force in V-1, P wave axis, P wave voltage, P wave area, and P wave dispersion. PWPs can be combined to yield an index (P wave index), such as the morphology-voltage-P-wave duration ECG risk score. Abnormal PWPs have been shown in population-based cohort studies to be independently associated with higher risks of atrial fibrillation, ischemic stroke, sudden cardiac death, and dementia. Additionally, PWPs, either individually or in combination (as a P wave index), have been reported to enhance prediction of atrial fibrillation or ischemic stroke. To facilitate translation of PWPs to routine clinical practice, additional work is needed to standardize measurement of PWPs (eg, via semiautomated or automated measurement), confirm their reliability and predictive value, leverage novel approaches (eg, wavelet analysis of P waves and machine learning algorithms), and finally, define the risk-benefit ratio of specific interventions in high-risk individuals. Our ultimate goal is to repurpose the ubiquitous 12-lead ECG to advance the study, diagnosis, and treatment of atrial cardiomyopathy, thus overcoming critical challenges in prevention of cardiovascular disease and dementia.
SN 1941-3149
EI 1941-3084
PD APR
PY 2022
VL 15
IS 4
BP 273
EP 289
AR e010435
DI 10.1161/CIRCEP.121.010435
UT WOS:000784025800004
PM 35333097
ER

PT J
AU Schirmer, MD
   Venkataraman, A
   Rekik, I
   Kim, M
   Mostofsky, SH
   Nebel, MB
   Rosch, K
   Seymour, K
   Crocetti, D
   Irzan, H
   Hutel, M
   Ourselin, S
   Marlow, N
   Melbourne, A
   Levchenko, E
   Zhou, S
   Kunda, M
   Lu, HP
   Dvornek, NC
   Zhuang, JT
   Pinto, G
   Samal, S
   Zhang, J
   Bernal-Rusiel, JL
   Pienaar, R
   Chung, AW
AF Schirmer, Markus D.
   Venkataraman, Archana
   Rekik, Islem
   Kim, Minjeong
   Mostofsky, Stewart H.
   Nebel, Mary Beth
   Rosch, Keri
   Seymour, Karen
   Crocetti, Deana
   Irzan, Hassna
   Hutel, Michael
   Ourselin, Sebastien
   Marlow, Neil
   Melbourne, Andrew
   Levchenko, Egor
   Zhou, Shuo
   Kunda, Mwiza
   Lu, Haiping
   Dvornek, Nicha C.
   Zhuang, Juntang
   Pinto, Gideon
   Samal, Sandip
   Zhang, Jennings
   Bernal-Rusiel, Jorge L.
   Pienaar, Rudolph
   Chung, Ai Wern
TI Neuropsychiatric disease classification using functional connectomics-
   results of the connectomics in neuroimaging transfer learning challenge
SO MEDICAL IMAGE ANALYSIS
AB Large, open-source datasets, such as the Human Connectome Project and the Autism Brain Imaging Data Exchange, have spurred the development of new and increasingly powerful machine learning approaches for brain connectomics. However, one key question remains: are we capturing biologically relevant and generalizable information about the brain, or are we simply overfitting to the data? To answer this, we organized a scientific challenge, the Connectomics in NeuroImaging Transfer Learning Challenge (CNI-TLC), held in conjunction with MICCAI 2019. CNI-TLC included two classification tasks: (1) diagnosis of Attention-Deficit/Hyperactivity Disorder (ADHD) within a pre-adolescent cohort; and (2) transference of the ADHD model to a related cohort of Autism Spectrum Disorder (ASD) patients with an ADHD comorbidity. In total, 240 resting-state fMRI (rsfMRI) time series averaged according to three standard parcellation atlases, along with clinical diagnosis, were released for training and validation (120 neurotyp-ical controls and 120 ADHD). We also provided Challenge participants with demographic information of age, sex, IQ, and handedness. The second set of 100 subjects (50 neurotypical controls, 25 ADHD, and 25 ASD with ADHD comorbidity) was used for testing. Classification methodologies were submitted in a standardized format as containerized Docker images through ChRIS, an open-source image analysis platform. Utilizing an inclusive approach, we ranked the methods based on 16 metrics: accuracy, area under the curve, F1-score, false discovery rate, false negative rate, false omission rate, false positive rate, geometric mean, informedness, markedness, Matthew's correlation coefficient, negative predictive value, optimized precision, precision, sensitivity, and specificity. The final rank was calculated using the rank product for each participant across all measures. Furthermore, we assessed the calibration curves of each methodology. Five participants submitted their method for evaluation, with one outperforming all other methods in both ADHD and ASD classification. However, further improvements are still needed to reach the clinical translation of functional connectomics. We have kept the CNI-TLC open as a publicly available resource for developing and validating new classification methodologies in the field of connectomics.
   (c) 2021 Published by Elsevier B.V.
RI Rekik, Islem/AAH-1730-2022; Zhuang, Juntang/ABF-9267-2021; Zhou,
   Shuo/GNW-5153-2022; Rekik, Islem/AFL-9626-2022; Marlow,
   Neil/D-2918-2009; Ourselin, Sebastien/K-6960-2015
OI Rekik, Islem/0000-0001-5595-6673; Rekik, Islem/0000-0001-5595-6673;
   Irzan, Hassna/0000-0002-2524-9529; Zhou, Shuo/0000-0002-8069-2814;
   Marlow, Neil/0000-0001-5890-2953; Chung, Ai Wern/0000-0002-0905-6698;
   Ourselin, Sebastien/0000-0002-5694-5340
SN 1361-8415
EI 1361-8423
PD MAY
PY 2021
VL 70
AR 101972
DI 10.1016/j.media.2021.101972
EA MAR 2021
UT WOS:000639613600011
PM 33677261
ER

PT J
AU Cheatham, SW
   Baker, R
AF Cheatham, Scott W.
   Baker, Russell
TI A Clinically Relevant Method of Quantifying the Mechanical Properties of
   RockTape (R) Kinesiology Tape at Different Elongation Lengths
SO JOURNAL OF SPORT REHABILITATION
AB Context: Kinesiology tape (KT) is a therapeutic intervention used to treat different musculoskeletal conditions and to enhance sports performance. The evidence is inconclusive, with researchers attributing the variable outcomes to different manufactured KT used in the research. Researchers have begun to measure and document the mechanical properties of different brands, using machines versus professionals. This prevents a clear translation to clinical practice, as it may be difficult to reproduce outcomes. There is a need to measure the mechanical properties of KT using more clinically relevant methodology. Objective: The purpose was to document a clinically relevant method of measuring the mechanical properties of 2 different types of precut RockTape (R) tape at common elongation lengths and to establish the methodology for future validation research on this testing method. Design: Controlled laboratory study. Setting: University laboratory. Participants: One researcher conducted all measurements. Procedures: Each tape was measured at 3 elongation lengths with a force gauge. Main Outcome Measures: Force, stress, and Young modulus. Results: The RockTape e 2 and RockTape (R) 3 elongation force were 25% = 2.27 (0.21) and 2.12 (0.26) N, 50% = 6.51 (0.27) and 5.93 (0.20) N, and 75% = 30.13 (0.63) and 21.23 (0.41) N. The stress values for the RockTape (R) 2 and RockTape (R) 3 were 25% = 0.88 (0.05) and 0.82 (0.03) kappa Pa, 50% = 2.52 (0.03) and 2.29 (0.01) kappa Pa, and 75% = 11.67 (0.04) and 8.23 (0.02) kappa Pa. The Young modulus values for the RockTape (R) 2 and RockTape (R) 3 were 25% = 3.51 (0.00) and 3.29 (0.00) kappa Pa, 50% = 5.04 (0.00) and 4.60 (0.00) kappa Pa, and 75% 15.57 (0.00) and 10.96 (0.00) kappa Pa. Conclusion: This investigation documented a novel method of measuring the mechanical properties of 2 types of RockTape (R) KT. Future research should attempt to validate these testing methods.
RI Baker, Russell/GNP-5676-2022
OI Baker, Russell/0000-0003-3352-9632
SN 1056-6716
EI 1543-3072
PD JAN
PY 2021
VL 30
IS 1
BP 173
EP 176
DI 10.1123/jsr.2019-0261
UT WOS:000602807400025
PM 32320946
ER

PT J
AU Petrova-Antonova, D
   Spasov, I
   Petkova, Y
   Manova, I
   Ilieva, S
AF Petrova-Antonova, Dessislava
   Spasov, Ivaylo
   Petkova, Yanita
   Manova, Ilina
   Ilieva, Sylvia
TI CogniSoft: A Platform for the Automation of Cognitive Assessment and
   Rehabilitation of Multiple Sclerosis
SO COMPUTERS
AB Cognitive disorders remain a major cause of disability in Multiple Sclerosis (MS). They lead to unemployment, the need for daily assistance, and a poor quality of life. The understanding of the origin, factors, processes, and consequences of cognitive disfunction is key to its prevention, early diagnosis, and rehabilitation. The neuropsychological testing and continuous monitoring of cognitive status as part of the overall evaluation of patients with MS in parallel with clinical and paraclinical examinations are highly recommended. In order to improve health and disease understanding, a close linkage between fundamental, clinical, epidemiological, and socio-economic research is required. The effective sharing of data, standardized data processing, and the linkage of such data with large-scale cohort studies is a prerequisite for the translation of research findings into the clinical setting. In this context, this paper proposes a software platform for the cognitive assessment and rehabilitation of patients with MS called CogniSoft. The platform automates the Beck Depression Inventory (BDI-II) test and diagnostic tests for the evaluation of memory and executive functions based on the nature of Brief International Cognitive Assessment for MS (BICAMS), as well as implementing a set of games for cognitive rehabilitation based on BICAMS. The software architecture, core modules, and technologies used for their implementation are presented. Special attention is given to the development of cognitive tests for diagnostics and rehabilitation. Their automation enables better perception, avoids bias as a result of conducting the classic paper tests of various neurophysiologists, provides easy administration, and allows data collection in a uniform manner, which further enables analysis using statistical and machine learning algorithms. The CogniSoft platform is registered as medical software by the Bulgarian Drug Agency and it is currently deployed in the Neurological Clinic of the National Hospital of Cardiology in Sofia, Bulgaria. The first experiments prove the feasibility of the platform, showing that it saves time and financial resources while providing subjectivity in the interpretation of the cognitive test results.
RI ILIEVA, Sylvia/S-6715-2016; Petrova-Antonova, Dessislava
   Georgieva/R-4742-2016
OI ILIEVA, Sylvia/0000-0002-6399-3551; Petrova-Antonova, Dessislava
   Georgieva/0000-0002-9920-8877
SN 2073-431X
PD DEC
PY 2020
VL 9
IS 4
AR 93
DI 10.3390/computers9040093
UT WOS:000601514300001
ER

PT J
AU Chen, YN
   Hu, DQ
   Li, MY
   Duan, HL
   Lu, XD
AF Chen, Yani
   Hu, Danqing
   Li, Mengyang
   Duan, Huilong
   Lu, Xudong
TI Automatic SNOMED CT coding of Chinese clinical terms via attention-based
   semantic matching
SO INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
AB Background: A considerable amount of meaningful information is routinely recorded in Chinese clinical data in text format, referred to as Chinese clinical terms. The lack of coding is a major difficulty hindering the application of clinical terms. SNOMED CT is a widely used and comprehensive clinical health care terminology collection because of its coverage, granularity, clinical orientation, and logical underpinning. It is useful and efficient for automatically assigning SNOMED CT codes to Chinese clinical terms, but it still faces several problems. Current cross-language clinical term matching studies rely on external resources, such as machine translation and rule-based methods. Semantic matching methods have achieved strong performance on text matching, but few studies have been done on cross-language clinical term matching. We present an effective attention-based semantic matching algorithm to automatically cross-language code Chinese clinical terms with SNOMED CT.Method: Firstly, BERT was used to turn the input into word embedding. Then, the word embeddings were encoded through a BiLSTM with self-attention to focus on capturing distant relationships among words with different weights depending on their contribution to semantic matching. Then, decomposable attention was used to make semantic matching trivially parallelizable to speed up calculation. Finally, fully connected layers and a sigmoid were utilized to output matching results.Results: The 29,960 manually coded Chinese clinical terms, 30,040 unmatched Chinese clinical terms and SNOMED CT codes were collected to evaluate the proposed method. Compared with the existing semantic matching method, the proposed approach achieves state-of-the-art results demonstrating the effectiveness of the method with an accuracy of 0.905, a precision of 0.856, a recall of 0.518, and an F-measure of 0.645. The proposed Chinese-English bilingual term mapping, Chinese character-level and word-level encoder, English word-level encoder, BERT model, and attention mechanism performed better than other methods.Conclusion: The proposed automatic SNOMED CT coding approach of Chinese clinical terms via attention-based semantic matching can improve the performance of automated SNOMED CT code assignment for Chinese clinical terms and improve the efficiency of the code assignment.
OI Hu, Danqing/0000-0003-0810-4819
SN 1386-5056
EI 1872-8243
PD MAR
PY 2022
VL 159
AR 104676
DI 10.1016/j.ijmedinf.2021.104676
EA JAN 2022
UT WOS:000788794900006
PM 34990940
ER

PT J
AU Asgrimsdottir, ES
   Arenas, E
AF Asgrimsdottir, Emilia Sif
   Arenas, Ernest
TI Midbrain Dopaminergic Neuron Development at the Single Cell Level:In
   vivoand in Stem Cells
SO FRONTIERS IN CELL AND DEVELOPMENTAL BIOLOGY
AB Parkinson's disease (PD) is a progressive neurodegenerative disorder that predominantly affects dopaminergic (DA) neurons of the substantia nigra. Current treatment options for PD are symptomatic and typically involve the replacement of DA neurotransmission by DA drugs, which relieve the patients of some of their motor symptoms. However, by the time of diagnosis, patients have already lost about 70% of their substantia nigra DA neurons and these drugs offer only temporary relief. Therefore, cell replacement therapy has garnered much interest as a potential treatment option for PD. Early studies using human fetal tissue for transplantation in PD patients provided proof of principle for cell replacement therapy, but they also highlighted the ethical and practical difficulties associated with using human fetal tissue as a cell source. In recent years, advancements in stem cell research have made human pluripotent stem cells (hPSCs) an attractive source of material for cell replacement therapy. Studies on how DA neurons are specified and differentiated in the developing mouse midbrain have allowed us to recapitulate many of the positional and temporal cues needed to generate DA neuronsin vitro. However, little is known about the developmental programs that govern human DA neuron development. With the advent of single-cell RNA sequencing (scRNA-seq) and bioinformatics, it has become possible to analyze precious human samples with unprecedented detail and extract valuable high-quality information from large data sets. This technology has allowed the systematic classification of cell types present in the human developing midbrain along with their gene expression patterns. By studying human development in such an unbiased manner, we can begin to elucidate human DA neuron development and determine how much it differs from our knowledge of the rodent brain. Importantly, this molecular description of the function of human cells has become and will increasingly be a reference to define, evaluate, and engineer cell types for PD cell replacement therapy and disease modeling.
RI Arenas, Ernest/AAO-5231-2020
OI Arenas, Ernest/0000-0003-0197-6577
SN 2296-634X
PD JUN 25
PY 2020
VL 8
AR 463
DI 10.3389/fcell.2020.00463
UT WOS:000553962400001
PM 32733875
ER

PT J
AU Swanberg, KM
   Landheer, K
   Pitt, D
   Juchem, C
AF Swanberg, Kelley M.
   Landheer, Karl
   Pitt, David
   Juchem, Christoph
TI Quantifying the Metabolic Signature of Multiple Sclerosis by in vivo
   Proton Magnetic Resonance Spectroscopy: Current Challenges and Future
   Outlook in the Translation From Proton Signal to Diagnostic Biomarker
SO FRONTIERS IN NEUROLOGY
AB Proton magnetic resonance spectroscopy (H-1-MRS) offers a growing variety of methods for querying potential diagnostic biomarkers of multiple sclerosis in living central nervous system tissue. For the past three decades, H-1-MRS has enabled the acquisition of a rich dataset suggestive of numerous metabolic alterations in lesions, normal-appearing white matter, gray matter, and spinal cord of individuals with multiple sclerosis, but this body of information is not free of seeming internal contradiction. The use of H-1-MRS signals as diagnostic biomarkers depends on reproducible and generalizable sensitivity and specificity to disease state that can be confounded by a multitude of influences, including experiment group classification and demographics; acquisition sequence; spectral quality and quantifiability; the contribution of macromolecules and lipids to the spectroscopic baseline; spectral quantification pipeline; voxel tissue and lesion composition; T-1 and T-2 relaxation; B-1 field characteristics; and other features of study design, spectral acquisition and processing, and metabolite quantification about which the experimenter may possess imperfect or incomplete information. The direct comparison of H-1-MRS data from individuals with and without multiple sclerosis poses a special challenge in this regard, as several lines of evidence suggest that experimental cohorts may differ significantly in some of these parameters. We review the existing findings of in vivo H-1-MRS on central nervous system metabolic abnormalities in multiple sclerosis and its subtypes within the context of study design, spectral acquisition and processing, and metabolite quantification and offer an outlook on technical considerations, including the growing use of machine learning, by future investigations into diagnostic biomarkers of multiple sclerosis measurable by H-1-MRS.
RI Swanberg, Kelley M./AAC-6427-2021
OI Swanberg, Kelley M./0000-0002-1254-190X
SN 1664-2295
PD NOV 15
PY 2019
VL 10
AR 1173
DI 10.3389/fneur.2019.01173
UT WOS:000499942600001
PM 31803127
ER

PT J
AU Kolhatkar, V
   Roussel, A
   Dipper, S
   Zinsmeister, H
AF Kolhatkar, Varada
   Roussel, Adam
   Dipper, Stefanie
   Zinsmeister, Heike
TI Anaphora With Non-nominal Antecedents in Computational Linguistics: a
   Survey
SO COMPUTATIONAL LINGUISTICS
AB This article provides an extensive overview of the literature related to the phenomenon of non-nominal-antecedent anaphora (also known as abstract anaphora or discourse deixis), a type of anaphora in which an anaphor like that refers to an antecedent (marked in boldface) that is syntactically non-nominal, such as the first sentence in It's way too hot here. That's why I'm moving to Alaska. Annotating and automatically resolving these cases of anaphora is interesting in its own right because of the complexities involved in identifying non-nominal antecedents, which typically represent abstract objects such as events, facts, and propositions. There is also practical value in the resolution of non-nominal-antecedent anaphora, as this would help computational systems in machine translation, summarization, and question answering, as well as, conceivably, any other task dependent on some measure of text understanding.Most of the existing approaches to anaphora annotation and resolution focus on nominal-antecedent anaphora, classifying many of the cases where the antecedents are syntactically non-nominal as non-anaphoric. There has been some work done on this topic, but it remains scattered and difficult to collect and assess. With this article, we hope to bring together and synthesize work done in disparate contexts up to now in order to identify fundamental problems and draw conclusions from an overarching perspective. Having a good picture of the current state of the art in this field can help researchers direct their efforts to where they are most necessary.Because of the great variety of theoretical approaches that have been brought to bear on the problem, there is an equally diverse array of terminologies that are used to describe it, so we will provide an overview and discussion of these terminologies. We also describe the linguistic properties of non-nominal-antecedent anaphora, examine previous annotation efforts that have addressed this topic, and present the computational approaches that aim at resolving non-nominal-antecedent anaphora automatically. We close with a review of the remaining open questions in this area and some of our recommendations for future research.
OI Dipper, Stefanie/0000-0003-4357-9078
SN 0891-2017
EI 1530-9312
PD SEP
PY 2018
VL 44
IS 3
BP 547
EP 612
DI 10.1162/coli_a_00327
UT WOS:000445216700007
ER

PT C
AU Barron-Cedeno, A
AF Barron-Cedeno, Alberto
BE Chen, HH
   Efthimiadis, EN
   Savoy, J
   Crestani, F
   MarchandMaillet, S
TI On the Mono- and Cross-Language Detection of Text Reuse and Plagiarism
SO SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR
   CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL
CT 33rd Annual International ACM SIGIR Conference on Research and
   Development in Information Retrieval
CY JUL 19-23, 2010
CL Geneva, SWITZERLAND
SP Special Interest Grp Informat Retrieval, Assoc Comp Machinery, Baidu, Google, Microsoft Res, Yahoo, Informat Retrieval Fac, IBM Res, FNSNF, Swiss Natl Sci Fdn
AB Plagiarism, the unacknowledged reuse of text, has increased in recent years due to the large amount of texts readily available. For instance, recent studies claim that nowadays a high rate of student reports include plagiarism, making manual plagiarism detection practically infeasible.
   Automatic plagiarism detection tools assist experts to analyse documents for plagiarism. Nevertheless, the lack of standard collections with cases of plagiarism has prevented accurate comparing models, making differences hard to appreciate. Seminal efforts on the detection of text reuse [2] have fostered the composition of standard resources for the accurate evaluation and comparison of methods.
   The aim of this PhD thesis is to address three of the main problems in the development of better models for automatic plagiarism detection: (i) the adequate identification of good potential sources for a given suspicious text; (ii) the detection of plagiarism despite modifications, such as words substitution and paraphrasing (special stress is given to cross-language plagiarism); and (iii) the generation of standard collections of cases of plagiarism and text reuse in order to provide a framework for accurate comparison of models.
   Regarding difficulties (i) and (ii), we have carried out preliminary experiments over the METER corpus [2]. Given a suspicious document d(q) and a collection of potential source documents D, the process is divided in two steps. First, a small subset of potential source documents D* subset of D is retrieved. The documents d is an element of D* are the most related to d(q), and, therefore, the most likely to include the source of the plagiarised fragments in it. We performed this stage on the basis of the Kullback-Leibler distance, over a sub-sample of document's vocabularies. Afterwards, a detailed analysis is carried out comparing dq to every d E D* in order to identify potential cases of plagiarism and their source. This comparison was made on the basis of word n-grams, by considering n = {2, 3}. These n-gram levels are flexible enough to properly retrieve plagiarised fragments and their sources despite modifications [1]. The result is offered to the user to take the final decision. Further experiments were done in both stages in order to compare other similarity measures, such as the cosine measure, the Jaccard coefficient and diverse fingerprinting and probabilistic models.
   One of the main weaknesses of currently available models is that they are unable to detect cross-language plagiarism. Approaching the detection of this kind of plagiarism is of high relevance, as the most of the information published is written in English, and authors in other languages may find it attractive to make use of direct translations.
   Our experiments, carried out over parallel and a comparable corpora, show that models of "standard" cross-language information retrieval are not enough. In fact, if the analysed source and target languages are related in some way (common linguistic ancestors or technical vocabulary), a simple comparison based on character n-grams seems to be the option. However; in those cases where the relation between the implied languages is weaker, other models, such as those based on statistical machine translation, are necessary [3].
   We plan to perform further experiments, mainly to approach the detection of cross-language plagiarism. In order to do that, we will use the corpora developed under the framework of the PAN competition on plagiarism detection (cf. PAN@CLEF: http://pan.webis.de). Models that consider cross-language thesauri and comparison of cognates will also be applied.
OI Barron-Cedeno, Alberto/0000-0003-4719-3420
BN 978-1-60558-896-4
PY 2010
BP 914
EP 914
UT WOS:000286904100208
ER

PT J
AU Seifrid, M
   Pollice, R
   Aguilar-Granda, A
   Chan, ZM
   Hotta, K
   Ser, CT
   Vestfrid, J
   Wu, TC
   Aspuru-Guzik, A
AF Seifrid, Martin
   Pollice, Robert
   Aguilar-Granda, Andres
   Chan, Zamyla Morgan
   Hotta, Kazuhiro
   Ser, Cher Tian
   Vestfrid, Jenya
   Wu, Tony C.
   Aspuru-Guzik, Alan
TI Autonomous Chemical Experiments: Challenges and Perspectives on
   Establishing a Self-Driving Lab
SO ACCOUNTS OF CHEMICAL RESEARCH
AB ACCESS Metrics & More Article Recommendations CONSPECTUS: We must accelerate the pace at which we make technological advancements to address climate change and disease risks worldwide. This swifter pace of discovery requires faster research and development cycles enabled by better integration between hypothesis generation, design, experimentation, and data analysis. Typical research cycles take months to years. However, data-driven automated laboratories, or self-driving laboratories, can significantly accelerate molecular and materials discovery. Recently, substantial advancements have been made in the areas of machine learning and optimization algorithms that have allowed researchers to extract valuable knowledge from multidimensional data sets. Machine learning models can be trained on large data sets from the literature or databases, but their performance can often be hampered by a lack of negative results or metadata. In contrast, data generated by self-driving laboratories can be information-rich, containing precise details of the experimental conditions and metadata. Consequently, much larger amounts of high-quality data are gathered in self-driving laboratories. When placed in open repositories, this data can be used by the research community to reproduce experiments, for more in-depth analysis, or as the basis for further investigation. Accordingly, high-quality open data sets will increase the accessibility and reproducibility of science, which is sorely needed. In this Account, we describe our efforts to build a self-driving lab for the development of a new class of materials: organic semiconductor lasers (OSLs). Since they have only recently been demonstrated, little is known about the molecular and material design rules for thin-film, electrically-pumped OSL devices as compared to other technologies such as organic light-emitting diodes or organic photovoltaics. To realize high-performing OSL materials, we are developing a flexible system for automated synthesis via iterative Suzuki-Miyaura cross-coupling reactions. This automated synthesis platform is directly coupled to the analysis and purification capabilities. Subsequently, the molecules of interest can be transferred to an optical characterization setup. We are currently limited to optical measurements of the OSL molecules in solution. However, material properties are ultimately most important in the solid state (e.g., as a thin-film device). To that end and for a different scientific goal, we are developing a self-driving lab for inorganic thin-film materials focused on the oxygen evolution reaction. While the future of self-driving laboratories is very promising, numerous challenges still need to be overcome. These challenges can be split into cognition and motor function. Generally, the cognitive challenges are related to optimization with constraints or unexpected outcomes for which general algorithmic solutions have yet to be developed. A more practical challenge that could be resolved in the near future is that of software control and integration because few instrument manufacturers design their products with self-driving laboratories in mind. Challenges in motor function are largely related to handling heterogeneous systems, such as dispensing solids or performing extractions.
   As a result, it is critical to understand that adapting experimental procedures that were designed for human experimenters is not as simple as transferring those same actions to an automated system, and there may be more efficient ways to achieve the same goal in an automated fashion. Accordingly, for self-driving laboratories, we need to carefully rethink the translation of manual experimental protocols.
RI ; Aspuru-Guzik, Alan/A-4984-2008
OI Pollice, Robert/0000-0001-8836-6266; Seifrid,
   Martin/0000-0001-5238-0058; Aspuru-Guzik, Alan/0000-0002-8277-4434
SN 0001-4842
EI 1520-4898
DI 10.1021/acs.accounts.2c00220
EA AUG 2022
UT WOS:000844647700001
PM 35948428
ER

PT J
AU Wang, M
   Liu, FF
   Zhou, X
   Dai, YT
   Yang, MH
AF Wang Min
   Liu Fu-Fei
   Zhou Xian
   Dai Yu-Tang
   Yang Ming-Hong
TI Optical fiber sensing technologies based on femtosecond laser
   micromachining and sensitive films
SO ACTA PHYSICA SINICA
AB Integration of novel functional material with fiber optic components is one of the new trends in the field of novel sensing technologies. The combination of fiber optics with functional materials offers great potential for realizing the novel sensors. Typically in optical fibre sensing technology, fibre itself acts as sensing element and also transmitting element, such as fiber Bragg grating (FBG), Brillouin or Raman optical time domain reflectometer. However such sensing components can only detect limited physical parameters such as temperature or strain based on the principle of characteristic wavelength drifts. While the idea of optical fiber sensing technology with functional materials is quite different from that of the traditional technology, functional materials can be employed as sensing components, therefore many parameters, including chemical or biological parameters, can be detected, depending on the designs of different sensing films. When compared with the common fiber sensing technologies such as FBG and optical time domain reflectometer, fiber optic sensors based on functional materials show advantages in the diversity of measurement parameters. However, functional materials can be realized by many techniques including e-beam evaporation, magnetron sputtering, spin-coating, electro-chemical plating, etc. The mechanical stability of tiny optical fibers is still problematic, which could be a challenge to industrial applications.
   In this work, a femtosecond laser fabricated fiber inline micro Mach-Zehnder interferometer with deposited palladium film for hydrogen sensing is presented. Simulation results show that the transmission spectrum of the interferometer is critically dependent on the microcavity length and the refractive index of Pd film, and a short microcavity length corresponds to a high sensitivity. The experimental results obtained in a wavelength region of 1200-1400 nm, and in a hydrogen concentration range of 0-16%, accord well with those of the simulations. The developed system has high potential in hydrogen sensing with high sensitivity. Three-dimensional multitrench microstructures, femtosecond laser ablated in fiber Bragg grating cladding, TbDyFe sputtering are proposed and demonstrated for magnetic field sensing probe. Parameters such as the number of straight microtrenches, translation speed (feed rate), and laser pulse power of laser beam have been systematically varied and optimized. A 5-mu m-thick giant Terfenol-D magnetostrictive film is sputtered onto FBG microtrenches, and acts as a magnetic sensing transducer. Eight microtrench samples produce the highest central wavelength shift of 120 pm, nearly fivefold more sensitive than nonmicrostructured standard FBG. An increase in laser pulse power to 20 mW generates a magnetic sensitivity of 0.58 pm/mT. Interestingly, reduction in translational speed contributes dramatically to the rise in the magnetic sensitivity of the sample. These sensor samples show magnetic response reversibility and have great potential in the magnetic field sensing domain. Furthermore hydrogen sensors based on fiber Bragg gratings micro-machined by femtosecond laser to form microgrooves and sputtered with Pd/Ag composite film are proposed and demonstrated. The atomic ratio of the two metals is controlled at Pd : Ag = 3 : 1. At room temperature, the hydrogen sensitivity of the sensor probe micro-machined by 75 mW laser power and sputtered with 520 nm of Pd/Ag film is 16.5 pm/% H. Comparably, the standard FBG hydrogen sensitivity becomes 2.5 pm/% H for the same 4% hydrogen concentration. At an ambient temperature of 35 degrees C, the processed sensor head has a dramatic rise in hydrogen sensitivity. Besides, the sensor shows good response and repeatability during hydrogen concentration test.
OI Yang, Minghong/0000-0001-7423-385X
SN 1000-3290
PD APR 5
PY 2017
VL 66
IS 7
AR 070703
DI 10.7498/aps.66.070703
UT WOS:000412898500003
ER

PT J
AU Rengarajan, B
   Patnaik, SS
   Finol, EA
AF Rengarajan, Balaji
   Patnaik, Sourav S.
   Finol, Ender A.
TI A Predictive Analysis of Wall Stress in Abdominal Aortic Aneurysms Using
   a Neural Network Model
SO JOURNAL OF BIOMECHANICAL ENGINEERING-TRANSACTIONS OF THE ASME
AB Rupture risk assessment of abdominal aortic aneurysms (AAAs) by means of quantifying wall stress is a common biomechanical strategy. However, the clinical translation of this approach has been greatly limited due to the complexity associated with the computational tools required for its implementation. Thus, being able to estimate wall stress using nonbiomechanical markers that can be quantified as a direct outcome of clinical image segmentation would be advantageous in improving the potential implementation of said strategy. In the present work, we investigated the use of geometric indices to predict patient-specific AAA wall stress by means of a novel neural network (NN) modeling approach. We conducted a retrospective review of existing clinical images of two patient groups: 98 asymptomatic and 50 symptomatic AAAs. The images were subject to a protocol consisting of image segmentation, processing, volume meshing, finite element modeling, and geometry quantification, from which 53 geometric indices and the spatially averaged wall stress (SAWS) were calculated. SAWS estimated from finite element analysis was considered the gold standard for the predictions. We developed feed-forward NN models composed of an input layer, two dense layers, and an output layer using Keras, a deep learning library in python. The NN models were trained, tested, and validated independently for both AAA groups using all geometric indices, as well as a reduced set of indices resulting from a variable reduction procedure. We compared the performance of the NN models with two standard machine learning algorithms (MARS: multivariate adaptive regression splines and GAM: generalized additive model) and a linear regression model (GLM: generalized linear model). With the reduced sets of indices, the NN-based approach exhibited the highest mean goodness-of-fit (for the symptomatic group 0.71 and for the asymptomatic group 0.79) and lowest mean relative error (17% for both groups). In contrast, MARS yielded a mean goodness-of-fit of 0.59 for the symptomatic group and 0.77 for the asymptomatic group, with relative errors of 17% for the symptomatic group and 22% for the asymptomatic group. GAM had a mean goodness-of-fit of 0.70 for the symptomatic group and 0.80 for the asymptomatic group, with relative errors of 16% for the symptomatic group and 20% for the asymptomatic group. GLM did not perform as well as the other algorithms, with a mean goodness-of-fit of 0.53 for the symptomatic group and 0.70 for the asymptomatic group, with relative errors of 19% for the symptomatic group and 23% for the asymptomatic group. Nevertheless, the NN models required a reduced set of 15 and 13 geometric indices to predict SAWS for the symptomatic and asymptomatic AAA groups, respectively. This was in contrast to the reduced set of nine and eight geometric indices required to predict SAWS with the MARS and GAM algorithms for each AAA group, respectively. The use of NN modeling represents a promising alternative methodology for the estimation of AAA wall stress using geometric indices as surrogates, in lieu of finite element modeling. The performance metrics of NN models are expected to improve with significantly larger group sizes, given the suitability of NN modeling for "big data" applications.
RI Patnaik, Sourav/J-5445-2019
OI Patnaik, Sourav/0000-0001-7457-8604
SN 0148-0731
EI 1528-8951
PD DEC 1
PY 2021
VL 143
IS 12
AR 121004
DI 10.1115/1.4051905
UT WOS:000721743500004
PM 34318314
ER

PT J
AU Seifrid, M
   Pollice, R
   Aguilar-Granda, A
   Chan, ZM
   Hotta, K
   Ser, CT
   Vestfrid, J
   Wu, TC
   Aspuru-Guzik, A
AF Seifrid, Martin
   Pollice, Robert
   Aguilar-Granda, Andres
   Chan, Zamyla Morgan
   Hotta, Kazuhiro
   Ser, Cher Tian
   Vestfrid, Jenya
   Wu, Tony C.
   Aspuru-Guzik, Alan
TI Autonomous Chemical Experiments: Challenges and Perspectives on
   Establishing a Self-Driving Lab
SO ACCOUNTS OF CHEMICAL RESEARCH
AB CONSPECTUS: We must accelerate the pace at which we make technological advancements to address climate change and disease risks worldwide. This swifter pace of discovery requires faster research and development cycles enabled by better integration between hypothesis generation, design, experimentation, and data analysis. Typical research cycles take months to years. However, data-driven automated laboratories, or self-driving laboratories, can significantly accelerate molecular and materials discovery. Recently, substantial advancements have been made in the areas of machine learning and optimization algorithms that have allowed researchers to extract valuable knowledge from multidimensional data sets. Machine learning models can be trained on large data sets from the literature or databases, but their performance can often be hampered by a lack of negative results or metadata. In contrast, data generated by self-driving laboratories can be information-rich, containing precise details of the experimental conditions and metadata. Consequently, much larger amounts of high-quality data are gathered in self-driving laboratories. When placed in open repositories, this data can be used by the research community to reproduce experiments, for more in-depth analysis, or as the basis for further investigation. Accordingly, high-quality open data sets will increase the accessibility and reproducibility of science, which is sorely needed. In this Account, we describe our efforts to build a self-driving lab for the development of a new class of materials: organic semiconductor lasers (OSLs). Since they have only recently been demonstrated, little is known about the molecular and material design rules for thin-film, electrically-pumped OSL devices as compared to other technologies such as organic light-emitting diodes or organic photovoltaics. To realize high-performing OSL materials, we are developing a flexible system for automated synthesis via iterative Suzuki-Miyaura cross-coupling reactions. This automated synthesis platform is directly coupled to the analysis and purification capabilities. Subsequently, the molecules of interest can be transferred to an optical characterization setup. We are currently limited to optical measurements of the OSL molecules in solution. However, material properties are ultimately most important in the solid state (e.g., as a thin-film device). To that end and for a different scientific goal, we are developing a self-driving lab for inorganic thin-film materials focused on the oxygen evolution reaction. While the future of self-driving laboratories is very promising, numerous challenges still need to be overcome. These challenges can be split into cognition and motor function. Generally, the cognitive challenges are related to optimization with constraints or unexpected outcomes for which general algorithmic solutions have yet to be developed. A more practical challenge that could be resolved in the near future is that of software control and integration because few instrument manufacturers design their products with self-driving laboratories in mind. Challenges in motor function are largely related to handling heterogeneous systems, such as dispensing solids or performing extractions. As a result, it is critical to understand that adapting experimental procedures that were designed for human experimenters is not as simple as transferring those same actions to an automated system, and there may be more efficient ways to achieve the same goal in an automated fashion.
   Accordingly, for self-driving laboratories, we need to carefully rethink the translation of manual experimental protocols.
RI ; Aspuru-Guzik, Alan/A-4984-2008
OI Pollice, Robert/0000-0001-8836-6266; Seifrid,
   Martin/0000-0001-5238-0058; Aspuru-Guzik, Alan/0000-0002-8277-4434
SN 0001-4842
EI 1520-4898
PD AUG 10
PY 2022
VL 55
IS 17
BP 2454
EP 2466
DI 10.1021/acs.accounts.2c002202454
UT WOS:000855255500001
PM 35948428
ER

PT J
AU Tselishchev, VV
AF Tselishchev, Vitaly V.
TI MATHEMATICAL REASONING: CONCEPTUAL PROOF AND LOGIC CONCLUSION
SO EPISTEMOLOGY & PHILOSOPHY OF SCIENCE-EPISTEMOLOGIYA I FILOSOFIYA NAUKI
AB The article is devoted to the comparison of two types of proofs in mathematical practice, the methodological differences of which go back to the difference in the understanding of the nature of mathematics by Descartes and Leibniz. In modern philosophy of mathematics, we talk about conceptual and formal proofs in connection with the so-called Hilbert Thesis, according to which every proof can be transformed into a logical conclusion in a suitable formal system. The analysis of the arguments of the proponents and opponents of the Thesis, "conceptualists" and "formalists", is presented respectively by the two main antagonists Y. Rav and J. Azzouni. The focus is on the possibility of reproducing the proof of "interesting" mathematical theorems in the form of a strict logical conclusion, in principle feasible by a mechanical procedure. The argument of conceptualists is based on pointing out the importance of other aspects of the proof besides the logical conclusion, namely, in introducing new concepts, methods, and establishing connections between different sections of meaningful mathematics, which is often illustrated by the case of proving Fermat's Last Theorem (Y. Rav). Formalists say that a conceptual proof "points" to the formal logical structure of the proof (J. Azzouni). The article shows that the disagreement is based on the assumption of asymmetry of mutual translation of syntactic and semantic structures of the language, as a result of which the formal proof loses important semantic factors of proof. In favor of a formal proof, the program of univalent foundations of mathematics In. Vojevodski, according to which the future of mathematical proofs is associated with the availability of computer verification programs. In favor of conceptual proofs, it is stated (A. Pelc) that the number of steps in the supposed formal logical conclusion when proving an "interesting" theorem exceeds the cognitive abilities of a person. The latter circumstance leads the controversy beyond the actual topic of mathematical proof into the epistemological sphere of discussions of "mentalists" and "mechanists" on the question of the supposed superiority of human intelligence over the machine, initiated by R. Penrose in his interpretation of the Second Theorem of Goedel, among whose supporters, as it turned out, was Goedel himself.
SN 1811-833X
EI 2311-7133
PY 2020
VL 57
IS 4
BP 74
EP 86
DI 10.5840/eps202057459
UT WOS:000612991000007
ER

PT J
AU Li, MH
   Mestre, TA
   Fox, SH
   Taati, B
AF Li, Michael H.
   Mestre, Tiago A.
   Fox, Susan H.
   Taati, Babak
TI Vision-based assessment of parkinsonism and levodopa-induced dyskinesia
   with pose estimation
SO JOURNAL OF NEUROENGINEERING AND REHABILITATION
AB BackgroundDespite the effectiveness of levodopa for treatment of Parkinson's disease (PD), prolonged usage leads to development of motor complications, most notably levodopa-induced dyskinesia (LID). Persons with PD and their physicians must regularly modify treatment regimens and timing for optimal relief of symptoms. While standardized clinical rating scales exist for assessing the severity of PD symptoms, they must be administered by a trained medical professional and are inherently subjective. Computer vision is an attractive, non-contact, potential solution for automated assessment of PD, made possible by recent advances in computational power and deep learning algorithms. The objective of this paper was to evaluate the feasibility of vision-based assessment of parkinsonism and LID using pose estimation.MethodsNine participants with PD and LID completed a levodopa infusion protocol, where symptoms were assessed at regular intervals using the Unified Dyskinesia Rating Scale (UDysRS) and Unified Parkinson's Disease Rating Scale (UPDRS). Movement trajectories of individual joints were extracted from videos of PD assessment using Convolutional Pose Machines, a pose estimation algorithm built with deep learning. Features of the movement trajectories (e.g. kinematic, frequency) were used to train random forests to detect and estimate the severity of parkinsonism and LID. Communication and drinking tasks were used to assess LID, while leg agility and toe tapping tasks were used to assess parkinsonism. Feature sets from tasks were also combined to predict total UDysRS and UPDRS Part III scores.ResultsFor LID, the communication task yielded the best results (detection: AUC=0.930, severity estimation: r=0.661). For parkinsonism, leg agility had better results for severity estimation (r=0.618), while toe tapping was better for detection (AUC=0.773). UDysRS and UPDRS Part III scores were predicted with r=0.741 and 0.530, respectively.ConclusionThe proposed system provides insight into the potential of computer vision and deep learning for clinical application in PD and demonstrates promising performance for the future translation of deep learning to PD clinical practices. Convenient and objective assessment of PD symptoms will facilitate more frequent touchpoints between patients and clinicians, leading to better tailoring of treatment and quality of care.
SN 1743-0003
PD NOV 6
PY 2018
VL 15
AR 97
DI 10.1186/s12984-018-0446-z
UT WOS:000449414600001
PM 30400914
ER

PT J
AU Xia, JG
   Broadhurst, DI
   Wilson, M
   Wishart, DS
AF Xia, Jianguo
   Broadhurst, David I.
   Wilson, Michael
   Wishart, David S.
TI Translational biomarker discovery in clinical metabolomics: an
   introductory tutorial
SO METABOLOMICS
AB Metabolomics is increasingly being applied towards the identification of biomarkers for disease diagnosis, prognosis and risk prediction. Unfortunately among the many published metabolomic studies focusing on biomarker discovery, there is very little consistency and relatively little rigor in how researchers select, assess or report their candidate biomarkers. In particular, few studies report any measure of sensitivity, specificity, or provide receiver operator characteristic (ROC) curves with associated confidence intervals. Even fewer studies explicitly describe or release the biomarker model used to generate their ROC curves. This is surprising given that for biomarker studies in most other biomedical fields, ROC curve analysis is generally considered the standard method for performance assessment. Because the ultimate goal of biomarker discovery is the translation of those biomarkers to clinical practice, it is clear that the metabolomics community needs to start "speaking the same language" in terms of biomarker analysis and reporting-especially if it wants to see metabolite markers being routinely used in the clinic. In this tutorial, we will first introduce the concept of ROC curves and describe their use in single biomarker analysis for clinical chemistry. This includes the construction of ROC curves, understanding the meaning of area under ROC curves (AUC) and partial AUC, as well as the calculation of confidence intervals. The second part of the tutorial focuses on biomarker analyses within the context of metabolomics. This section describes different statistical and machine learning strategies that can be used to create multi-metabolite biomarker models and explains how these models can be assessed using ROC curves. In the third part of the tutorial we discuss common issues and potential pitfalls associated with different analysis methods and provide readers with a list of nine recommendations for biomarker analysis and reporting. To help readers test, visualize and explore the concepts presented in this tutorial, we also introduce a web-based tool called ROCCET (ROC Curve Explorer & Tester, http://www.roccet.ca). ROCCET was originally developed as a teaching aid but it can also serve as a training and testing resource to assist metabolomics researchers build biomarker models and conduct a range of common ROC curve analyses for biomarker studies.
RI Wishart, David Scott/ABI-3181-2020; Xia, Jianguo/Z-1935-2019
OI Xia, Jianguo/0000-0003-2040-2624; Broadhurst, David/0000-0003-0775-9581;
   Wishart, David S/0000-0002-3207-2434
SN 1573-3882
EI 1573-3890
PD APR
PY 2013
VL 9
IS 2
BP 280
EP 299
DI 10.1007/s11306-012-0482-9
UT WOS:000316913200003
PM 23543913
ER

PT J
AU Peng, LY
   Lin, LF
   Lin, YS
   Chen, YW
   Mo, ZH
   Vlasova, RM
   Kim, SH
   Evans, AC
   Dager, SR
   Estes, AM
   McKinstry, RC
   Botteron, KN
   Gerig, G
   Schultz, RT
   Hazlett, HC
   Piven, J
   Burrows, CA
   Grzadzinski, RL
   Girault, JB
   Shen, MD
   Styner, MA
AF Peng, Liying
   Lin, Lanfen
   Lin, Yusen
   Chen, Yen-wei
   Mo, Zhanhao
   Vlasova, Roza M.
   Kim, Sun Hyung
   Evans, Alan C.
   Dager, Stephen R.
   Estes, Annette M.
   McKinstry, Robert C.
   Botteron, Kelly N.
   Gerig, Guido
   Schultz, Robert T.
   Hazlett, Heather C.
   Piven, Joseph
   Burrows, Catherine A.
   Grzadzinski, Rebecca L.
   Girault, Jessica B.
   Shen, Mark D.
   Styner, Martin A.
TI Longitudinal Prediction of Infant MR Images With Multi-Contrast
   Perceptual Adversarial Learning
SO FRONTIERS IN NEUROSCIENCE
AB The infant brain undergoes a remarkable period of neural development that is crucial for the development of cognitive and behavioral capacities (Hasegawa et al., 2018). Longitudinal magnetic resonance imaging (MRI) is able to characterize the developmental trajectories and is critical in neuroimaging studies of early brain development. However, missing data at different time points is an unavoidable occurrence in longitudinal studies owing to participant attrition and scan failure. Compared to dropping incomplete data, data imputation is considered a better solution to address such missing data in order to preserve all available samples. In this paper, we adapt generative adversarial networks (GAN) to a new application: longitudinal image prediction of structural MRI in the first year of life. In contrast to existing medical image-to-image translation applications of GANs, where inputs and outputs share a very close anatomical structure, our task is more challenging as brain size, shape and tissue contrast vary significantly between the input data and the predicted data. Several improvements over existing GAN approaches are proposed to address these challenges in our task. To enhance the realism, crispness, and accuracy of the predicted images, we incorporate both a traditional voxel-wise reconstruction loss as well as a perceptual loss term into the adversarial learning scheme. As the differing contrast changes in T1w and T2w MR images in the first year of life, we incorporate multi-contrast images leading to our proposed 3D multi-contrast perceptual adversarial network (MPGAN). Extensive evaluations are performed to assess the qualityand fidelity of the predicted images, including qualitative and quantitative assessments of the image appearance, as well as quantitative assessment on two segmentation tasks. Our experimental results show that our MPGAN is an effective solution for longitudinal MR image data imputation in the infant brain. We further apply our predicted/imputed images to two practical tasks, a regression task and a classification task, in order to highlight the enhanced task-related performance following image imputation. The results show that the model performance in both tasks is improved by including the additional imputed data, demonstrating the usability of the predicted images generated from our approach.
RI VLasova, Roza/AFQ-7077-2022
OI VLasova, Roza/0000-0002-6455-8949; Burrows,
   Catherine/0000-0003-2812-3953
EI 1662-453X
PD SEP 9
PY 2021
VL 15
AR 653213
DI 10.3389/fnins.2021.653213
UT WOS:000698804200001
PM 34566556
ER

PT J
AU Konopka, T
   Ng, S
   Smedley, D
AF Konopka, Tomasz
   Ng, Sandra
   Smedley, Damian
TI Diffusion enables integration of heterogeneous data and user-driven
   learning in a desktop knowledge-base
SO PLOS COMPUTATIONAL BIOLOGY
AB Integrating reference datasets (e.g. from high-throughput experiments) with unstructured and manually-assembled information (e.g. notes or comments from individual researchers) has the potential to tailor bioinformatic analyses to specific needs and to lead to new insights. However, developing bespoke analysis pipelines from scratch is time-consuming, and general tools for exploring such heterogeneous data are not available. We argue that by treating all data as text, a knowledge-base can accommodate a range of bioinformatic data types and applications. We show that a database coupled to nearest-neighbor algorithms can address common tasks such as gene-set analysis as well as specific tasks such as ontology translation. We further show that a mathematical transformation motivated by diffusion can be effective for exploration across heterogeneous datasets. Diffusion enables the knowledge-base to begin with a sparse query, impute more features, and find matches that would otherwise remain hidden. This can be used, for example, to map multi-modal queries consisting of gene symbols and phenotypes to descriptions of diseases. Diffusion also enables user-driven learning: when the knowledge-base cannot provide satisfactory search results in the first instance, users can improve the results in real-time by adding domain-specific knowledge. User-driven learning has implications for data management, integration, and curation.
   Author summary Biological datasets can be too large to unravel without computational tools and, at the same time, too small to benefit from machine-learning approaches that require large data volumes to train. Analyses of niche datasets are also hindered by the difficulty of incorporating knowledge from domain experts into practical algorithms. In this work, we argue that such challenges may be addressed by data integration platforms that can learn in real-time from users. We support the argument with an implementation of a practical, general-purpose, tool. We show that its search capabilities can solve common bioinformatic tasks such as gene-set analysis and matching genes and phenotypes to diseases, which are usually tackled using statistical methods and bespoke algorithms. Importantly, our tool leaves domain experts full control to modulate search results in transparent, biologically meaningful ways. It also allows users to improve search outputs if the default results are incomplete. These features extend the capabilities of existing knowledge-bases and empower domain experts to tailor data exploration to their needs.
OI Ng, Sandra/0000-0003-4524-6058; Konopka, Tomasz/0000-0003-3042-4712
SN 1553-734X
EI 1553-7358
PD AUG
PY 2021
VL 17
IS 8
AR e1009283
DI 10.1371/journal.pcbi.1009283
UT WOS:000684013300002
PM 34379637
ER

PT J
AU Plattner, H
AF Plattner, Helmut
TI Evolutionary Cell Biology of Proteins from Protists to Humans and Plants
SO JOURNAL OF EUKARYOTIC MICROBIOLOGY
AB During evolution, the cell as a fine-tuned machine had to undergo permanent adjustments to match changes in its environment, while closed for repair work was not possible. Evolution from protists (protozoa and unicellular algae) to multicellular organisms may have occurred in basically two lineages, Unikonta and Bikonta, culminating in mammals and angiosperms (flowering plants), respectively. Unicellular models for unikont evolution are myxamoebae (Dictyostelium) and increasingly also choanoflagellates, whereas for bikonts, ciliates are preferred models. Information accumulating from combined molecular database search and experimental verification allows new insights into evolutionary diversification and maintenance of genes/proteins from protozoa on, eventually with orthologs in bacteria. However, proteins have rarely been followed up systematically for maintenance or change of function or intracellular localization, acquirement of new domains, partial deletion (e.g. of subunits), and refunctionalization, etc. These aspects are discussed in this review, envisaging evolutionary cell biology. Protozoan heritage is found for most important cellular structures and functions up to humans and flowering plants. Examples discussed include refunctionalization of voltage-dependent Ca2+ channels in cilia and replacement by other types during evolution. Altogether components serving Ca2+ signaling are very flexible throughout evolution, calmodulin being a most conservative example, in contrast to calcineurin whose catalytic subunit is lost in plants, whereas both subunits are maintained up to mammals for complex functions (immune defense and learning). Domain structure of R-type SNAREs differs in mono- and bikonta, as do Ca2+-dependent protein kinases. Unprecedented selective expansion of the subunit a which connects multimeric base piece and head parts (V0, V1) of H+-ATPase/pump may well reflect the intriguing vesicle trafficking system in ciliates, specifically in Paramecium. One of the most flexible proteins is centrin when its intracellular localization and function throughout evolution is traced. There are many more examples documenting evolutionary flexibility of translation products depending on requirements and potential for implantation within the actual cellular context at different levels of evolution. From estimates of gene and protein numbers per organism, it appears that much of the basic inventory of protozoan precursors could be transmitted to highest eukaryotic levels, with some losses and also with important additional inventions.
SN 1066-5234
EI 1550-7408
PD MAR-APR
PY 2018
VL 65
IS 2
BP 255
EP 289
DI 10.1111/jeu.12449
UT WOS:000426628300011
PM 28719054
ER

PT J
AU Wang, JL
   Torii, M
   Liu, HF
   Hart, GW
   Hu, ZZ
AF Wang, Jinlian
   Torii, Manabu
   Liu, Hongfang
   Hart, Gerald W.
   Hu, Zhang-Zhi
TI dbOGAP - An Integrated Bioinformatics Resource for Protein
   O-GlcNAcylation
SO BMC BIOINFORMATICS
AB Background: Protein O-GlcNAcylation (or O-GlcNAc-ylation) is an O-linked glycosylation involving the transfer of beta-N acetylglucosamine to the hydroxyl group of serine or threonine residues of proteins. Growing evidences suggest that protein O-GlcNAcylation is common and is analogous to phosphorylation in modulating broad ranges of biological processes. However, compared to phosphorylation, the amount of protein O-GlcNAcylation data is relatively limited and its annotation in databases is scarce. Furthermore, a bioinformatics resource for O-GlcNAcylation is lacking, and an O-GlcNAcylation site prediction tool is much needed.
   Description: We developed a database of O-GlcNAcylated proteins and sites, dbOGAP, primarily based on literature published since O-GlcNAcylation was first described in 1984. The database currently contains similar to 800 proteins with experimental O-GlcNAcylation information, of which similar to 61% are of humans, and 172 proteins have a total of similar to 400 O-GlcNAcylation sites identified. The O-GlcNAcylated proteins are primarily nucleocytoplasmic, including membrane-and non-membrane bounded organelle-associated proteins. The known O-GlcNAcylated proteins exert a broad range of functions including transcriptional regulation, macromolecular complex assembly, intracellular transport, translation, and regulation of cell growth or death. The database also contains similar to 365 potential O-GlcNAcylated proteins inferred from known O-GlcNAcylated orthologs. Additional annotations, including other protein posttranslational modifications, biological pathways and disease information are integrated into the database. We developed an O-GlcNAcylation site prediction system, OGlcNAcScan, based on Support Vector Machine and trained using protein sequences with known O-GlcNAcylation sites from dbOGAP. The site prediction system achieved an area under ROC curve of 74.3% in five-fold cross-validation. The dbOGAP website was developed to allow for performing search and query on O-GlcNAcylated proteins and associated literature, as well as for browsing by gene names, organisms or pathways, and downloading of the database. Also available from the website, the OGlcNAcScan tool presents a list of predicted O-GlcNAcylation sites for given protein sequences.
   Conclusions: dbOGAP is the first public bioinformatics resource to allow systematic access to the O-GlcNAcylated proteins, and related functional information and bibliography, as well as to an O-GlcNAcylation site prediction tool. The resource will facilitate research on O-GlcNAcylation and its proteomic identification.
OI Torii, Manabu/0000-0002-4825-0696; Hart, Gerald/0000-0001-7812-4351
SN 1471-2105
PD APR 6
PY 2011
VL 12
AR 91
DI 10.1186/1471-2105-12-91
UT WOS:000289954200001
PM 21466708
ER

PT J
AU Katta, J
   Stapleton, T
   Ingham, E
   Jin, ZM
   Fisher, J
AF Katta, J.
   Stapleton, T.
   Ingham, E.
   Jin, Z. M.
   Fisher, J.
TI The effect of glycosaminoglycan depletion on the friction and
   deformation of articular cartilage
SO PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART H-JOURNAL OF
   ENGINEERING IN MEDICINE
AB Glycosaminoglycans (GAGs) have been shown to be responsible for the interstitial fluid pressurization of articular cartilage and hence its compressive stiffness and load-bearing properties. Contradictory evidence has been presented in the literature on the effect of depleting GAGs on the friction properties of articular cartilage. The aim of this study was to investigate the effect of depleting GAGs on the friction and deformation characteristics of articular cartilage under different tribological conditions.
   A pin-on-plate machine was utilized to measure the coefficient of friction of native and chondroitinase ABC (CaseABC)-treated articular cartilage under two different models: static (4 mm/s start-up velocity) and dynamic (4 mm/s sliding velocity; 4 mm stroke length) under a load of 25 N (0.4 MPa contact stress) and with phosphate-buffered saline as the lubricant. Indentation tests were carried out at I N and 2 N loads (0.14 MPa and 0.28 MPa contact stress levels) to study the deformation characteristics of both native and GAG-depleted cartilage samples. CaseABC treatment rendered the cartilage tissue soft owing to the loss of compressive stiffness and a sulphated-sugar assay confirmed the loss of GAGs from the cartilage samples. CaseABC treatment significantly increased (by more than 50 per cent) the friction levels in the dynamic model (p < 0.05) at higher loading times owing to the loss of biphasic lubrication.
   CaseABC treatment had no effect on friction in the static model in which the cartilage surfaces did not have an opportunity to recover fluid because of static loading unlike the cartilage tissue in the dynamic model, in which translation of the cartilage surfaces was involved, ensuring effective biphasic lubrication. Therefore the depletion of GAGs had a smaller effect on the coefficient of friction for the static model. Indentation tests showed that GAG-depleted cartilage samples had a lower elastic modulus and higher permeability than native tissue. These results corroborate the role of GAGs in the compressive and friction properties of articular cartilage and emphasize the need for developing strategies to control GAG loss from diseased articular cartilage tissue.
OI Ingham, Eileen/0000-0002-9757-3045
SN 0954-4119
EI 2041-3033
PD JAN
PY 2008
VL 222
IS H1
BP 1
EP 11
DI 10.1243/09544119JEIM325
UT WOS:000254063600001
PM 18335713
ER

PT J
AU Bahamonde, S
   Dialektopoulos, KF
   Escamilla-Rivera, C
   Farrugia, G
   Gakis, V
   Hendry, M
   Hohmann, M
   Said, JL
   Mifsud, J
   Di Valentino, E
AF Bahamonde, Sebastian
   Dialektopoulos, Konstantinos F.
   Escamilla-Rivera, Celia
   Farrugia, Gabriel
   Gakis, Viktor
   Hendry, Martin
   Hohmann, Manuel
   Levi Said, Jackson
   Mifsud, Jurgen
   Di Valentino, Eleonora
TI Teleparallel gravity: from theory to cosmology
SO REPORTS ON PROGRESS IN PHYSICS
AB Teleparallel gravity (TG) has significantly increased in popularity in recent decades, bringing attention to Einstein's other theory of gravity. In this Review, we give a comprehensive introduction to how teleparallel geometry is developed as a gauge theory of translations together with all the other properties of gauge field theory. This relates the geometry to the broader metric-affine approach to forming gravitational theories where we describe a systematic way of constructing consistent teleparallel theories that respect certain physical conditions such as local Lorentz invariance. We first use TG to formulate a teleparallel equivalent of general relativity (GR) which is dynamically equivalent to GR but which may have different behaviors for other scenarios, such as quantum gravity. After setting this foundation, we describe the plethora of modified teleparallel theories of gravity that have been proposed in the literature. We attempt to connect them together into general classes of covariant gravitational theories. Of particular interest, we highlight the recent proposal of a teleparallel analogue of Horndeski gravity which offers the possibility of reviving all of the regular Horndeski contributions. In the second part of the Review, we first survey works in teleparallel astrophysics literature where we focus on the open questions in this regime of physics. We then discuss the cosmological consequences for the various formulations of TG. We do this at background level by exploring works using various approaches ranging from dynamical systems to Noether symmetries, and more. Naturally, we then discuss perturbation theory, firstly by giving a concise approach in which this can be applied in TG theories and then apply it to a number of important theories in the literature. Finally, we examine works in observational and precision cosmology across the plethora of proposal theories. This is done using some of the latest observations and is used to tackle cosmological tensions which may be alleviated in teleparallel cosmology. We also introduce a number of recent works in the application of machine learning to gravity, we do this through deep learning and Gaussian processes, together with discussions about other approaches in the literature.
OI Dialektopoulos, Konstantinos/0000-0002-0672-1496; Bahamonde,
   Sebastian/0000-0001-6235-120X; Di Valentino,
   Eleonora/0000-0001-8408-6961; Escamilla-Rivera,
   Celia/0000-0002-8929-250X; Levi Said, Jackson/0000-0002-7835-4365
SN 0034-4885
EI 1361-6633
PD FEB 1
PY 2023
VL 86
IS 2
AR 026901
DI 10.1088/1361-6633/ac9cef
UT WOS:000924420500001
PM 36279849
ER

PT J
AU Birkenbihl, C
   Emon, MA
   Vrooman, H
   Westwood, S
   Lovestone, S
   Hofmann-Apitius, M
   Frohlich, H
AF Birkenbihl, Colin
   Emon, Mohammad Asif
   Vrooman, Henri
   Westwood, Sarah
   Lovestone, Simon
   Hofmann-Apitius, Martin
   Froehlich, Holger
CA AddNeuroMed Consortium
   Alzheimers Dis Neuroimaging Initia
TI Differences in cohort study data affect external validation of
   artificial intelligence models for predictive diagnostics of
   dementia-lessons for translation into clinical practice
SO EPMA JOURNAL
AB Artificial intelligence (AI) approaches pose a great opportunity for individualized, pre-symptomatic disease diagnosis which plays a key role in the context of personalized, predictive, and finally preventive medicine (PPPM). However, to translate PPPM into clinical practice, it is of utmost importance that AI-based models are carefully validated. The validation process comprises several steps, one of which is testing the model on patient-level data from an independent clinical cohort study. However, recruitment criteria can bias statistical analysis of cohort study data and impede model application beyond the training data. To evaluate whether and how data from independent clinical cohort studies differ from each other, this study systematically compares the datasets collected from two major dementia cohorts, namely, the Alzheimer's Disease Neuroimaging Initiative (ADNI) and AddNeuroMed. The presented comparison was conducted on individual feature level and revealed significant differences among both cohorts. Such systematic deviations can potentially hamper the generalizability of results which were based on a single cohort dataset. Despite identified differences, validation of a previously published, ADNI trained model for prediction of personalized dementia risk scores on 244 AddNeuroMed subjects was successful: External validation resulted in a high prediction performance of above 80% area under receiver operator characteristic curve up to 6 years before dementia diagnosis. Propensity score matching identified a subset of patients from AddNeuroMed, which showed significantly smaller demographic differences to ADNI. For these patients, an even higher prediction performance was achieved, which demonstrates the influence systematic differences between cohorts can have on validation results. In conclusion, this study exposes challenges in external validation of AI models on cohort study data and is one of the rare cases in the neurology field in which such external validation was performed. The presented model represents a proof of concept that reliable models for personalized predictive diagnostics are feasible, which, in turn, could lead to adequate disease prevention and hereby enable the PPPM paradigm in the dementia field.
RI Fröhlich, Holger/H-1976-2017
OI Fröhlich, Holger/0000-0002-5328-1243; Birkenbihl,
   Colin/0000-0002-7212-7700
SN 1878-5077
EI 1878-5085
PD SEP
PY 2020
VL 11
IS 3
SI SI
BP 367
EP 376
DI 10.1007/s13167-020-00216-z
EA JUN 2020
UT WOS:000542081600001
PM 32843907
ER

PT J
AU Lin, Y
   Tan, XL
   Yang, B
   Yang, K
   Zhang, JW
   Yu, J
AF Lin, Yi
   Tan, Xianlong
   Yang, Bo
   Yang, Kai
   Zhang, Jianwei
   Yu, Jing
TI Real-time Controlling Dynamics Sensing in Air Traffic System
SO SENSORS
AB In order to obtain real-time controlling dynamics in air traffic system, a framework is proposed to introduce and process air traffic control (ATC) speech via radiotelephony communication. An automatic speech recognition (ASR) and controlling instruction understanding (CIU)-based pipeline is designed to convert the ATC speech into ATC related elements, i.e., controlling intent and parameters. A correction procedure is also proposed to improve the reliability of the information obtained by the proposed framework. In the ASR model, acoustic model (AM), pronunciation model (PM), and phoneme- and word-based language model (LM) are proposed to unify multilingual ASR into one model. In this work, based on their tasks, the AM and PM are defined as speech recognition and machine translation problems respectively. Two-dimensional convolution and average-pooling layers are designed to solve special challenges of ASR in ATC. An encoder-decoder architecture-based neural network is proposed to translate phoneme labels into word labels, which achieves the purpose of ASR. In the CIU model, a recurrent neural network-based joint model is proposed to detect the controlling intent and label the controlling parameters, in which the two tasks are solved in one network to enhance the performance with each other based on ATC communication rules. The ATC speech is now converted into ATC related elements by the proposed ASR and CIU model. To further improve the accuracy of the sensing framework, a correction procedure is proposed to revise minor mistakes in ASR decoding results based on the flight information, such as flight plan, ADS-B. The proposed models are trained using real operating data and applied to a civil aviation airport in China to evaluate their performance. Experimental results show that the proposed framework can obtain real-time controlling dynamics with high performance, only 4% word-error rate. Meanwhile, the decoding efficiency can also meet the requirement of real-time applications, i.e., an average 0.147 real time factor. With the proposed framework and obtained traffic dynamics, current ATC applications can be accomplished with higher accuracy. In addition, the proposed ASR pipeline has high reusability, which allows us to apply it to other controlling scenes and languages with minor changes.
RI Zhang, Jianwei/HJA-0011-2022; Lin, Yi/ABH-6092-2020
OI Lin, Yi/0000-0002-7194-5023
SN 1424-8220
PD FEB 1
PY 2019
VL 19
IS 3
AR 679
DI 10.3390/s19030679
UT WOS:000459941200242
PM 30736452
ER

PT J
AU Madhamshettiwar, PB
   Maetschke, SR
   Davis, MJ
   Reverter, A
   Ragan, MA
AF Madhamshettiwar, Piyush B.
   Maetschke, Stefan R.
   Davis, Melissa J.
   Reverter, Antonio
   Ragan, Mark A.
TI Gene regulatory network inference: evaluation and application to ovarian
   cancer allows the prioritization of drug targets
SO GENOME MEDICINE
AB Background: Altered networks of gene regulation underlie many complex conditions, including cancer. Inferring gene regulatory networks from high-throughput microarray expression data is a fundamental but challenging task in computational systems biology and its translation to genomic medicine. Although diverse computational and statistical approaches have been brought to bear on the gene regulatory network inference problem, their relative strengths and disadvantages remain poorly understood, largely because comparative analyses usually consider only small subsets of methods, use only synthetic data, and/or fail to adopt a common measure of inference quality.
   Methods: We report a comprehensive comparative evaluation of nine state-of-the art gene regulatory network inference methods encompassing the main algorithmic approaches (mutual information, correlation, partial correlation, random forests, support vector machines) using 38 simulated datasets and empirical serous papillary ovarian adenocarcinoma expression-microarray data. We then apply the best-performing method to infer normal and cancer networks. We assess the druggability of the proteins encoded by our predicted target genes using the CancerResource and PharmGKB webtools and databases.
   Results: We observe large differences in the accuracy with which these methods predict the underlying gene regulatory network depending on features of the data, network size, topology, experiment type, and parameter settings. Applying the best-performing method (the supervised method SIRENE) to the serous papillary ovarian adenocarcinoma dataset, we infer and rank regulatory interactions, some previously reported and others novel. For selected novel interactions we propose testable mechanistic models linking gene regulation to cancer. Using network analysis and visualization, we uncover cross-regulation of angiogenesis-specific genes through three key transcription factors in normal and cancer conditions. Druggabilty analysis of proteins encoded by the 10 highest-confidence target genes, and by 15 genes with differential regulation in normal and cancer conditions, reveals 75% to be potential drug targets.
   Conclusions: Our study represents a concrete application of gene regulatory network inference to ovarian cancer, demonstrating the complete cycle of computational systems biology research, from genome-scale data analysis via network inference, evaluation of methods, to the generation of novel testable hypotheses, their prioritization for experimental validation, and discovery of potential drug targets.
RI Davis, Melissa/B-9168-2013; Reverter, Antonio/C-9699-2013; Ragan, Mark
   A/A-9825-2014
OI Reverter, Antonio/0000-0002-4681-9404; Ragan, Mark
   A/0000-0003-1672-7020; Maetschke, Stefan/0000-0002-9540-1717; Davis,
   Melissa/0000-0003-4864-7033
SN 1756-994X
PD MAY 1
PY 2012
VL 4
AR 41
DI 10.1186/gm340
UT WOS:000314569300001
PM 22548828
ER

PT J
AU Moudjari, L
   Benamara, F
   Akli-Astouati, K
AF Moudjari, Leila
   Benamara, Farah
   Akli-Astouati, Karima
TI Multi-level embeddings for processing Arabic social media contents
SO COMPUTER SPEECH AND LANGUAGE
AB Embeddings are very popular representations that allow computing semantic and syntactic similarities between linguistic units from text co-occurrence matrix. Units can vary from character n-grams to words, including more coarse-grained units such as sentences and documents. Recently, multi-level embeddings combining representations from different units have been proposed as an alternative to single-level embeddings to account for the internal structure of words (i.e., morphology) and help systems to generalise well over out of vocabulary words. These representations, either pre-trained or learned, have shown to be quite effective, outperforming word-level baselines in several NLP tasks such as machine translation, part of speech tagging and named entity recognition. Our aim here is to contribute to this line of research proposing for the first time in Arabic NLP an in-depth study of the impact of various subwords configurations ranging from character to character n-grams (including word) for social media text classification. We propose several neural architectures to learn character, subword and word embeddings, as well as a combination of these three levels, exploring different composition functions to obtain the final representation of a given text. To evaluate the effectiveness of these representations, we perform extrinsic evaluations on three text classification tasks (sentiment analysis, emotion detection and irony detection) while accounting for different Arabic varieties (Modern Standard Arabic, dialects (Levantine and Maghrebi)). For each task, we experiment with well-known dialect-agnostic and dialect specific datasets, including those that have been recently used in shared tasks to better compare our results with those reported in previous studies on the same datasets. The results show that the multi-level embeddings we propose outperform current static and contextualised embeddings as well as best performing state of the art models in sentiment and emotion detection. In addition, we achieve competitive results in irony detection. Our models are also the most productive across dialects observing that different dialects require different composition configurations. We finally show that these performances tend to increase when coupling the multi-level representations with task-specific features.
   (c) 2021 Elsevier Ltd. All rights reserved.
SN 0885-2308
EI 1095-8363
PD NOV
PY 2021
VL 70
AR 101240
DI 10.1016/j.csl.2021.101240
EA MAY 2021
UT WOS:000661725200013
ER

PT J
AU Rybalkin, V
   Sudarshan, C
   Weis, C
   Lappas, J
   Wehn, N
   Cheng, L
AF Rybalkin, Vladimir
   Sudarshan, Chirag
   Weis, Christian
   Lappas, Jan
   Wehn, Norbert
   Cheng, Li
TI Efficient Hardware Architectures for 1D-and MD-LSTM Networks
SO JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO
   TECHNOLOGY
AB Recurrent Neural Networks, in particular One-dimensional and Multidimensional Long Short-Term Memory (1D-LSTM and MD-LSTM) have achieved state-of-the-art classification accuracy in many applications such as machine translation, image caption generation, handwritten text recognition, medical imaging and many more. However, high classification accuracy comes at high compute, storage, and memory bandwidth requirements, which make their deployment challenging, especially for energy-constrained platforms such as portable devices. In comparison to CNNs, not so many investigations exist on efficient hardware implementations for 1D-LSTM especially under energy constraints, and there is no research publication on hardware architecture for MD-LSTM. In this article, we present two novel architectures for LSTM inference: a hardware architecture for MD-LSTM, and a DRAM-based Processing-in-Memory (DRAM-PIM) hardware architecture for 1D-LSTM. We present for the first time a hardware architecture for MD-LSTM, and show a trade-off analysis for accuracy and hardware cost for various precisions. We implement the new architecture as an FPGA-based accelerator that outperforms NVIDIA K80 GPU implementation in terms of runtime by up to 84x and energy efficiency by up to 1238x for a challenging dataset for historical document image binarization from DIBCO 2017 contest, and a well known MNIST dataset for handwritten digits recognition. Our accelerator demonstrates highest accuracy and comparable throughput in comparison to state-of-the-art FPGA-based implementations of multilayer perceptron for MNIST dataset. Furthermore, we present a new DRAM-PIM architecture for 1D-LSTM targeting energy efficient compute platforms such as portable devices. The DRAM-PIM architecture integrates the computation units in a close proximity to the DRAM cells in order to maximize the data parallelism and energy efficiency. The proposed DRAM-PIM design is 16.19 x more energy efficient as compared to FPGA implementation. The total chip area overhead of this design is 18 % compared to a commodity 8 Gb DRAM chip. Our experiments show that the DRAM-PIM implementation delivers a throughput of 1309.16 GOp/s for an optical character recognition application.
OI Wehn, Norbert/0000-0002-9010-086X
SN 1939-8018
EI 1939-8115
PD NOV
PY 2020
VL 92
IS 11
SI SI
BP 1219
EP 1245
DI 10.1007/s11265-020-01554-x
EA JUL 2020
UT WOS:000545070500001
ER

PT J
AU Yaddanapudi, S
   Cai, B
   Harry, T
   Dolly, S
   Sun, BZ
   Li, H
   Stinson, K
   Noel, C
   Santanam, L
   Pawlicki, T
   Mutic, S
   Goddu, SM
AF Yaddanapudi, Sridhar
   Cai, Bin
   Harry, Taylor
   Dolly, Steven
   Sun, Baozhou
   Li, Hua
   Stinson, Keith
   Noel, Camille
   Santanam, Lakshmi
   Pawlicki, Todd
   Mutic, Sasa
   Goddu, S. Murty
TI Rapid acceptance testing of modern linac using on-board MV and kV
   imaging systems
SO MEDICAL PHYSICS
AB Purpose: The purpose of this study was to develop a novel process for using on-board MV and kV Electronic Portal Imaging Devices (EPIDs) to perform linac acceptance testing (AT) for two reasons: (a) to standardize the assessment of new equipment performance, and (b) to reduce the time to clinical use while reducing physicist workload.
   Methods and materials: In this study, Varian TrueBeam linacs equipped with amorphous silicon-based EPID (aS1000) were used. The conventional set of AT tests and tolerances were used as a baseline guide. A novel methodology was developed or adopted from published literature to perform as many tests as possible using the MVand kV EPIDs. The developer mode on Varian TrueBeam linacs was used to automate the process. In the EPID-based approach, most of mechanical tests were conducted by acquiring images through a custom phantom and software tools were developed for quantitative analysis to extract different performance parameters. The embedded steel-spheres in a custom phantom provided both visual and radiographic guidance for beam geometry testing. For photon beams, open field EPID images were used to extract inline/crossline profiles to verify the beam energy, flatness and symmetry. EPID images through a double wedge phantom were used for evaluating electron beam properties via diagonal profile. Testing was augmented with a commercial automated application (Machine Performance Check) which was used to perform several geometric accuracy tests such as gantry, collimator rotations, and couch rotations/translations.
   Results: The developed process demonstrated that the tests, which required customer demonstration, were efficiently performed using EPIDs. The AT tests that were performed using EPIDs were fully automated using the developer mode on the Varian TrueBeam system, while some tests, such as the light field versus radiation field congruence, and collision interlock checks required user interaction.
   Conclusions: On-board imagers are quite suitable for both geometric and dosimetric testing of linac system involved in AT. Electronic format of the acquired data lends itself to benchmarking, transparency, as well as longitudinal use of AT data. While the tests were performed on a specific model of a linear accelerator, the proposed approach can be extended to other linacs. (C) 2017 American Association of Physicists in Medicine
OI Yaddanapudi, Sridhar/0000-0001-9796-8576; Dolly,
   Steven/0000-0003-1306-8234; Sun, Baozhou/0000-0003-2316-2195
SN 0094-2405
EI 2473-4209
PD JUL
PY 2017
VL 44
IS 7
BP 3393
EP 3406
DI 10.1002/mp.12294
UT WOS:000405103800004
PM 28432806
ER

PT J
AU Li, JL
   Xue, CQ
   Ling, X
   Xie, Y
   Pavan, D
   Chen, HM
   Peng, QB
   Lin, SY
   Li, KS
   Zheng, SY
   Zhou, PY
AF Li, Jiale
   Xue, Chuqing
   Ling, Xiao
   Xie, Yu
   Pavan, Desai
   Chen, Huimin
   Peng, Qinbao
   Lin, Shaoyan
   Li, Kunsheng
   Zheng, Shaoyi
   Zhou, Pengyu
TI A Novel Rat Model of Cardiac Donation After Circulatory Death Combined
   With Normothermic ex situ Heart Perfusion
SO FRONTIERS IN CARDIOVASCULAR MEDICINE
AB Background: In heart transplantation, the adoption of hearts from donation after circulatory death (DCD) is considered to be a promising approach to expanding the donor pool. Normothermic ex situ heart perfusion (ESHP) is emerging as a novel preservation strategy for DCD hearts. Therefore, pre-clinical animal models of ESHP are essential to address some key issues before efficient clinical translation. We aim to develop a novel, reproducible, and economical rat model of DCD protocol combined with normothermic ESHP. Methods: Circulatory death of the anesthetized rats in the DCD group was declared when systolic blood pressure below 30 mmHg or asystole was observed after asphyxiation. Additional 15 min of standoff period was allowed to elapse. After perfusion of cold cardioplegia, the DCD hearts were excised and perfused with allogenic blood-based perfusate at constant flow for 90 min in the normothermic ESHP system. Functional assessment and blood gas analysis were performed every 30 min during ESHP. The alteration of DCD hearts submitted to different durations of ESHP (30, 60, and 90 min) in oxidative stress, apoptosis, tissue energy state, inflammatory response, histopathology, cell swelling, and myocardial infarction during ESHP was evaluated. Rats in the non-DCD group were treated similarly but not exposed to warm ischemia and preserved by the normothermic ESHP system for 90 min. Results: The DCD hearts showed compromised function at the beginning of ESHP and recovered over time, while non-DCD hearts presented better cardiac function during ESHP. The alteration of DCD hearts in oxidative stress, apoptosis, tissue energy state, histopathological changes, cell swelling, and inflammatory response didn't differ among different durations of ESHP. At the end of 90-min ESHP, DCD, and non-DCD hearts presented similarly in apoptosis, oxidative stress, inflammatory response, myocardial infarction, and histopathological changes. Moreover, the DCD hearts had lower energy storage and more evident cell swelling compared to the non-DCD hearts. Conclusion: We established a reproducible, clinically relevant, and economical rat model of DCD protocol combined with normothermic ESHP, where the DCD hearts can maintain a stable state during 90-min ESHP.
RI Zheng, sy/GRS-3136-2022; Zheng, Shaoyi/ABW-1441-2022
SN 2297-055X
PD JUL 23
PY 2021
VL 8
AR 639701
DI 10.3389/fcvm.2021.639701
UT WOS:000708381500001
PM 34368241
ER

PT J
AU Zeng, W
   Ismail, SA
   Pappas, E
AF Zeng, Wei
   Ismail, Shiek Abdullah
   Pappas, Evangelos
TI The impact of feature extraction and selection for the classification of
   gait patterns between ACL deficient and intact knees based on different
   classification models
SO EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING
AB The anterior cruciate ligament (ACL) plays an important role in stabilizing translation and rotation of the tibia relative to the femur. Individuals with ACL deficiency usually demonstrate alterations in gait characteristics. Evidence indicates that walking speed, alterations in kinetics and kinematics on the ACL deficient limb, and inter-limb asymmetries between deficient and intact knees may contribute to poor long-term outcomes following ACL deficiency. They corrode function of the knee joint and put it at higher risk of degeneration. For the purpose of developing an automatic and highly accurate system for detection of ACL deficiency, this study investigated the classification capability of different dynamical features extracted from gait kinematic and kinetic signals when evaluating their impact on different classification models. A general feature extraction framework was proposed and various dynamical features, such as recurrence rate, determinism and entropy from the recurrence quantification analysis, fuzzy entropy, Teager-Kaiser energy feature and statistical analysis, were included. Different classification models, including support vector machine (SVM), K-nearest neighbor (KNN), naive Bayes (NB) classifier, decision tree (DT) classifier and ensemble learning based Adaboost (ELA) classifier, derived for discriminant analysis of multiple dynamical gait features were evaluated for a comparative study. The effectiveness of this strategy was verified using a dataset of knee, hip and ankle kinematic and kinetic waveforms from 43 patients with unilateral ACL deficiency. When evaluated with 2-fold, 10-fold and leave-one-out cross-validation styles, the highest classification accuracy for discriminating between groups of ACL deficient and contralateral ACL intact knees was reported to be 91.22 %, 95.12% and 96.34%, respectively,by using the SVM classifier and the optimal feature set. For other four classifiers, KNN achieved the accuracy of 78.05%, 85.37% and 87.80%, respectively. NB achieved the accuracy of 57.56%, 60.98% and 61.22%, respectively. DT achieved the accuracy of 77.56%, 80.49% and 83.66%, respectively. ELA achieved the accuracy of 73.66%, 78.05% and 79.27%, respectively. Compared with other state-of-the-art methods, the results demonstrate superior performance and support the validity of the proposed method.
RI Pappas, Evangelos/L-4430-2019
OI Pappas, Evangelos/0000-0002-8340-0303
SN 1687-6180
PD OCT 11
PY 2021
VL 2021
IS 1
AR 95
DI 10.1186/s13634-021-00796-6
UT WOS:000706147600001
ER

PT J
AU Liu, PZY
   O'Brien, R
   Heng, SM
   Newall, M
   Downes, S
   Shieh, CC
   Corde, S
   Jackson, M
   Keall, P
AF Liu, Paul Zhi Yuan
   O'Brien, Ricky
   Heng, Soo-Min
   Newall, Matthew
   Downes, Simon
   Shieh, Chun-Chen
   Corde, Stephanie
   Jackson, Michael
   Keall, Paul
TI Development and commissioning of a full-size prototype fixed-beam
   radiotherapy system with horizontal patient rotation
SO MEDICAL PHYSICS
AB Purpose Compared to conventional linacs with rotating gantries, a fixed-beam radiotherapy system could be smaller, more robust and more cost-effective. In this work, we developed and commissioned a prototype x-ray radiotherapy system utilizing a fixed vertical radiation beam and horizontal patient rotation. Methods The prototype system consists of an Elekta Synergy linac with gantry fixed at 0 degrees and a custom-built patient rotation system (PRS). The PRS was designed to immobilize patients and safely rotate them about the horizontal axis. The interlocks and emergency stops of the linac and PRS were connected. Custom software was developed to monitor the system status, control the motion of the PRS and modify treatment plans for the fixed-beam configuration. Following installation, the prototype system was commissioned for three-dimensional (3D) conformal therapy based on guidelines specified in AAPM TG-45 and TG-142, with modifications for the fixed-beam geometry made where necessary. Results The system and control software was tested in a variety of machine states and executed motion, stop and beam gating commands as expected. Interlocks and emergency stops of the linac and PRS were found to correctly stop PRS motion and both kV and MV radiation beams when triggered. For 3D conformal treatments, the prototype system met all AAPM TG-45 and TG-142 specifications for geometric and dosimetric accuracy. Motion of the PRS was within 0.6 +/- 0.3 mm and 0.10 degrees +/- 0.07 degrees of input values for translation and rotation respectively. The axis of rotation of the PRS was coincident with the radiation beam axis to less than 1 mm. End-to-end treatment verification for 6 MV conformal treatments showed less than 2% difference between planned and delivered dose for all fields. Conclusion In this work, we have developed and commissioned a radiotherapy system that utilizes a fixed vertical radiation beam and horizontal patient rotation. This system is a proof-of-concept prototype for a fixed-beam treatment system without a rotating gantry. Fixed-beam systems that are smaller and more cost-effective could help in improving global access to radiotherapy. (c) 2018 American Association of Physicists in Medicine
RI Jackson, Michael/AAF-1868-2019; CORDE, Stéphanie/C-3173-2013; Keall,
   Paul/A-6453-2018
OI Jackson, Michael/0000-0002-9201-4738; CORDE,
   Stéphanie/0000-0003-0388-7340; O'Brien, Ricky/0000-0002-6586-7356; Heng,
   Soo Min/0000-0001-9137-8268; Keall, Paul/0000-0003-4803-6507
SN 0094-2405
EI 2473-4209
PD MAR
PY 2019
VL 46
IS 3
BP 1331
EP 1340
DI 10.1002/mp.13356
UT WOS:000461095300021
PM 30582751
ER

PT J
AU Jena, DP
   Panigrahi, SN
   Kumar, R
AF Jena, D. P.
   Panigrahi, S. N.
   Kumar, Rajesh
TI Multiple-teeth defect localization in geared systems using filtered
   acoustic spectrogram
SO APPLIED ACOUSTICS
AB Contactless health monitoring of machines is highly desirable in industrial setups where the environment inherently imposes restrictions on contact-based data acquisition. This motivates the use of acoustic signal as an effective alternative for condition monitoring of equipments located in such inaccessible environments. However, condition monitoring and fault diagnosis by processing acoustic signals still remains a challenge for researchers. The aim of the proposed work is to establish a robust technique of acoustic signal processing for detection and localization of multiple teeth defect in geared systems. Towards this, the present work proposes the use of time marginal integration (TMI) of the continuous wavelet transform (CWT) coefficients of the decomposed signal derived from an undecimated wavelet transform (UWT) of the raw acoustic signal. UWT, owing to its well established translation invariant property, is implemented on the raw data to extract the de-noised signal for further processing with CWT. The time-axis of the TMI graph is finally correlated to the angular displacement of the driver gear in order to locate the defective teeth and measure their relative positions. An artificial neural network (ANN) model using signal statistical parameters as neurons is proposed as a pre-check to identify the presence of any defect in the gears. In addition, the efficiency of UWT as a de-noising tool is reestablished through the accuracy improvement in ANN based identification. A synthetic signal is simulated to conceptualize and evaluate the effectiveness of the proposed method. Synthetic signal analysis also offers vital clues about the suitability of the biorthogonal 3.1 wavelet over Daubechies and Symlet wavelets in the proposed algorithm. The experimental validation of the proposed method is presented using a customized gear drive test setup by introducing gears with seeded defects in one or more of their teeth. Measurement of the angles between two or more damaged teeth with a high level of accuracy is shown to be possible using the proposed algorithm. Experiments reveal that acoustic signal analysis can be used as a suitable contactless alternative for precise gear defect identification and gear health monitoring. (C) 2013 Elsevier Ltd. All rights reserved.
RI yanxue, wang/C-8336-2016; Kumar, Rajesh/AAY-2468-2021
OI KUMAR, RAJESH/0000-0002-7284-2309; , dibya prakash
   jena/0000-0003-2159-6539
SN 0003-682X
EI 1872-910X
PD JUN
PY 2013
VL 74
IS 6
BP 823
EP 833
DI 10.1016/j.apacoust.2012.12.010
UT WOS:000317152300005
ER

PT J
AU Kofman, AG
   Ashhab, S
   Nori, F
AF Kofman, Abraham G.
   Ashhab, Sahel
   Nori, Franco
TI Nonperturbative theory of weak pre- and post-selected measurements
SO PHYSICS REPORTS-REVIEW SECTION OF PHYSICS LETTERS
AB This paper starts with a brief review of the topic of strong and weak pre- and post-selected (PPS) quantum measurements, as well as weak values, and afterwards presents original work. In particular, we develop a nonperturbative theory of weak PPS measurements of an arbitrary system with an arbitrary meter, for arbitrary initial states of the system and the meter. New and simple analytical formulas are obtained for the average and the distribution of the meter pointer variable. These formulas hold to all orders in the weak value. In the case of a mixed preselected state, in addition to the standard weak value, an associated weak value is required to describe weak PPS measurements. In the linear regime, the theory provides the generalized Aharonov-Albert-Vaidman formula. Moreover, we reveal two new regimes of weak PPS measurements: the strongly-nonlinear regime and the inverted region (the regime with a very large weak value), where the system-dependent contribution to the pointer deflection decreases with increasing the measurement strength. The optimal conditions for weak PPS measurements are obtained in the strongly-nonlinear regime, where the magnitude of the average pointer deflection is equal or close to the maximum. This maximum is independent of the measurement strength, being typically of the order of the pointer uncertainty. In the optimal regime, the small parameter of the theory is comparable to the overlap of the pre- and post-selected states. We show that the amplification coefficient in the weak PPS measurements is generally a product of two qualitatively different factors. The effects of the free system and meter Hamiltonians are discussed. We also estimate the size of the ensemble required for a measurement and identify optimal and efficient meters for weak measurements. Exact solutions are obtained for a certain class of the measured observables. These solutions are used for numerical calculations, the results of which agree with the theory. Moreover, the theory is extended to allow for a completely general post-selection measurement. We also discuss time-symmetry properties of PPS measurements of any strength and the relation between PPS and standard (not post-selected) measurements. (c) 2012 Elsevier B.V. All rights reserved.
RI Nori, Franco/B-1222-2009; Ashhab, Sahel/I-4794-2012; Kofman,
   Abraham/A-6838-2013
OI Nori, Franco/0000-0003-3682-7432; Ashhab, Sahel/0000-0003-1931-1178;
   Kofman, Abraham/0000-0001-5816-6415
SN 0370-1573
EI 1873-6270
PD NOV
PY 2012
VL 520
IS 2
BP 43
EP 133
DI 10.1016/j.physrep.2012.07.001
UT WOS:000310942900001
ER

PT J
AU Dunkelberger, N
   Schearer, EM
   O'Malley, MK
AF Dunkelberger, Nathan
   Schearer, Eric M.
   O'Malley, Marcia K.
TI A review of methods for achieving upper limb movement following spinal
   cord injury through hybrid muscle stimulation and robotic assistance
SO EXPERIMENTAL NEUROLOGY
AB Individuals with tetraplegia, typically attributed to spinal cord injuries (SCI) at the cervical level, experience significant health care costs and loss of independence due to their limited reaching and grasping capabilities. Neuromuscular electrical stimulation (NMES) is a promising intervention to restore arm and hand function because it activates a person's own paralyzed muscles; however, NMES sometimes lacks the accuracy and repeatability necessary to position the limb for functional tasks, and repeated muscle stimulation can lead to fatigue. Robotic devices have the potential to restore function when used as assistive devices to supplement or replace limited or lost function of the upper limb following SCI. Unfortunately, most robotic solutions are bulky or require significant power to operate, limiting their applicability to restore functional independence in a home environment. Combining NMES and robotic support systems into a single hybrid neuroprosthesis is compelling, since the robotic device can supplement the action of the muscles and improve repeatability and accuracy. Research groups have begun to explore applications of movement assistance for individuals with spinal cord injury using these technologies in concert. In this review, we present the state of the art in hybrid NMES-orthotic systems for upper limb movement restoration following spinal cord injury, and suggest areas for emphasis necessary to move the field forward. Currently, NMES-robotic systems use either surface or implanted electrodes to stimulate muscles, with rigid robotic supports holding the limb against gravity, or providing assistance in reaching movements. Usability of such systems outside of the lab or clinic is limited due to the complexity of both the mechanical components, stimulation systems, and human-machine interfaces. Assessment of system and participant performance is not reported in a standardized way. Future directions should address wearability through improvements in component technologies and user interfaces. Further, increased integration of the control action between NMES and robotic subsystems to reanimate the limb should be pursued. Standardized reporting of system performance and expanded clinical assessments of these systems are also needed. All of these advancements are critical to facilitate translation from lab to home.
OI O'Malley, Marcia/0000-0002-3563-1051; Dunkelberger,
   Nathan/0000-0002-2398-1986
SN 0014-4886
EI 1090-2430
PD JUN
PY 2020
VL 328
AR 113274
DI 10.1016/j.expneurol.2020.113274
UT WOS:000527888300002
PM 32145251
ER

PT C
AU Borgolte, K
   Kruegel, C
   Vigna, G
AF Borgolte, Kevin
   Kruegel, Christopher
   Vigna, Giovanni
GP Assoc Comp Machinery
TI Relevant Change Detection A Framework for the Precise Extraction of
   Modified and Novel Web-based Content as a Filtering Technique for
   Analysis Engines
SO WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON
   WORLD WIDE WEB
CT 23rd International Conference on World Wide Web (WWW)
CY APR 07-11, 2014
CL Seoul, SOUTH KOREA
SP Assoc Comp Machinery, IW3C2, ACM SIGWEB, NAVER, Google, Microsoft, SK Planet, Facebook, Yahoo! Labs, Daum, European Patent Off, LG Elect, Baidu, Kaggle, Samsung, Korea Tourism Org, Korean Federat Sci & Technol Soc, Seoul Metropolitan Govt, Korea Inst Informat Scientists & Engineers, Int World Wide Web Conf Steering Comm, Korea Adv Inst Sci & Technol, Korean Agcy Technol & Stand
AB Tracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content.
   Prior work in the area of change detection, such as XyDiff [3], X-Diff [8] or AT&T's internet difference engine [4], focused mainly on "diffing" XML-encoded literary documents or XML-encoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload.
   In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages.
   In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns [2] on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software.
BN 978-1-4503-2745-9
PY 2014
BP 595
EP 597
DI 10.1145/2567948.2578039
UT WOS:000455947000198
ER

PT J
AU Yoon, H
   Bak, MS
   Ha Kim, S
   Lee, JH
   Chung, G
   Kim, SJ
   Kim, SK
AF Yoon, Heera
   Bak, Myeong Seong
   Ha Kim, Seung
   Lee, Ji Hwan
   Chung, Geehoon
   Kim, Sang Jeong
   Kim, Sun Kwang
TI Development of a spontaneous pain indicator based on brain cellular
   calcium using deep learning
SO EXPERIMENTAL AND MOLECULAR MEDICINE
AB Chronic pain remains an intractable condition in millions of patients worldwide. Spontaneous ongoing pain is a major clinical problem of chronic pain and is extremely challenging to diagnose and treat compared to stimulus-evoked pain. Although extensive efforts have been made in preclinical studies, there still exists a mismatch in pain type between the animal model and humans (i.e., evoked vs. spontaneous), which obstructs the translation of knowledge from preclinical animal models into objective diagnosis and effective new treatments. Here, we developed a deep learning algorithm, designated AI-bRNN (Average training, Individual test-bidirectional Recurrent Neural Network), to detect spontaneous pain information from brain cellular Ca2+ activity recorded by two-photon microscopy imaging in awake, head-fixed mice. AI-bRNN robustly determines the intensity and time points of spontaneous pain even in chronic pain models and evaluates the efficacy of analgesics in real time. Furthermore, AI-bRNN can be applied to various cell types (neurons and glia), brain areas (cerebral cortex and cerebellum) and forms of somatosensory input (itch and pain), proving its versatile performance. These results suggest that our approach offers a clinically relevant, quantitative, real-time preclinical evaluation platform for pain medicine, thereby accelerating the development of new methods for diagnosing and treating human patients with chronic pain.
   Pain: AI-based platform could aid analgesic drug discovery A microscopy technique coupled with an artificial intelligence (AI) platform could help researchers discover new types of pain-relief medicines. A team from South Korea led by Sun Kwang Kim of Kyung Hee University and Sang Jeong Kim of Seoul National University created a machine-learning algorithm that converts calcium signaling data in the brain, as estimated via imaging on genetically engineered mice, into a measurement of pain intensity. The researchers applied the technique to several mouse models of chronic pain and showed that it accurately captured the analgesic effects of known painkillers. They also extended the system to multiple brain regions, cell types and another brain-controlled sensory process, itch. The researchers propose using the AI-based tool to evaluate candidate anti-pain and anti-itch medicines ahead of human trials.
OI Chung, Geehoon/0000-0003-0771-4391; Kim, Sang Jeong/0000-0001-8931-3713
SN 1226-3613
EI 2092-6413
PD AUG
PY 2022
VL 54
IS 8
BP 1179
EP 1187
DI 10.1038/s12276-022-00828-7
EA AUG 2022
UT WOS:000842196100001
PM 35982300
ER

PT J
AU Zabihollahy, F
   Rajchl, M
   White, JA
   Ukwatta, E
AF Zabihollahy, Fatemeh
   Rajchl, Martin
   White, James A.
   Ukwatta, Eranga
TI Fully automated segmentation of left ventricular scar from 3D late
   gadolinium enhancement magnetic resonance imaging using a cascaded
   multi-planar U-Net (CMPU-Net)
SO MEDICAL PHYSICS
AB Purpose Three-dimensional (3D) late gadolinium enhancement magnetic resonance (LGE-MR) imaging enables the quantification of myocardial scar at high resolution with unprecedented volumetric visualization. Automated segmentation of myocardial scar is critical for the potential clinical translation of this technique given the number of tomographic images acquired.
   Methods In this paper, we describe the development of cascaded multi-planar U-Net (CMPU-Net) to efficiently segment the boundary of the left ventricle (LV) myocardium and scar from 3D LGE-MR images. In this approach, two subnets, each containing three U-Nets, were cascaded to first segment the LV myocardium and then segment the scar within the presegmented LV myocardium. The U-Nets were trained separately using two-dimensional (2D) slices extracted from axial, sagittal, and coronal slices of 3D LGE-MR images. We used 3D LGE-MR images from 34 subjects with chronic ischemic cardiomyopathy. The U-Nets were trained using 8430 slices, extracted in three orthogonal directions from 18 images. In the testing phase, the outputs of U-Nets of each subnet were combined using the majority voting system for final label prediction of each voxel in the image. The developed method was tested for accuracy by comparing its results to manual segmentations of LV myocardium and LV scar from 7250 slices extracted from 16 3D LGE-MR images. Our method was also compared to numerous alternative methods based on machine learning, energy minimization, and intensity-thresholds.
   Results Our algorithm reported a mean dice similarity coefficient (DSC), absolute volume difference (AVD), and Hausdorff distance (HD) of 85.14% +/- 3.36%, 43.72 +/- 27.18 cm(3), and 19.21 +/- 4.74 mm for determining the boundaries of LV myocardium from LGE-MR images. Our method also yielded a mean DSC, AVD, and HD of 88.61% +/- 2.54%, 9.33 +/- 7.24 cm(3), and 17.04 +/- 9.93 mm for LV scar segmentation on the unobserved test dataset. Our method significantly outperformed the alternative techniques in segmentation accuracy (P < 0.05).
   Conclusions The CMPU-Net method provided fully automated segmentation of LV scar from 3D LGE-MR images and outperformed the alternative techniques.
RI Zabihollahy, Fatemeh/AAL-5827-2020
OI Zabihollahy, Fatemeh/0000-0003-3362-1009
SN 0094-2405
EI 2473-4209
PD APR
PY 2020
VL 47
IS 4
BP 1645
EP 1655
DI 10.1002/mp.14022
EA FEB 2020
UT WOS:000512208000001
PM 31955415
ER

PT J
AU Leo, M
   Cazzato, D
   De Marco, T
   Distante, C
AF Leo, Marco
   Cazzato, Dario
   De Marco, Tommaso
   Distante, Cosimo
TI Unsupervised Eye Pupil Localization through Differential Geometry and
   Local Self-Similarity Matching
SO PLOS ONE
AB The automatic detection and tracking of human eyes and, in particular, the precise localization of their centers (pupils), is a widely debated topic in the international scientific community. In fact, the extracted information can be effectively used in a large number of applications ranging from advanced interfaces to biometrics and including also the estimation of the gaze direction, the control of human attention and the early screening of neurological pathologies. Independently of the application domain, the detection and tracking of the eye centers are, currently, performed mainly using invasive devices. Cheaper and more versatile systems have been only recently introduced: they make use of image processing techniques working on periocular patches which can be specifically acquired or preliminarily cropped from facial images. In the latter cases the involved algorithms must work even in cases of non-ideal acquiring conditions (e.g in presence of noise, low spatial resolution, non-uniform lighting conditions, etc.) and without user's awareness (thus with possible variations of the eye in scale, rotation and/or translation). Getting satisfying results in pupils' localization in such a challenging operating conditions is still an open scientific topic in Computer Vision. Actually, the most performing solutions in the literature are, unfortunately, based on supervised machine learning algorithms which require initial sessions to set the working parameters and to train the embedded learning models of the eye: this way, experienced operators have to work on the system each time it is moved from an operational context to another. It follows that the use of unsupervised approaches is more and more desirable but, unfortunately, their performances are not still satisfactory and more investigations are required. To this end, this paper proposes a new unsupervised approach to automatically detect the center of the eye: its algorithmic core is a representation of the eye's shape that is obtained through a differential analysis of image intensities and the subsequent combination with the local variability of the appearance represented by self-similarity coefficients. The experimental evidence of the effectiveness of the method was demonstrated on challenging databases containing facial images. Moreover, its capabilities to accurately detect the centers of the eyes were also favourably compared with those of the leading state-of-the-art methods.
RI Leo, Marco/AAG-6296-2019; Cazzato, Dario/O-4780-2019; Distante,
   Cosimo/M-7996-2013
OI Leo, Marco/0000-0001-5636-6130; Cazzato, Dario/0000-0002-5472-9150;
   Distante, Cosimo/0000-0002-1073-2390
SN 1932-6203
PD AUG 14
PY 2014
VL 9
IS 8
AR e102829
DI 10.1371/journal.pone.0102829
UT WOS:000341017000006
PM 25122452
ER

PT J
AU Dragoo, JL
   Padrez, K
   Workman, R
   Lindsey, DR
AF Dragoo, Jason L.
   Padrez, Kevin
   Workman, Rosemary
   Lindsey, Derek R.
TI The effect of relaxin on the female anterior cruciate ligament: Analysis
   of mechanical properties in an animal model
SO KNEE
AB Background: The peptide hormone relaxin, found in pregnant and non-pregnant females, has been shown to have collagenolytic effects on ligamentous tissue. Relaxin receptors have recently been identified on the human female anterior cruciate ligament (ACL). Relaxin may affect the load bearing properties of the female ACL and contribute to non-contact ACL injuries.
   Hypothesis: The administration of recombinant relaxin estrogen will lead to a significant decrease in ACL integrity in the guinea pig model.
   Study design: Controlled laboratory study.
   Methods: Adult female guinea pigs were divided into three experimental groups. Group 1 (n=4) was administered only 20 mu g/h of recombinant porcine relaxin for 3 weeks. Group 2 (n=4) was administered 20 mu g/h of recombinant porcine relaxin and 5 mu g/h of estradiol for 3 weeks. Group 3 (n=4) served as both a normal control before surgical transection of the ACL and a positive ACL tear control after transection. All hormones were administered using separate implanted osmotic pumps. ACL laxity was tested by implanting radio-opaque markers in the femur and tibia of each leg. After applying a standard anterior force (22 N), the distance between markers was measured radiographically at day 0 and day 21. The animals were then sacrificed and the ACL's were analyzed for load-to-failure using a material testing machine.
   Results: Load-to-failure testing indicated that animals treated with relaxin only had significantly weaker ACL's (mu=40.4 N, p=0.001) compared to controls (mu=64.1 N). The relaxin+estrogen group (mu=32.7 N) was also significantly weaker than controls (p=0.007). There were no statistical differences between relaxin and relaxin+estrogen groups. Both relaxin only and relaxin+estrogen groups showed an increase in anterior translation of the tibia after 3 weeks of infusion, but it did not achieve statistical significance.
   Conclusions: Relaxin significantly alters the mechanical properties of the ACL in an animal model. Clinical relevance: The effects of relaxin, possibly in conjunction with estrogen, may contribute to a comprehensive etiology for human female non-contact ACL injuries. (C) 2008 Elsevier BY. All rights reserved.
RI Lindsey, Derek/K-2579-2019
OI Lindsey, Derek/0000-0003-2699-8190
SN 0968-0160
PD JAN
PY 2009
VL 16
IS 1
BP 69
EP 72
DI 10.1016/j.knee.2008.09.005
UT WOS:000262743000015
PM 18964043
ER

PT J
AU Daberdaku, S
   Ferrari, C
AF Daberdaku, Sebastian
   Ferrari, Carlo
TI Exploring the potential of 3D Zernike descriptors and SVM for
   protein-protein interface prediction
SO BMC BIOINFORMATICS
AB Background: The correct determination of protein-protein interaction interfaces is important for understanding disease mechanisms and for rational drug design. To date, several computational methods for the prediction of protein interfaces have been developed, but the interface prediction problem is still not fully understood. Experimental evidence suggests that the location of binding sites is imprinted in the protein structure, but there are major differences among the interfaces of the various protein types: the characterising properties can vary a lot depending on the interaction type and function. The selection of an optimal set of features characterising the protein interface and the development of an effective method to represent and capture the complex protein recognition patterns are of paramount importance for this task.
   Results: In this work we investigate the potential of a novel local surface descriptor based on 3D Zernike moments for the interface prediction task. Descriptors invariant to roto-translations are extracted from circular patches of the protein surface enriched with physico-chemical properties from the HQI8 amino acid index set, and are used as samples for a binary classification problem. Support Vector Machines are used as a classifier to distinguish interface local surface patches from non-interface ones. The proposed method was validated on 16 classes of proteins extracted from the Protein-Protein Docking Benchmark 5.0 and compared to other state-of-the-art protein interface predictors (SPPIDER, PrISE and NPS-HomPPI).
   Conclusions: The 3D Zernike descriptors are able to capture the similarity among patterns of physico-chemical and biochemical properties mapped on the protein surface arising from the various spatial arrangements of the underlying residues, and their usage can be easily extended to other sets of amino acid properties. The results suggest that the choice of a proper set of features characterising the protein interface is crucial for the interface prediction task, and that optimality strongly depends on the class of proteins whose interface we want to characterise. We postulate that different protein classes should be treated separately and that it is necessary to identify an optimal set of features for each protein class.
RI Daberdaku, Sebastian/U-9846-2019; Ferrari, Carlo/AAU-8107-2021;
   Daberdaku, Sebastian/H-7333-2016
OI Daberdaku, Sebastian/0000-0003-0544-1465; Daberdaku,
   Sebastian/0000-0003-0544-1465
SN 1471-2105
PD FEB 6
PY 2018
VL 19
AR 35
DI 10.1186/s12859-018-2043-3
UT WOS:000424369800002
PM 29409446
ER

PT J
AU Mills-Finnerty, C
   Frangos, E
   Allen, K
   Komisaruk, B
   Wise, N
AF Mills-Finnerty, Colleen
   Frangos, Eleni
   Allen, Kachina
   Komisaruk, Barry
   Wise, Nan
TI Functional Magnetic Resonance Imaging Studies in Sexual Medicine: A
   Primer
SO JOURNAL OF SEXUAL MEDICINE
AB Background: Over the past 30 years, functionalmagnetic resonance imaging (fMRI) has emerged as a powerful tool to non-invasively study the activity and function of the human brain. But along with the potential of fMRI to shed light on neurological, psychiatric, and psychological processes, there are methodological challenges and criticisms. Aim: We herein provide an fMRI primer designed for a diverse audience, from the neuroimaging novice to the experienced user. Methods: This primer is structured as follows: Part 1: Overview: "What is fMRI and what can it tell us?." Part 2: Basic fMRI principles: MR physics, the BOLD signal, and components of a typical scan session. Part 3: Basic fMRI experimental design: why timing is critical, and common sources of noise in the signal. Part 4: Basic fMRI analysis methods: software, the 3 stages of data analysis (preprocessing, individual, and group level), and a survey of advanced topics and methods including connectivity, machine learning, and assessing statistical significance. Part 5: Criticism, crises, and opportunities related to power of studies, computing requirements, logistical, and interpretational challenges, and methodological debate (assessing causality, circular correlations, and open science best practices). Outcomes n/a Clinical Translation: fMRI has primarily been used in clinical research to elucidate the brain correlates of sexual behavior. The translational potential of the method into clinical practice has not yet been realizedfMRI has primarily been used in clinical research to elucidate the brain correlates of sexual behavior. The translational potential of the method into clinical practice has not yet been realized Strengths and Limitations: fMRI is a useful and powerful tool for understanding the brain basis of human sexuality. However, it is also expensive, requires extensive methods expertise, and lacks the precision needed to be immediately translatable to clinical practice. The recency of the method, need for basic research, technical limitations, as well as inherent variability in individuals brain activity also impact the pace at which fMRI for sexual medicine can move from the scanner to the clinic. Conclusion: This primer provides the novice an understanding of the appropriate uses and limitations of fMRI, and for the experienced user, a concise update on current issues and methodological advances.
RI Allen, Kachina/O-5191-2018
OI Allen, Kachina/0000-0003-3828-8030
SN 1743-6095
EI 1743-6109
PD JUL
PY 2022
VL 19
IS 7
BP 1073
EP 1089
DI 10.1016/j.jsxm.2022.03.217
UT WOS:000911711600005
PM 35422400
ER

PT J
AU Muhamad, MR
   Jamaludin, MF
   Ab Karim, MS
   Yusof, F
   Zou, Y
AF Muhamad, M. R.
   Jamaludin, M. F.
   Ab Karim, M. S.
   Yusof, F.
   Zou, Y.
TI Effects of electrolysis on magnetic abrasive finishing of AA6063-T1 tube
   internal surface using combination machining tool
SO MATERIALWISSENSCHAFT UND WERKSTOFFTECHNIK
AB In this research, we studied the effects of electrolysis in the magnetic abrasive finishing of an AA6063-T1 tube internal surface. The finishing surfaces hairline morphology was removed in a short time by physical characteristic transformation of the finishing surface in which the aluminum oxide film was formed during the electrolysis. Next, magnetic abrasive finishing was used to eliminate the oxidation layer and polish the surface to a mirror-finishing standard. The two-step process effectively improved the surface roughness in a shorter time. The morphology changes before and after the finishing process, was studied by surface roughness measurement and scanning electron microscope photographs. Notably, the pit or micro holes formation during the electrolysis on the aluminum oxide film was examined and its effect on the surface roughness was studied. The elements' residual on the surface was investigated by X-ray photoelectron spectroscopy analyzer before and after the process to confirm the formation and removal of oxidation film on the finishing surface. The lower value for torque measurement in electrolysis combined process compared to the conventional method was due to the porous characteristic of aluminum oxide film.
   Translation abstract In dieser Studie wurden die Auswirkungen der Elektrolyse in der magnetischen Schleifbearbeitung einer AA6063-T1-Rohrinnenoberflache untersucht. Die Haarlinienmorphologie der Appreturoberflache wurde in kurzer Zeit durch physikalisch charakteristische Umwandlung der Appreturoberflache, in der der Aluminiumoxidfilm wahrend der Elektrolyse gebildet wurde, entfernt. Als nachstes wurde eine magnetische Schleifbearbeitung verwendet, um die Oxidationsschicht zu beseitigen und die Oberflache zu einem Hochglanzpolierstandard zu polieren. Der zweistufige Prozess verbesserte die Oberflachenrauheit in kurzerer Zeit. Die Veranderungen der Morphologie vor und nach der Fertigstellung wurden durch Oberflachenrauheitsmessungen und Rasterelektronenmikroskopaufnahmen untersucht. Bemerkenswerterweise wurde die Bildung von Grubchen oder Mikrolochern wahrend der Elektrolyse auf dem Aluminiumoxidfilm untersucht und ihre Auswirkung auf die Oberflachenrauheit untersucht. Die Reste der Elemente auf der Oberflache wurden mit einem Rontgenphotoelektronenspektroskopie-Analysegerat vor und nach dem Verfahren untersucht, um die Bildung und Entfernung des Oxidationsfilms auf der Endoberflache zu bestatigen. Der niedrigere Wert fur die Drehmomentmessung im kombinierten Elektrolyseverfahren im Vergleich zu dem herkommlichen Verfahren war auf die porose Eigenschaft des Aluminiumoxidfilms zuruckzufuhren.
RI bin Muhamad, Mohd Ridha/S-8078-2016; YUSOF, FARAZILA/B-5217-2010; Karim,
   Mohd Sayuti Ab/F-4908-2012; Jamaludin, Mohd Fadzil/I-4236-2014; Muhamad,
   Mohd/S-7461-2019; yusof, farazila/ABB-5202-2021
OI bin Muhamad, Mohd Ridha/0000-0002-2428-1025; YUSOF,
   FARAZILA/0000-0001-6034-0996; Jamaludin, Mohd
   Fadzil/0000-0003-0469-5623; Muhamad, Mohd/0000-0002-2428-1025; yusof,
   farazila/0000-0001-6034-0996
SN 0933-5137
EI 1521-4052
PD APR
PY 2018
VL 49
IS 4
SI SI
BP 442
EP 452
DI 10.1002/mawe.201700266
UT WOS:000431007700005
ER

PT J
AU Brangier, E
   Pino, P
AF Brangier, E
   Pino, P
TI Amyotrophic lateral sclerosis: Ergonomic approach of patient assistance
SO TRAVAIL HUMAIN
AB The general purpose of this paper is to introduce assistive technologies from the point of view of ergonomics. This approach is based on an appropriate understanding of the requirements of people with motor impairments, to design augmentative or palliative forms communication.
   First, the authors summarise of the main features of Amyotrophic Lateral Sclerosis (ALS) and its medical, psychological and technological approach, with a view to having a better understanding of this disease and reducing suffering.
   Second, the paper presents interactive telethesis and its functions. The user can have some control on some aspects of his/her immediate environment : 1 / controlling alarm signals : bell-warning medical staff; 2 / text interfacing: reading with integrated page markers and writing/editing text with online assistance for ward prediction (currently based on a 90 000 word lexicon); 3 / television remote control : channel and volume control; 4 / CD reader control to listen to music and audio books; 5 / using pre-recorded sentences to communicate with nurses, Friends and relatives. The objective of the investigation is to remedy the gradual deterioration of the patient's interactive functions by designing new procedures for interaction that ave compatible with the central cognitive process, and to alleviate practical diffculties in the last few years of life for people suffering motor impairments. Telethesis can be viewed as an extension of interactive users who have lost most of their motion capacity. In fact, telethesis tends to support the development of intellectual, affective and social stimuli.
   Third, the paper presents the results of a 6-month analysis and sequential recording (240 hours of technical monitoring) of all the actions performed by a patient. The data collected made it possible to optimise the man-machine interface and to evaluate the impact of telethesis on the emotional ease experienced by a patient affected by Ars. The results show that assistive technologies designed from the viewpoint of ergonomics and on the basis of a better understanding of the stream of consciousness developing in the final years of life make it possible, not only to compensate partly for the physical deterioration, but also to develop adaptive strategies. Our findings served as a basis to elaborate a type of palliative communication to be implemented interactively supported by powerful mechanisms to cope with the hardships associated with this crippling disease.
OI Brangier, Eric/0000-0001-6987-8602
SN 0041-1868
PD JUN
PY 2000
VL 63
IS 2
BP 171
EP 190
UT WOS:000087419500004
ER

PT J
AU Campbell, DE
AF Campbell, Daniel E.
TI Emergy baseline for the Earth: A historical review of the science and a
   new calculation
SO ECOLOGICAL MODELLING
AB Quantifying the emergy baseline of the Earth is a practical necessity for emergy evaluations, because it serves as a unified basis for determining transformities of the available energy storages and flows of the geobiosphere. The current debate over the value and significance of the planetary baseline has been in progress since 1998, when the author first brought new data on geopotential energy formation in the world oceans to H.T. Odum's attention. In this paper, past studies of the baseline were reviewed and errors in data translation and model formulation were found to be sufficient to justify a new calculation. A fundamental epistemological obstacle to establishing a unified planetary baseline (i.e., the production functions for deep Earth heat and tide as a function of solar radiation are unknown) is overcome by using the transitive property of equalities to estimate equivalences between solar radiation and Earth's deep heat exergy flows (4200 solar equivalent joules per joule, seJ J(-1)) and between the exergy of solar radiation and the tidal exergy dissipated in the oceans (35,400 seJ J(-1)). At present, the planetary baseline for the Earth with its ice-covered, polar oceans is approximately 1.16 x 10(25) seJ y(-1) and the distribution of the emergy or the organizing power of the inputs is: 1/3 solar radiation, 1/3 deep Earth heat and 1/3 tidal geopotential energy. In addition, the planetary baseline has been remarkably stable over the past 555,000,000y (1.00 x 10(25) +/- 1.13 x 10(24) seJ y(-1) or within +/- 11%). The tidal exergy dissipated in the world oceans over this time varies from 31% to 155% of its present value largely due to the changing efficiency of the Earth as a "machine" for generating tidal exergy. Close correspondence of the value and properties of this new baseline with the principles of Energy Systems Theory indicates that it should be preferred over prior determinations. Published by Elsevier B.V.
SN 0304-3800
EI 1872-7026
PD NOV 10
PY 2016
VL 339
SI SI
BP 96
EP 125
DI 10.1016/j.ecolmodel.2015.12.010
UT WOS:000384871000011
ER

PT C
AU Pagliarini, L
   Lund, HH
AF Pagliarini, Luigi
   Lund, Henrik Hautop
BE Ito, T
   Lee, JJ
   Jia, Y
   Sugisaka, M
TI ALife for Real and Virtual Audio-Video Performances
SO PROCEEDINGS OF INTERNATIONAL CONFERENCE ON ARTIFICIAL LIFE AND ROBOTICS
   (ICAROB 2014)
CT International Conference on Artificial Life and Robotics (ICAROB)
CY JAN 11-13, 2014
CL Oita, JAPAN
SP Int Conf Artificial Life and Robot, Int Steering Comm, ALife Robot Corp Ltd, Int Steering Comm, IEEE Robot & Automat Soc, Fukuoka Sect, Chinese Assoc Artificial Intelligence, Univ Sultan Zainal Abidin
AB MAG (i.e.: an Italian acronym which stands for Musical Genetic Algorithms) is an electronic art piece in which a multifaceted software attempts to "translate" musical expression into a corresponding static or animated graphical expressions. The mechanism at the base of such "translation" consists in a quite complex and articulated algorithm that, in short, is based on artificial learning. Indeed, MAG implements different learning techniques to allow artificial agents to learn about music flow by developing an adaptive behaviour. In our specific case, such a technique consists of a population of neural networks - one dimensional artificial agents that populate their two dimensional artificial world, and which are served by a simple input output control system - that can use both genetic and reinforcement learning algorithms to evolve appropriate behavioural answers to an impressively large shapes of inputs, through both a fitness formula based genetic pressure, and, eventually, a user-machine based feedbacks.
   More closely, in the first version of MAG algorithm the agents' control system is a perceptron; the world of the agents is a two dimensional grid that changes its dimensions accordingly to the host-screen; the most important input artificial agents get (i.e. not necessarily the only one) is the musical wave that any given musical file produces, run-time; the output is the behavioural answer that agents produce by moving, and thereby drawing on to a computer screen, therefore graphical. The combination of artificial evolution and the flows of a repeated song or different musical tunes make it possible for the software to obtain a special relationship between sound waves and the aesthetics of consequent graphical results.
   Further, we started to explore the concept of run-time creation of both music and graphical expression. Recently, we developed a software by which it is possible to allow any user to create new song versions of popular music with the MusicTiles app simply by connecting musical building blocks. This creation of musical expression can happen as a performance (i.e. run-time). When connecting the MusicTiles app to the MAG software, we provide the connection and the possibility to melt both musical expression and graphical expression in parallel and at run-time, and therefore creating a audio-video performance that is always unique.
BN 978-4-9902880-8-2
PY 2014
BP 5
EP 9
UT WOS:000387182800002
ER

PT J
AU Wangmo, T
   Lipps, M
   Kressig, RW
   Ienca, M
AF Wangmo, Tenzin
   Lipps, Mirjam
   Kressig, Reto W.
   Ienca, Marcello
TI Ethical concerns with the use of intelligent assistive technology:
   findings from a qualitative study with professional stakeholders
SO BMC MEDICAL ETHICS
AB Background: Advances in artificial intelligence (AI), robotics and wearable computing are creating novel technological opportunities for mitigating the global burden of population ageing and improving the quality of care for older adults with dementia and/or age-related disability. Intelligent assistive technology (IAT) is the umbrella term defining this ever-evolving spectrum of intelligent applications for the older and disabled population. However, the implementation of IATs has been observed to be sub-optimal due to a number of barriers in the translation of novel applications from the designing labs to the bedside. Furthermore, since these technologies are designed to be used by vulnerable individuals with age- and multi-morbidity-related frailty and cognitive disability, they are perceived to raise important ethical challenges, especially when they involve machine intelligence, collect sensitive data or operate in close proximity to the human body. Thus, the goal of this paper is to explore and assess the ethical issues that professional stakeholders perceive in the development and use of IATs in elderly and dementia care.
   Methods: We conducted a multi-site study involving semi-structured qualitative interviews with researchers and health professionals. We analyzed the interview data using a descriptive thematic analysis to inductively explore relevant ethical challenges.
   Results: Our findings indicate that professional stakeholders find issues of patient autonomy and informed consent, quality of data management, distributive justice and human contact as ethical priorities. Divergences emerged in relation to how these ethical issues are interpreted, how conflicts between different ethical principles are resolved and what solutions should be implemented to overcome current challenges.
   Conclusions: Our findings indicate a general agreement among professional stakeholders on the ethical promises and challenges raised by the use of IATs among older and disabled users. Yet, notable divergences persist regarding how these ethical challenges can be overcome and what strategies should be implemented for the safe and effective implementation of IATs. These findings provide technology developers with useful information about unmet ethical needs. Study results may guide policy makers with firsthand information from relevant stakeholders about possible solutions for ethically-aligned technology governance.
OI Wangmo, Tenzin/0000-0003-0857-0510; Ienca, Marcello/0000-0001-8835-5444
SN 1472-6939
PD DEC 19
PY 2019
VL 20
IS 1
AR 98
DI 10.1186/s12910-019-0437-z
UT WOS:000511150000002
PM 31856798
ER

PT J
AU Speaker, T
   O'Donnell, S
   Wittemyer, G
   Bruyere, B
   Loucks, C
   Dancer, A
   Carter, M
   Fegraus, E
   Palmer, J
   Warren, E
   Solomon, J
AF Speaker, Talia
   O'Donnell, Stephanie
   Wittemyer, George
   Bruyere, Brett
   Loucks, Colby
   Dancer, Anthony
   Carter, Marianne
   Fegraus, Eric
   Palmer, Jonathan
   Warren, Ellie
   Solomon, Jennifer
TI A global community-sourced assessment of the state of conservation
   technology
SO CONSERVATION BIOLOGY
AB Conservation technology holds the potential to vastly increase conservationists' ability to understand and address critical environmental challenges, but systemic constraints appear to hamper its development and adoption. Understanding of these constraints and opportunities for advancement remains limited. We conducted a global online survey of 248 conservation technology users and developers to identify perceptions of existing tools' current performance and potential impact, user and developer constraints, and key opportunities for growth. We also conducted focus groups with 45 leading experts to triangulate findings. The technologies with the highest perceived potential were machine learning and computer vision, eDNA and genomics, and networked sensors. A total of 95%, 94%, and 92% respondents, respectively, rated them as very helpful or game changers. The most pressing challenges affecting the field as a whole were competition for limited funding, duplication of efforts, and inadequate capacity building. A total of 76%, 67%, and 55% respondents, respectively, identified these as primary concerns. The key opportunities for growth identified in focus groups were increasing collaboration and information sharing, improving the interoperability of tools, and enhancing capacity for data analyses at scale. Some constraints appeared to disproportionately affect marginalized groups. Respondents in countries with developing economies were more likely to report being constrained by upfront costs, maintenance costs, and development funding (p = 0.048, odds ratio [OR] = 2.78; p = 0.005, OR = 4.23; p = 0.024, OR = 4.26), and female respondents were more likely to report being constrained by development funding and perceived technical skills (p = 0.027, OR = 3.98; p = 0.048, OR = 2.33). To our knowledge, this is the first attempt to formally capture the perspectives and needs of the global conservation technology community, providing foundational data that can serve as a benchmark to measure progress. We see tremendous potential for this community to further the vision they define, in which collaboration trumps competition; solutions are open, accessible, and interoperable; and user-friendly processing tools empower the rapid translation of data into conservation action. Article impact statement: Addressing financing, coordination, and capacity-building constraints is critical to the development and adoption of conservation technology.
RI Wittemyer, George/AFQ-1840-2022
OI Speaker, Talia/0000-0002-1274-2330; Dancer, Anthony/0000-0002-3322-2502
SN 0888-8892
EI 1523-1739
PD JUN
PY 2022
VL 36
IS 3
DI 10.1111/cobi.13871
EA FEB 2022
UT WOS:000750923700001
PM 34904294
ER

PT J
AU Van Wyk, K
   Marvel, JA
AF Van Wyk, Karl
   Marvel, Jeremy A.
TI Strategies for Improving and Evaluating Robot Registration Performance
SO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
AB The ability to calculate rigid-body transformations between arbitrary coordinate systems (i.e., registration) is an invaluable tool in robotics. This effort builds upon previous work by investigating strategies for improving the registration accuracy between a robotic arm and an extrinsic coordinate system with relatively inexpensive parts and minimal labor. The framework previously presented is expanded with a new test methodology to characterize the effects of strategies that improve registration performance. In addition, statistical analyses of physical trials reveal that leveraging more data and applying machine learning are two major components for significantly reducing registration error. One-shot peg-in-hole tests are conducted to show the application-level performance gains obtained by improving registration accuracy. Trends suggest that the maximum translation positioning error (postregistration) is a good, albeit not perfect, indicator for peg insertion performance.
   Note to Practitioners-In a dynamic robotic workcell environment where robots may be frequently relocated or may need to collaborate with other robots, it is simpler and more robust to program robots in an external or unifying reference frame. The process of robot registration involves finding the location of a robot with respect to another reference frame. For instance, if parts are in known locations on a table and a robot can locate itself with respect to the table (an external reference frame), then the robot will also know the location of the parts. Furthermore, if two or more robots can locate themselves with respect to the table, then each robot will not only know the location of the parts, but also the location of every other robot. This knowledge facilitates the coordination of robot motions and robot collaboration and eases the integration of additional robots into the workcell. Since robot registration is critically necessary and occurs frequently, its process needs to be inexpensive, fast, and accurate. This paper details the requirements for a relatively inexpensive and fast robot registration experience, along with detailing strategies that incur significant improvements to registered robot positioning accuracy with minimal overhead. A quantitative verification process is presented to evaluate the performance impacts of these strategies. Peg-in-hole experiments are conducted to validate the notion that more accurate robot registration translates to more reliable task-level performance.
SN 1545-5955
EI 1558-3783
PD JAN
PY 2018
VL 15
IS 1
BP 320
EP 328
DI 10.1109/TASE.2017.2720478
UT WOS:000419498100025
PM 31080373
ER

PT J
AU Demirci, MDS
   Adan, A
AF Demirci, Muserref Duygu Sacar
   Adan, Aysun
TI Computational analysis of microRNA-mediated interactions in SARS-CoV-2
   infection
SO PEERJ
AB MicroRNAs (miRNAs) are post-transcriptional regulators of gene expression found in more than 200 diverse organisms. Although it is still not fully established if RNA viruses could generate miRNAs, there are examples of miRNA like sequences from RNA viruses with regulatory functions. In the case of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there are several mechanisms that would make miRNAs impact the virus, like interfering with viral replication, translation and even modulating the host expression. In this study, we performed a machine learning based miRNA prediction analysis for the SARS-CoV-2 genome to identify miRNA-like hairpins and searched for potential miRNA-based interactions between the viral miRNAs and human genes and human miRNAs and viral genes. Overall, 950 hairpin structured sequences were extracted from the virus genome and based on the prediction results, 29 of them could be precursor miRNAs. Targeting analysis showed that 30 viral mature miRNA-like sequences could target 1,367 different human genes. PANTHER gene function analysis results indicated that viral derived miRNA candidates could target various human genes involved in crucial cellular processes including transcription, metabolism, defense system and several signaling pathways such as Wnt and EGFR signalings. Protein class-based grouping of targeted human genes showed that host transcription might be one of the main targets of the virus since 96 genes involved in transcriptional processes were potential targets of predicted viral miRNAs. For instance, basal transcription machinery elements including several components of human mediator complex (MED1, MED9, MED 12L, MED 19), basal transcription factors such as TAF4, TAF5, TAF7L and site-specific transcription factors such as STATI were found to be targeted. In addition, many known human miRNAs appeared to be able to target viral genes involved in viral life cycle such as S, M, N, E proteins and ORF lab, ORF3a, ORF8, ORF7a and ORF10. Considering the fact that miRNA-based therapies have been paid attention, based on the findings of this study, comprehending mode of actions of miRNAs and their possible roles during SARS-CoV-2 infections could create new opportunities for the development and improvement of new therapeutics.
RI demirci, müşerref duygu saçar/N-7458-2017
OI demirci, müşerref duygu saçar/0000-0003-2012-0598; adan,
   aysun/0000-0002-3747-8580
SN 2167-8359
PD JUN 5
PY 2020
VL 8
AR e9369
DI 10.7717/peerj.9369
UT WOS:000538335100008
PM 32547891
ER

PT J
AU Yu, XM
   Shen, YD
   Ni, Y
   Huang, XW
   Wang, XL
   Chen, QC
   Tang, BZ
AF Yu, Xiaoming
   Shen, Yedan
   Ni, Yuan
   Huang, Xiaowei
   Wang, Xiaolong
   Chen, Qingcai
   Tang, Buzhou
TI CapsTM: capsule network for Chinese medical text matching
SO BMC MEDICAL INFORMATICS AND DECISION MAKING
CT International Conference on Health Big Data and Artificial Intelligence
CY OCT 29-NOV 01, 2020
CL Guangzhou, PEOPLES R CHINA
AB Background Text Matching (TM) is a fundamental task of natural language processing widely used in many application systems such as information retrieval, automatic question answering, machine translation, dialogue system, reading comprehension, etc. In recent years, a large number of deep learning neural networks have been applied to TM, and have refreshed benchmarks of TM repeatedly. Among the deep learning neural networks, convolutional neural network (CNN) is one of the most popular networks, which suffers from difficulties in dealing with small samples and keeping relative structures of features. In this paper, we propose a novel deep learning architecture based on capsule network for TM, called CapsTM, where capsule network is a new type of neural network architecture proposed to address some of the short comings of CNN and shows great potential in many tasks. Methods CapsTM is a five-layer neural network, including an input layer, a representation layer, an aggregation layer, a capsule layer and a prediction layer. In CapsTM, two pieces of text are first individually converted into sequences of embeddings and are further transformed by a highway network in the input layer. Then, Bidirectional Long Short-Term Memory (BiLSTM) is used to represent each piece of text and attention-based interaction matrix is used to represent interactive information of the two pieces of text in the representation layer. Subsequently, the two kinds of representations are fused together by BiLSTM in the aggregation layer, and are further represented with capsules (vectors) in the capsule layer. Finally, the prediction layer is a connected network used for classification. CapsTM is an extension of ESIM by adding a capsule layer before the prediction layer. Results We construct a corpus of Chinese medical question matching, which contains 36,360 question pairs. This corpus is randomly split into three parts: a training set of 32,360 question pairs, a development set of 2000 question pairs and a test set of 2000 question pairs. On this corpus, we conduct a series of experiments to evaluate the proposed CapsTM and compare it with other state-of-the-art methods. CapsTM achieves the highest F-score of 0.8666. Conclusion The experimental results demonstrate that CapsTM is effective for Chinese medical question matching and outperforms other state-of-the-art methods for comparison.
OI Tang, Buzhou/0000-0003-0271-8246
EI 1472-6947
PD JUL 30
PY 2021
VL 21
IS SUPPL 2
SU 2
SI SI
AR 94
DI 10.1186/s12911-021-01442-9
UT WOS:000679718400018
PM 34330253
ER

PT J
AU Lemley, MA
   Casey, B
AF Lemley, Mark A.
   Casey, Bryan
TI Remedies for Robots
SO UNIVERSITY OF CHICAGO LAW REVIEW
AB What happens when artificially intelligent robots misbehave? The question is not just hypothetical. As robotics and artificial intelligence systems increasingly integrate into our society, they will do bad things. We seek to explore what remedies the law can and should provide once a robot has caused harm.
   Remedies are sometimes designed to make plaintiffs whole by restoring them to the condition they would have been in "but for" the wrong. But they can also contain elements of moral judgment, punishment, and deterrence. In other instances, the law may order defendants to do (or stop doing) something unlawful or harmful.
   Each of these goals of remedies law, however, runs into difficulties when the bad actor in question is neither a person nor a corporation but a robot. We might order a robot-or, more realistically, the designer or owner of the robot-to pay for the damages it causes. But it turns out to be much harder for a judge to "order" a robot, rather than a human, to engage in or refrain from certain conduct. Robots can't directly obey court orders not written in computer code. And bridging the translation gap between natural language and code is often harder than we might expect. This is particularly true of modern artificial intelligence techniques that empower machines to learn and modify their decision-making over time. If we don't know how the robot "thinks," we won't know how to tell it to behave in a way likely to cause it to do what we actually want it to do.
   Moreover, if the ultimate goal of a legal remedy is to encourage good behavior or discourage bad behavior, punishing owners or designers for the behavior of their robots may not always make sense-if only for the simple reason that their owners didn't act wrongfully in any meaningful way. The same problem affects injunctive relief. Courts are used to ordering people and companies to do (or stop doing) certain things, with a penalty of contempt of court for noncompliance. But ordering a robot to abstain from certain behavior won't be trivial in many cases. And ordering it to take affirmative acts may prove even more problematic.
   In this Article, we begin to think about how we might design a system of remedies for robots. Robots will require us to rethink many of our current doctrines. They also offer important insights into the law of remedies we already apply to people and corporations.
SN 0041-9494
PD SEP
PY 2019
VL 86
IS 5
BP 1311
EP 1396
UT WOS:000482252000002
ER

PT J
AU Smith, DR
   Chapman, MR
AF Smith, Daniel R.
   Chapman, Matthew R.
TI Economical Evolution: Microbes Reduce the Synthetic Cost of
   Extracellular Proteins
SO MBIO
AB Protein evolution is not simply a race toward improved function. Because organisms compete for limited resources, fitness is also affected by the relative economy of an organism's proteome. Indeed, many abundant proteins contain relatively high percentages of amino acids that are metabolically less taxing for the cell to make, thus reducing cellular cost. However, not all abundant proteins are economical, and many economical proteins are not particularly abundant. Here we examined protein composition and found that the relative synthetic cost of amino acids constrains the composition of microbial extracellular proteins. In Escherichia coli, extracellular proteins contain, on average, fewer energetically expensive amino acids independent of their abundance, length, function, or structure. Economic pressures have strategically shaped the amino acid composition of multicomponent surface appendages, such as flagella, curli, and type I pili, and extracellular enzymes, including type III effector proteins and secreted serine proteases. Furthermore, in silico analysis of Pseudomonas syringae, Mycobacterium tuberculosis, Saccharomyces cerevisiae, and over 25 other microbes spanning a wide range of GC content revealed a broad bias toward more economical amino acids in extracellular proteins. The synthesis of any protein, especially those rich in expensive aromatic amino acids, represents a significant investment. Because extracellular proteins are lost to the environment and not recycled like other cellular proteins, they present a greater burden on the cell, as their amino acids cannot be reutilized during translation. We hypothesize that evolution has optimized extracellular proteins to reduce their synthetic burden on the cell.
   IMPORTANCE Microbes secrete proteins to perform essential interactions with their environment, such as motility, pathogenesis, biofilm formation, and resource acquisition. However, because microbes generally lack protein import systems, secretion is often a one-way street. Consequently, secreted proteins are less likely to be recycled by the cell due to environmental loss. We demonstrate that evolution has in turn selected these extracellular proteins for increased economy at the level of their amino acid composition. Compared to their cellular counterparts, extracellular proteins have fewer synthetically expensive amino acids and more inexpensive amino acids. The resulting bias lessens the loss of cellular resources due to secretion. Furthermore, this economical bias was observed regardless of the abundance, length, structure, or function of extracellular proteins. Thus, it appears that economy may address the compositional bias seen in many extracellular proteins and deliver further insight into the forces driving their evolution.
OI Chapman, Matthew/0000-0002-2645-1294
SN 2150-7511
PD JUL-AUG
PY 2010
VL 1
IS 3
AR e00131-10
DI 10.1128/mBio.00131-10
UT WOS:000284717500013
PM 20824102
ER

PT J
AU LaPrade, MD
   Kallenbach, SL
   Aman, ZS
   Moatshe, G
   Storaci, HW
   Turnbull, TL
   Arendt, EA
   Chahla, J
   LaPrade, RF
AF LaPrade, Matthew D.
   Kallenbach, Samantha L.
   Aman, Zachary S.
   Moatshe, Gilbert
   Storaci, Hunter W.
   Turnbull, Travis Lee
   Arendt, Elizabeth A.
   Chahla, Jorge
   LaPrade, Robert F.
TI Biomechanical Evaluation of the Medial Stabilizers of the Patella
SO AMERICAN JOURNAL OF SPORTS MEDICINE
AB Background: Quantification of the biomechanical properties of each individual medial patellar ligament will facilitate an understanding of injury patterns and enhance anatomic reconstruction techniques by improving the selection of grafts possessing appropriate biomechanical properties for each ligament.
   Purpose: To determine the ultimate failure load, stiffness, and mechanism of failure of the medial patellofemoral ligament (MPFL), medial patellotibial ligament (MPTL), and medial patellomeniscal ligament (MPML) to assist with selection of graft tissue for anatomic reconstructions.
   Study Design: Descriptive laboratory study.
   Methods: Twenty-two nonpaired, fresh-frozen cadaveric knees were dissected free of all soft tissue structures except for the MPFL, MPTL, and MPML. Two specimens were ultimately excluded because their medial structure fibers were lacerated during dissection. The patella was obliquely cut to test the MPFL and the MPTL-MPML complex separately. To ensure that the common patellar insertion of the MPTL and MPML was not compromised during testing, only one each of the MPML and MPTL were tested per specimen (n = 10 each). Specimens were secured in a dynamic tensile testing machine, and the ultimate load, stiffness, and mechanism of failure of each ligament (MPFL = 20, MPML = 10, and MPTL = 10) were recorded.
   Results: The mean 6 SD ultimate load of the MPFL (178 6 46 N) was not significantly greater than that of the MPTL (147 6 80 N; P = .706) but was significantly greater than that of the MPML (105 6 62 N; P = .001). The mean ultimate load of the MPTL was not significantly different from that of the MPML (P = .210). Of the 20 MPFLs tested, 16 failed by midsubstance rupture and 4 by bony avulsion on the femur. Of the 10 MPTLs tested, 9 failed by midsubstance rupture and 1 by bony avulsion on the patella. Finally, of the 10 MPMLs tested, all 10 failed by midsubstance rupture. No significant difference was found in mean stiffness between the MPFL (23 6 6 N/mm(2)) and the MPTL (31 6 21 N/mm(2); P = .169), but a significant difference was found between the MPFL and the MPML (14 6 8 N/mm(2); P = .003) and between the MPTL and MPML (P = .028).
   Conclusion: The MPFL and MPTL had comparable ultimate loads and stiffness, while the MPML had lower failure loads and stiffness. Midsubstance failure was the most common type of failure; therefore, reconstruction grafts should meet or exceed the values reported herein.
RI LaPrade, Robert F./AFA-8673-2022; Aman, Zachary/ABA-9898-2021
OI LaPrade, Robert F./0000-0002-9823-2306; Aman,
   Zachary/0000-0001-7839-6576
SN 0363-5465
EI 1552-3365
PD JUN
PY 2018
VL 46
IS 7
BP 1575
EP 1582
DI 10.1177/0363546518758654
UT WOS:000433616400010
PM 29554436
ER

PT J
AU Mihcin, S
   Karakitsios, I
   Le, N
   Strehlow, J
   Demedts, D
   Schwenke, M
   Haase, S
   Preusser, T
   Melzer, A
AF Mihcin, Senay
   Karakitsios, Ioannis
   Nhan Le
   Strehlow, Jan
   Demedts, Daniel
   Schwenke, Michael
   Haase, Sabrina
   Preusser, Tobias
   Melzer, Andreas
TI Methodology on quantification of sonication duration for safe
   application of MR guided focused ultrasound for liver tumour ablation
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
AB Background and objective: Magnetic Resonance Guided Focused Ultrasound (MRgFUS) for liver tumour ablation is a challenging task due to motion caused by breathing and occlusion due the ribcage between the transducer and the tumour. To overcome these challenges, a novel system for liver tumour ablation during free breathing has been designed.
   Methods: The novel TRANS-FUSIMO Treatment System (TTS, EUFP7) interacts with a Magnetic Resonance (MR) scanner and a focused ultrasound transducer to sonicate to a moving target in liver. To meet the requirements of ISO 13485; a quality management system for medical device design, the system needs to be tested for certain process parameters. The duration of sonication and, the delay after the sonication button is activated, are among the parameters that need to be quantified for efficient and safe ablation of tumour tissue. A novel methodology is developed to quantify these process parameters. A computerised scope is programmed in LabVIEW to collect data via hydrophone; where the coordinates of fiber-optic sensor assembly was fed into the TRANS-FUSIMO treatment software via Magnetic Resonance Imaging (MRI) to sonicate to the tip of the sensor, which is synchronised with the clock of the scope, embedded in a degassed water tank via sensor assembly holder. The sonications were executed for 50 W, 100 W, 150 W for 10 s to quantify the actual sonication duration and the delay after the emergency stop by two independent operators for thirty times. The deviation of the system from the predefined specs was calculated. Student's-T test was used to investigate the user dependency.
   Results: The duration of sonication and the delay after the sonication were quantified successfully with the developed method. TTS can sonicate with a maximum deviation of 0.16 s (Std 0.32) from the planned duration and with a delay of 14 ms (Std 0.14) for the emergency stop. Student's T tests indicate that the results do not depend on operators (p > .05).
   Conclusion: The evidence obtained via this protocol is crucial for translation-of-research into the clinics for safe application of MRgFUS. The developed protocol could be used for system maintenance in compliance with quality systems in clinics for daily quality assurance routines. (C) 2017 Elsevier B.V. All rights reserved.
RI Mihçin, Şenay/AAC-7506-2019; Le, Nhan/HPG-8755-2023
OI Schwenke, Michael/0000-0003-0662-3949
SN 0169-2607
EI 1872-7565
PD DEC
PY 2017
VL 152
BP 125
EP 130
DI 10.1016/j.cmpb.2017.09.006
UT WOS:000413258300013
PM 29054252
ER

PT J
AU Aboian, M
   Bousabarah, K
   Kazarian, E
   Zeevi, T
   Holler, W
   Merkaj, S
   Petersen, GC
   Bahar, R
   Subramanian, H
   Sunku, P
   Schrickel, E
   Bhawnani, J
   Zawalich, M
   Mahajan, A
   Malhotra, A
   Payabvash, S
   Tocino, I
   Lin, MD
   Westerhoff, M
AF Aboian, Mariam
   Bousabarah, Khaled
   Kazarian, Eve
   Zeevi, Tal
   Holler, Wolfgang
   Merkaj, Sara
   Cassinelli Petersen, Gabriel
   Bahar, Ryan
   Subramanian, Harry
   Sunku, Pranay
   Schrickel, Elizabeth
   Bhawnani, Jitendra
   Zawalich, Mathew
   Mahajan, Amit
   Malhotra, Ajay
   Payabvash, Sam
   Tocino, Irena
   Lin, MingDe
   Westerhoff, Malte
TI Clinical implementation of artificial intelligence in neuroradiology
   with development of a novel workflow-efficient picture archiving and
   communication system-based automated brain tumor segmentation and
   radiomic feature extraction
SO FRONTIERS IN NEUROSCIENCE
AB PurposePersonalized interpretation of medical images is critical for optimum patient care, but current tools available to physicians to perform quantitative analysis of patient's medical images in real time are significantly limited. In this work, we describe a novel platform within PACS for volumetric analysis of images and thus development of large expert annotated datasets in parallel with radiologist performing the reading that are critically needed for development of clinically meaningful AI algorithms. Specifically, we implemented a deep learning-based algorithm for automated brain tumor segmentation and radiomics extraction, and embedded it into PACS to accelerate a supervised, end-to- end workflow for image annotation and radiomic feature extraction. Materials and methodsAn algorithm was trained to segment whole primary brain tumors on FLAIR images from multi-institutional glioma BraTS 2021 dataset. Algorithm was validated using internal dataset from Yale New Haven Health (YHHH) and compared (by Dice similarity coefficient [DSC]) to radiologist manual segmentation. A UNETR deep-learning was embedded into Visage 7 (Visage Imaging, Inc., San Diego, CA, United States) diagnostic workstation. The automatically segmented brain tumor was pliable for manual modification. PyRadiomics (Harvard Medical School, Boston, MA) was natively embedded into Visage 7 for feature extraction from the brain tumor segmentations. ResultsUNETR brain tumor segmentation took on average 4 s and the median DSC was 86%, which is similar to published literature but lower than the RSNA ASNR MICCAI BRATS challenge 2021. Finally, extraction of 106 radiomic features within PACS took on average 5.8 +/- 0.01 s. The extracted radiomic features did not vary over time of extraction or whether they were extracted within PACS or outside of PACS. The ability to perform segmentation and feature extraction before radiologist opens the study was made available in the workflow. Opening the study in PACS, allows the radiologists to verify the segmentation and thus annotate the study. ConclusionIntegration of image processing algorithms for tumor auto-segmentation and feature extraction into PACS allows curation of large datasets of annotated medical images and can accelerate translation of research into development of personalized medicine applications in the clinic. The ability to use familiar clinical tools to revise the AI segmentations and natively embedding the segmentation and radiomic feature extraction tools on the diagnostic workstation accelerates the process to generate ground-truth data.
EI 1662-453X
PD OCT 13
PY 2022
VL 16
AR 860208
DI 10.3389/fnins.2022.860208
UT WOS:000876380200001
PM 36312024
ER

PT J
AU Bilchenko, YV
AF Bilchenko, Yevgenia, V
TI Philosophy as a Model of Dialogue: The Concept "Death of Father" in the
   Connotations of the Contemporary Humanities
SO TOMSK STATE UNIVERSITY JOURNAL
AB The author of the article solves the problem of substantiating the repressive character of the neoliberal sociocultural state of the subject in global multicultural capitalism and forming a dialogical model of philosophy as a positive alternative to alienation and identity crisis. The state of the crisis is designated in the framework of the postmodern discourse as "emptiness", "gaping", "rhizome". The main important methodological guidelines for the study were: Alain Badiou's school of psychic and political economy, the Ljubljana school of post-Lacan analysis of the relationship between the real and the symbolic (Slavoj ZiZek, Mladen Dolar), Jean Baudrillard's and Benno Htibner's social phenomenology and aesthetics. The author analyzes the contemporary economic, psychoanalytic and aesthetic materials employing the methods of structural psychoanalysis, semiotic analysis, hermeneutic interpretation and philosophical comparative studies. She structures the symbolic order of the hegemony of globalism as a complex formation of three elements: economic (universal market), mental ("Death of Father", or loss of a vertical figure of control and signification), and sensual-aesthetic (transgression of desire, translation of desire into the automatic regime of a "machine", forced aesthetics, pleasure from the Other, etc.). "Death of Father" is defined as a neoliberal crisis of tradition, leading to a new totalitarianism within the framework of globalism due to voluntary self-censorship and digital control. In the course of the analysis of the relationship between the "Father" (tradition) and the "Son" (subject) as figures to which the concepts of "donor" and "recipient" correspond in semiotics, the author comes to the conclusion that it is necessary to return to the "Father" at a new dialectical turn based on the synthesis of traditionalism (diversity) and universalism (unity). The interaction between the subject and tradition is seen as a process of deliberate fusion of lacks in the philosophy of dialogue. Dialogue appears as the identification of a subject with a symptom (community with the mortality of the Other) outside the chain of signifiers. The tradition presented in the concept "Return of Father" is seen as a lack, symptom, solidarity ("Son") experienced individually as a poetic myth and critically rethought collectively as a principle of social community. Philosophy as a dialogue is a worldview context within which ethical and civilizational principles are synthesized on the basis of two procedures of truth: the phenomenological shift of attention from an object to consciousness and the subjective articulation of values in philosophical hermeneutics.
SN 1561-7793
EI 1561-803X
PD OCT
PY 2020
IS 459
BP 68
EP 79
DI 10.17223/15617793/459/8
UT WOS:000601335200008
ER

PT J
AU Theobald, SJ
   Kreer, C
   Khailaie, S
   Bonifacius, A
   Eiz-Vesper, B
   Figueiredo, C
   Mach, M
   Backovic, M
   Ballmaier, M
   Koenig, J
   Olbrich, H
   Schneider, A
   Volk, V
   Danisch, S
   Gieselmann, L
   Ercanoglu, MS
   Messerle, M
   von Kaisenberg, C
   Witte, T
   Klawonn, F
   Meyer-Hermann, M
   Klein, F
   Stripecke, R
AF Theobald, Sebastian J.
   Kreer, Christoph
   Khailaie, Sahamoddin
   Bonifacius, Agnes
   Eiz-Vesper, Britta
   Figueiredo, Constanca
   Mach, Michael
   Backovic, Marija
   Ballmaier, Matthias
   Koenig, Johannes
   Olbrich, Henning
   Schneider, Andreas
   Volk, Valery
   Danisch, Simon
   Gieselmann, Lutz
   Ercanoglu, Meryem Seda
   Messerle, Martin
   von Kaisenberg, Constantin
   Witte, Torsten
   Klawonn, Frank
   Meyer-Hermann, Michael
   Klein, Florian
   Stripecke, Renata
TI Repertoire characterization and validation of gB-specific human IgGs
   directly cloned from humanized mice vaccinated with dendritic cells and
   protected against HCMV
SO PLOS PATHOGENS
AB Human cytomegalovirus (HCMV) causes serious complications to immune compromised hosts. Dendritic cells (iDCgB) expressing granulocyte-macrophage colony-stimulating factor, interferon-alpha and HCMV-gB were developed to promotede novoantiviral adaptive responses. Mice reconstituted with a human immune system (HIS) were immunized with iDCgB and challenged with HCMV, resulting into 93% protection. Immunization stimulated the expansion of functional effector memory CD8(+)and CD4(+)T cells recognizing gB. Machine learning analyses confirmed bone marrow T/CD4(+), liver B/IgA(+)and spleen B/IgG(+)cells as predictive biomarkers of immunization (approximate to 87% accuracy). CD8(+)and CD4(+)T cell responses against gB were validated. Splenic gB-binding IgM(-)/IgG(+)B cells were sorted and analyzed at a single cell level. iDCgB immunizations elicited human-like IgG responses with a broad usage of various IgG heavy chain V gene segments harboring variable levels of somatic hypermutation. From this search, two gB-binding human monoclonal IgGs were generated that neutralized HCMV infectionin vitro. Passive immunization with these antibodies provided proof-of-concept evidence of protection against HCMV infection. This HIS/HCMVin vivomodel system supported the validation of novel active and passive immune therapies for future clinical translation.
   Author summary Human cytomegalovirus (HCMV) is a ubiquitous pathogen. As long as the immune system is functional, T and B cells can control HCMV. Yet, for patients who have debilitated immune functions, HCMV infections and reactivations cause major complications. Vaccines or antibodies to prevent or treat HCMV are not yet approved. Novel animal models for testing new immunization approaches are emerging and are important tools to identify biomedical products with a reasonable chance to work in patients. Here, we used a model based on mice transplanted with human immune cells and infected with a traceable HCMV. We tested a cell vaccine (iDCgB) carrying gB, a potent HCMV antigen. The model showed that iDCgB halted the HCMV infection in more than 90% of the mice. We found that antibodies were key players mediating protection. Using state-of-the-art methods, we were able to use the sequences of the human antibodies generated in the mice to construct and produce monoclonal antibodies in the laboratory. Proof-of-concept experiments indicated that administration of these monoclonal antibodies into mice protected them against HCMV infection. In summary, this humanized mouse model was useful to test a vaccine and to generate and test novel antibodies that can be further developed for human use.
RI Witte, Torsten/B-5783-2016; Backovic, Marija/AAX-8863-2021; Klein,
   Florian/HKF-6067-2023; Klawonn, Frank/HFH-5887-2022; von Kaisenberg,
   Constantin/AAR-2756-2021; Kreer, Christoph/ABG-7275-2021; Ballmaier,
   Matthias/J-4175-2014
OI Backovic, Marija/0000-0001-8814-4428; Klein,
   Florian/0000-0003-1376-1792; Klawonn, Frank/0000-0001-9613-182X; Kreer,
   Christoph/0000-0002-9140-9850; Ballmaier, Matthias/0000-0002-1352-5995;
   Meyer-Hermann, Michael/0000-0002-4300-2474; Bonifacius,
   Agnes/0000-0002-0112-967X
SN 1553-7366
EI 1553-7374
PD JUL
PY 2020
VL 16
IS 7
AR e1008560
DI 10.1371/journal.ppat.1008560
UT WOS:000553111000003
PM 32667948
ER

PT J
AU Grimm, F
   Edl, F
   Kerscher, SR
   Nieselt, K
   Gugel, I
   Schuhmann, MU
AF Grimm, Florian
   Edl, Florian
   Kerscher, Susanne R.
   Nieselt, Kay
   Gugel, Isabel
   Schuhmann, Martin U.
TI Semantic segmentation of cerebrospinal fluid and brain volume with a
   convolutional neural network in pediatric hydrocephalus-transfer
   learning from existing algorithms
SO ACTA NEUROCHIRURGICA
AB Background For the segmentation of medical imaging data, a multitude of precise but very specific algorithms exist. In previous studies, we investigated the possibility of segmenting MRI data to determine cerebrospinal fluid and brain volume using a classical machine learning algorithm. It demonstrated good clinical usability and a very accurate correlation of the volumes to the single area determination in a reproducible axial layer. This study aims to investigate whether these established segmentation algorithms can be transferred to new, more generalizable deep learning algorithms employing an extended transfer learning procedure and whether medically meaningful segmentation is possible. Methods Ninety-five routinely performed true FISP MRI sequences were retrospectively analyzed in 43 patients with pediatric hydrocephalus. Using a freely available and clinically established segmentation algorithm based on a hidden Markov random field model, four classes of segmentation (brain, cerebrospinal fluid (CSF), background, and tissue) were generated. Fifty-nine randomly selected data sets (10,432 slices) were used as a training data set. Images were augmented for contrast, brightness, and random left/right and X/Y translation. A convolutional neural network (CNN) for semantic image segmentation composed of an encoder and corresponding decoder subnetwork was set up. The network was pre-initialized with layers and weights from a pre-trained VGG 16 model. Following the network was trained with the labeled image data set. A validation data set of 18 scans (3289 slices) was used to monitor the performance as the deep CNN trained. The classification results were tested on 18 randomly allocated labeled data sets (3319 slices) and on a T2-weighted BrainWeb data set with known ground truth. Results The segmentation of clinical test data provided reliable results (global accuracy 0.90, Dice coefficient 0.86), while the CNN segmentation of data from the BrainWeb data set showed comparable results (global accuracy 0.89, Dice coefficient 0.84). The segmentation of the BrainWeb data set with the classical FAST algorithm produced consistent findings (global accuracy 0.90, Dice coefficient 0.87). Likewise, the area development of brain and CSF in the long-term clinical course of three patients was presented. Conclusion Using the presented methods, we showed that conventional segmentation algorithms can be transferred to new advances in deep learning with comparable accuracy, generating a large number of training data sets with relatively little effort. A clinically meaningful segmentation possibility was demonstrated.
RI Kerscher, Susanne R./AAL-7818-2021
OI Kerscher, Susanne R./0000-0001-7470-6717
SN 0001-6268
EI 0942-0940
PD OCT
PY 2020
VL 162
IS 10
BP 2463
EP 2474
DI 10.1007/s00701-020-04447-x
EA JUN 2020
UT WOS:000543080000002
PM 32583085
ER

PT J
AU Biondo, F
   Jewell, A
   Pritchard, M
   Aarsland, D
   Steves, CJ
   Mueller, C
   Cole, JH
AF Biondo, Francesca
   Jewell, Amelia
   Pritchard, Megan
   Aarsland, Dag
   Steves, Claire J.
   Mueller, Christoph
   Cole, James H.
TI Brain-age is associated with progression to dementia in memory clinic
   patients
SO NEUROIMAGE-CLINICAL
AB Background: Biomarkers for the early detection of dementia risk hold promise for better disease monitoring and targeted interventions. However, most biomarker studies, particularly in neuroimaging, have analysed artificially "clean' research groups, free from comorbidities, erroneous referrals, contraindications and from a narrow sociodemographic pool. Such biases mean that neuroimaging samples are often unrepresentative of the target population for dementia risk (e.g., people referred to a memory clinic), limiting the generalisation of these studies to real-world clinical settings. To facilitate better translation from research to the clinic, datasets that are more representative of dementia patient groups are warranted. Methods: We analysed T1-weighted MRI scans from a real-world setting of patients referred to UK memory clinic services (n = 1140; 60.2 % female and mean [SD] age of 70.0[10.8] years) to derive "brain-age'. Brain-age is an index of age-related brain health based on quantitative analysis of structural neuroimaging, largely reflecting brain atrophy. Brain-predicted age difference (brain-PAD) was calculated as brain-age minus chronological age. We determined which patients went on to develop dementia between three months and 7.8 years after neuro-imaging assessment (n = 476) using linkage to electronic health records. Results: Survival analysis, using Cox regression, indicated a 3 % increased risk of dementia per brain-PAD year (hazard ratio [95 % CI] = 1.03 [1.02,1.04], p < 0.0001), adjusted for baseline age, age2, sex, Mini Mental State Examination (MMSE) score and normalised brain volume. In sensitivity analyses, brain-PAD remained significant when time-to-dementia was at least 3 years (hazard ratio [95 % CI] = 1.06 [1.02, 1.09], p = 0.0006), or when baseline MMSE score >= 27 (hazard ratio [95 % CI] = 1.03 [1.01, 1.05], p = 0.0006). Conclusions: Memory clinic patients with older-appearing brains are more likely to receive a subsequent dementia diagnosis. Potentially, brain-age could aid decision-making during initial memory clinic assessment to improve early detection of dementia. Even when neuroimaging assessment was more than 3 years prior to diagnosis and when cognitive functioning was not clearly impaired, brain-age still proved informative. These real-world results support the use of quantitative neuroimaging biomarkers like brain-age in memory clinics.
OI Biondo, Francesca/0000-0001-9952-0249
SN 2213-1582
PY 2022
VL 36
AR 103175
DI 10.1016/j.nicl.2022.103175
EA SEP 2022
UT WOS:000863120700004
PM 36087560
ER

PT J
AU Chen, B
   Su, LH
   Zhang, ZY
   Liu, XZ
   Dai, TG
   Song, MP
   Yu, H
   Wang, YH
   Yang, JY
AF Chen, Bei
   Su, Lianghao
   Zhang, Zhaoyang
   Liu, Xiaozhi
   Dai, Tingge
   Song, Muping
   Yu, Hui
   Wang, Yuehai
   Yang, Jianyi
TI Wavelet convolutional neural network for robust and fast temperature
   measurements in Brillouin optical time domain reflectometry
SO OPTICS EXPRESS
AB In this paper, a wavelet convolutional neural network (WNN) consisting of a onedimensional (1D) convolutional neural network and a self-adaptive wavelet neural network has been proposed and demonstrated experimentally for temperature measurement in a Brillouin optical time domain reflectometry (BOTDR) system. Based on the analysis of the system noise, it follows the Gaussian white noise distribution along the time-related sensing distance. The impact of the noise in time-domain on the measured Brillouin gain spectra (BGSs) could be neglected, so that the BGSs in the fiber can be regarded as a series of 1D input data of the proposed WNN. Different self-adaptive wavelet activation functions connected to each output of the full-connection network are adopted to realize the multi-scaled analysis and the scale translation, which can obtain more local characteristics in frequency-domain. The output extracted by the WNN is Brillouin frequency shift (BFS), which presents linearity correlation to the actual temperature. Considering the multi-parameters including different frequency ranges, signal-to-noise-ratios (SNRs), BFSs and spectral widths (SWs), a general model of the proposed WNN is trained to handle more extreme cases, in which it doesn't require retraining for different single-mode (SM) optical fibers in BOTDR sensing system. The performances of the WNN are compared with other two techniques, the Lorentzian curve fitting based on Levenberg-Marquardt (LM) algorithm and the basic neural network (NN) containing input and output layers together with two hidden layers. Both the simulated and measured results show that the WNN has better robustness and flexibility than the LM and the NN. Besides, the computational accuracy of the WNN is improved and the fluctuation of that is slighter, especially when the SNR is less than 11 dB. Moreover, the WNN takes approximately 0.54 s to measure the temperature from the 18,000 collected BGSs transmitted through the 18 km SM optical fiber. The calculating time of the WNN is greatly reduced by three orders of magnitude in comparison with that of the LM, and is comparable to that of the NN. It proves that the proposed WNN may provide a feasible or even better scheme tbr the robust and fast temperature measurement in BOTDR system. (C) 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement
SN 1094-4087
PD APR 25
PY 2022
VL 30
IS 9
BP 13942
EP 13958
DI 10.1364/OE.451877
UT WOS:000793726300003
PM 35473148
ER

PT J
AU Schneckenreither, G
   Tschandl, P
   Rippinger, C
   Sinz, C
   Brunmeir, D
   Popper, N
   Kittler, H
AF Schneckenreither, Gunter
   Tschandl, Philipp
   Rippinger, Claire
   Sinz, Christoph
   Brunmeir, Dominik
   Popper, Nikolas
   Kittler, Harald
TI Reproduction of patterns in melanocytic proliferations by agent-based
   simulation and geometric modeling
SO PLOS COMPUTATIONAL BIOLOGY
AB Spatio-temporal patterns of melanocytic proliferations observed in vivo are important for diagnosis but the mechanisms that produce them are poorly understood. Here we present an agent-based model for simulating the emergence of the main biologic patterns found in melanocytic proliferations. Our model portrays the extracellular matrix of the dermo-epidermal junction as a two-dimensional manifold and we simulate cellular migration in terms of geometric translations driven by adhesive, repulsive and random forces. Abstracted cellular functions and melanocyte-matrix interactions are modeled as stochastic events. For identification and validation we use visual renderings of simulated cell populations in a horizontal perspective that reproduce growth patterns observed in vivo by sequential dermatoscopy and corresponding vertical views that reproduce the arrangement of melanocytes observed in histopathologic sections. Our results show that a balanced interplay of proliferation and migration produces the typical reticular pattern of nevi, whereas the globular pattern involves additional cellular mechanisms. We further demonstrate that slight variations in the three basic cellular properties proliferation, migration, and adhesion are sufficient to produce a large variety of morphological appearances of nevi. We anticipate our model to be a starting point for the reproduction of more complex scenarios that will help to establish functional connections between abstracted microscopic behavior and macroscopic patterns in all types of melanocytic proliferations including melanoma.
   Author summary
   Clinicians and pathologists use pigmentation patterns to classify melanocytic nevi into subgroups and to differentiate nevi from melanoma. The mechanisms that produce these patterns are poorly understood. Here, we reproduce the main patterns of nevi in computer simulations using an abstracted model of individual cell behavior. We developed a novel geometric approach for reconstructing the microanatomy of the epidermis in silico and to model the interaction of cell-agents with the physiological environment. We generated visual representations of simulated populations of melanocytes and demonstrate that the interplay of the collective behavior of individual cells and the physiologic microenvironment is responsible for the typical patterns of melanocytic lesions. Further, we show that the variations of nevi observed in vivo can be reproduced by simple agent-based models that depend on a few key parameters that reflect basic cellular functions such as proliferation, migration, and adhesion. Our simulations confirm existing assumptions about nest formation in melanocytic lesions and offer new insights and testable hypotheses for important biologic phenomena such as senescence and tumor progression. Furthermore, synthetic nevi created by our model may serve as training cases for machine learning algorithms.
RI Sinz, Christoph/AAW-2085-2020; Popper, Niki/AAL-6464-2021
OI Sinz, Christoph/0000-0001-5589-1865; Popper, Niki/0000-0003-4615-2774;
   KITTLER, HARALD/0000-0002-0051-8016; Schneckenreither,
   Gunter/0000-0002-9217-9399; Tschandl, Philipp/0000-0003-0391-7810;
   Brunmeir, Dominik/0000-0002-9005-4066
SN 1553-734X
EI 1553-7358
PD FEB
PY 2021
VL 17
IS 2
AR e1008660
DI 10.1371/journal.pcbi.1008660
UT WOS:000616720700001
PM 33539342
ER

PT J
AU de Avila, BEF
   Gao, WW
   Karshalev, E
   Zhang, LF
   Wang, J
AF de Avila, Berta Esteban-Fernandez
   Gao, Weiwei
   Karshalev, Emil
   Zhang, Liangfang
   Wang, Joseph
TI Cell-Like Micromotors
SO ACCOUNTS OF CHEMICAL RESEARCH
AB CONSPECTUS:In the past decade, versatile micro- and nanosized machines have emerged as active agents for large-scale detoxification, sensing, microfabrication, and many other promising applications. Micromachines have also been envisioned as the next advancement in dynamic therapy with numerous proof-of-concept studies in drug delivery, microsurgery, and detoxification. However, the practical use of synthetic micromotors in the body requires the development of fully biocompatible designs facilitating micromotor movement in biological fluids of diverse composition and displaying desired functions in specific locations. The combination of the efficient movement of synthetic micromotors with the biological functions of natural cells has resulted in cell-like micromotors with expanded therapeutic and toxin-removing capabilities toward different biological applications. Thus, these biocompatible and biomimetic cell-like micromotors can provide efficient movement in complex biofluids and mimic the functionalities of natural cells. This Account highlights a variety of recent proof-of-concept examples of cell-like micromotors, based on different designs and actuation mechanisms, which perform diverse in vivo tasks. The cell-like micromotors are divided into two groups: (i) cell membrane-coated micromotors, which use natural cell membranes derived from red blood cells, platelets, or a combination of different cells to cloak and functionalize synthetic motors, and (ii) cell-based micromotors, which directly use entire cells such as blood cells, spermatozoa, and bacteria as the micromotor engine. Cell-like micromotors, composed of different cellular components and actuated by different mechanisms, have shown unique advantages for operation in complex biofluids such as blood. Due to the inherent biocompatibility of cell-derived materials, these cell-like micromotors do not provoke an immune response while utilizing useful secondary functions of the blood cells such as strong ability to soak up foreign agents or bind toxins. Additionally, the utilization of autonomously motile cells (e.g., bacteria) allows for built-in chemotactic motion, which eliminates the need for harmful fuels or complex actuation equipment. Furthermore, a broad range of cells, both passive and motile, can be incorporated into micromachine designs constituting a large library of functional components depending on the limits of the desired application. The coupling of cellular and artificial components has led to active biohybrid swimming microsystems with greatly enhanced capabilities and functionalities compared to the individual biological or synthetic components. These characteristics have positioned these cell-like micromotors as promising biomimetic dynamic tools for potential actuation in vivo. Finally, the key challenges and limitations of cell-like micromotors are discussed in the context of expanded future clinical uses and translation to human trials.
RI Wang, Joseph/C-6175-2011; Karshalev, Emil/U-5531-2019
OI Karshalev, Emil/0000-0001-7802-6153; Wang, Joseph/0000-0002-4921-9674;
   Zhang, Liangfang/0000-0003-0637-0654
SN 0001-4842
EI 1520-4898
PD SEP
PY 2018
VL 51
IS 9
BP 1901
EP 1910
DI 10.1021/acs.accounts.8b00202
UT WOS:000445441200002
PM 30074758
ER

PT J
AU Radhakrishnan, R
   Vijaykrishnan, N
   John, LK
   Sivasubramaniam, A
   Rubio, J
   Sabarinathan, J
AF Radhakrishnan, R
   Vijaykrishnan, N
   John, LK
   Sivasubramaniam, A
   Rubio, J
   Sabarinathan, J
TI Java runtime systems: Characterization and architectural implications
SO IEEE TRANSACTIONS ON COMPUTERS
AB The Java Virtual Machine (JVM) is the cornerstone of Java technology and its efficiency in executing the portable Java bytecodes is crucial for the success of this technology. Interpretation, Just-in-Time (JIT) compilation, and hardware realization are well-known solutions for a JVM and previous research has proposed optimizations for each of these techniques. However, each technique has its pros and cons and may not be uniformly attractive for all hardware platforms, Instead, an understanding of the architectural implications of JVM implementations with real applications can be crucial to the development of enabling technologies for efficient Java runtime system development on a wide range of platforms. Toward this goal, this paper examines architectural issues from both the hardware and JVM implementation perspectives. The paper starts by identifying the important execution characteristics of Java applications from a bytecode perspective. It then explores the potential of a smart JIT compiler strategy that can dynamically interpret or compile based on associated costs and investigates the CPU and cache architectural support that would benefit JVM implementations. We also study the available parallelism during the different execution modes using applications from the SPECjvm98 benchmarks. At the bytecode level. it is observed that less than 45 out of the 256 bytecodes constitute 90 percent of the dynamic bytecode stream. Method sizes fall into a trinodal distribution with peaks of 1, 9, and 26 bytecodes across all benchmarks. The architectural issues explored in this study show that. when Java applications are executed with a JIT compiler, selective translation using good heuristics can improve performance, but the saving is only 10-15 percent at best. The instruction and data cache performance of Java applications are seen to be better than that of C/C++ applications except in the case of data cache performance in the JIT mode. Write misses resulting from installation of JIT compiler output dominate the misses and deteriorate the data cache performance in JIT mode. A study on the available parallelism shows that Java programs executed using JIT compilers have parallelism comparable to C/C++ programs for small window sizes, but falls behind when the window size is increased. Java programs executed using the interpreter have very little parallelism due to the stack nature of the JVM instruction set, which is dominant in the interpreted execution mode. In addition, this work gives revealing insights and architectural proposals for designing an efficient Java runtime system.
SN 0018-9340
EI 1557-9956
PD FEB
PY 2001
VL 50
IS 2
BP 131
EP 146
DI 10.1109/12.908989
UT WOS:000167090400003
ER

PT J
AU Aherwar, A
   Singh, A
   Patnaik, A
AF Aherwar, A.
   Singh, A.
   Patnaik, A.
TI Mechanical and wear performance evaluation of tungsten added
   Co-30Cr-4Mo-1Ni biomedical alloy with Taguchi optimization
SO MATERIALWISSENSCHAFT UND WERKSTOFFTECHNIK
AB Development of biomaterial with diverse properties (physical, mechanical, wear, and many others) for hip femoral head is one of the most exigent tasks. Improper material often causes component failure during functioning. Therefore, in this study, a series of implant materials containing tungsten of different weight percentages were fabricated by high temperature vacuum casting induction furnace and the physical, mechanical, and wear properties were examined. The proportions were varied from 0 wt. % - 4wt.% in a cobalt-chromium alloy (Co-30Cr-4Mo-1Ni). The mechanical properties were tested by the micro-hardness tester and the compression testing machine, while the wear performance was analyzed through a pin-on-disc tribometer under different operating conditions at room temperature. Initially in this study, steady state experimental analysis was performed to obtain the volumetric wear loss and coefficient of friction by varying sliding velocity and normal load respectively. Afterwards, the Taguchi experimental design has been conducted to obtain the optimum wear response. Lastly, scanning electron microscopy and atomic force microscopy were utilized to analyze the contour of wear mechanism and 3D surface topography. From the results obtained, it was found that Co-30Cr-4Mo-1Ni-2W implant material provides the best combination of the properties for a given application.
   Translation abstract Die Entwicklung von Biomaterial mit diversen Eigenschaften (physikalisch, mechanisch, tribologisch und viele andere) fur den Huftfemurkopf ist eine der schwierigsten Aufgaben. Unsachgema ss er Werkstoff verursacht oft Funktionsstorungen in der Nutzungsphase. Daher wurde in dieser Studie eine Reihe von Implantatwerkstoffen, die Wolfram mit unterschiedlichen Gewichtsprozenten enthielten, im Hochtemperatur-Vakuumguss-Induktionsofen hergestellt und die physikalischen, mechanischen und tribologischen Eigenschaften untersucht. Die Anteile wurden von 0-4 Gew.-% in einer Kobalt-Chrom-Legierung (Co-30Cr-4Mo-1Ni) variiert. Die mechanischen Eigenschaften wurden durch ein Mikroharteprufgerat und eine Kompressionsprufmaschine untersucht, wahrend der Verschlei ss widerstand durch ein Pin-on-Disc-Tribometer unter verschiedenen Betriebsbedingungen bei Raumtemperatur analysiert wurde. Zunachst wurde in dieser Studie eine stationare experimentelle Analyse durchgefuhrt, um den volumetrischen Verschlei ss verlust und den Reibungskoeffizienten zu bestimmen, indem die Gleitgeschwindigkeit und die normale Belastung variiert wurden. Danach wurde das Experimentierdesign von Taguchi durchgefuhrt, um den optimalen Verschlei ss widerstand zu ermitteln. Schlie ss lich wurden Rasterelektronenmikroskopie (SEM) und Atomkraftmikroskopie (AFM) verwendet, um die Kontur des Verschlei ss mechanismus und der 3D-Oberflachentopographie zu analysieren. Aus den erzielten Ergebnissen wurde festgestellt, dass Co-30Cr-4Mo-1Ni-2W Implantatwerkstoff die beste Kombination der Eigenschaften fur eine gegebene Anwendung liefert.
RI Patnaik, Amar/GZM-0065-2022; Aherwar, Amit/HNQ-9693-2023; Aherwar,
   Amit/AAG-3186-2019; Kumar Singh, Amit/HCH-1935-2022
OI Patnaik, Amar/0000-0001-9506-782X; Aherwar, Amit/0000-0002-7245-2925;
   Aherwar, Amit/0000-0002-7245-2925; Singh, Amit/0000-0002-1536-9594
SN 0933-5137
EI 1521-4052
PD JUL
PY 2018
VL 49
IS 7
BP 912
EP 927
DI 10.1002/mawe.201700063
UT WOS:000439817700008
ER

PT J
AU Wang, MY
   Shi, SH
   Shi, T
   Fu, GY
   Pang, YF
   Yu, SQ
   Gong, YQ
AF Wang Mingyu
   Shi Shihong
   Shi Tuo
   Fu Geyan
   Pang Yifan
   Yu Siqi
   Gong Yanqi
TI Conformal Discrete Layering of Multivariant Twisted Structure Based on
   Inside-Laser Powder Feeding
SO CHINESE JOURNAL OF LASERS-ZHONGGUO JIGUANG
AB Objective In the fields of aerospace, machinery, ships, etc., there are many multivariant twisted structures, such as fan blades in turbofan engine intakes, ternary blades in centrifugal compressors, and ship propellers. These parts have common structural features including large inclination and twisting, which cause great difficulties in processing. Currently, multivariant twisted structural parts are mainly based on computer numerical control milling, casting, electrochemical machining, etc. However, these machining methods have their own problems, such as low material utilization and long production cycles, and in some cases, it is difficult to meet performance requirements. Laser-cladding forming technology is a new type of rapid prototyping technology for metal parts that was proposed in the 1990s. It can be used for rapid and mold-free manufacturing of high-performance and complex parts. Therefore, research on laser-cladding forming of multivariant twisted structures has broad applications. At present, the laser-cladding forming of multivariant twisted structures at home and abroad is mostly based on uniform cross-section and single-direction twisting, while there are few reports on the structural parts that are twisted in multiple directions in space, especially the formation of the gradual cross-section in this type of structural parts. Based on the self- developed optical internal powder feeding technology, this paper adopts the method of conformal discrete layering to obtain the movement trajectory of the laser-cladding nozzle, and realizes the accumulation and forming of the multivariant twisted structure.
   Methods The multivariant twisted structure described in this paper presents a three-dimensional twisted shape in space, with complex twisting and tilting characteristics. To realize the laser-cladding forming of the twisted structural part, based on the principle of normal delamination, this paper proposes a method of discrete layering following the shape to layer the multivariant twisted structural part. First, the structure is divided according to the shape characteristics of the structural part. The structure are divided into different parts for layering and the center line of each part is extracted; then, normal slices are made along the center line of each part. Finally, according to the characteristics of each slice layer, the slices are discretized twice to produce a different discrete cladding unit with geometric characteristics. The movement trajectory of the light spot is determined by the position and direction information of each part of the discrete unit. The cladding nozzle moves according to the position and direction information in the discrete unit to accumulate and form the multivariant twisted structure.
   Results and Discussions The conformal discrete layering method is proposed to layer the multivariant twisted structure to obtain discrete cladding units with different geometric characteristics [Fig. 3(d)]. The trajectory of the light spot is determined by the position and direction information of each part of the discrete unit. The formation of the cladding layer can be regarded as the translation and rotation of the tool coordinate system, where the light spot is located relative to the base coordinate system in which the substrate is located [Fig. 4(a)]. Through translation and rotation operations, the homogeneous transformation matrix of each discrete unit relative to the base coordinate system is obtained, and the changes in position and direction change of each discrete unit relative to the base coordinate system are obtained, thereby yielding the laser-cladding nozzle' s movement track. This study uses the method of robot trajectory approximation, where, through the control of program commands, the robot does not stop at the dislocation position, and realizes the gradual cladding formation of the multivariant twisted structure [Fig. 4(b)]. The experiment uses a self-developed inside-laser powder-feed nozzle, which has good powder-beam bundling, and realizes the laser-cladding forming of multivariant twisted structural parts (Fig. 7).
   Conclusions To obtain the forming trajectory of multivariant twisted structural parts, a method of discrete layering according to shape is proposed: a normal discrete slice of the entire structural part is made, and then each layer of the slices is discretized twice to obtain a discrete cladding unit with different geometric characteristics. The conformal discrete layering method is used to solve the layering problem of the gradual structure of the cross section in the multivariant twisted structure, obtain the spatial movement track information of the laser-cladding nozzle, and complete the accumulation of the multivariant twisted structure. The inspection results of the formed parts are as follows: the surface of the formed parts is smooth with a surface roughness value within 5.579 ttm; the average thickness of the formed parts is 6.03 mm, and the thickness of each part is slightly increased; the forming accuracy of the formed parts is higher, with a shape and size error from - 3.45-3.09%; the hardness of the different formed parts differed slightly but were basically stable at 271.6-284. 5 HV; there is no obvious difference in the overall structure of the formed part; and the structure of each part is dense and uniform, without obvious pores or cracks.
SN 0258-7025
PD MAY
PY 2021
VL 48
IS 10
AR 1002114
DI 10.3788/CJL202148.1002114
UT WOS:000686546400014
ER

PT J
AU Nguyen, HS
   Luu, TP
AF Nguyen, Hoai Son
   Luu, Trieu Phat
TI Tremor-Suppression Orthoses for the Upper Limb: Current Developments and
   Future Challenges
SO FRONTIERS IN HUMAN NEUROSCIENCE
AB Introduction: Pathological tremor is the most common motor disorder in adults and characterized by involuntary, rhythmic muscular contraction leading to shaking movements in one or more parts of the body. Functional Electrical Stimulation (FES) and biomechanical loading using wearable orthoses have emerged as effective and non-invasive methods for tremor suppression. A variety of upper-limb orthoses for tremor suppression have been introduced; however, a systematic review of the mechanical design, algorithms for tremor extraction, and the experimental design is still missing.
   Methods: To address this gap, we applied a standard systematic review methodology to conduct a literature search in the PubMed and PMC databases. Inclusion criteria and full-text access eligibility were used to filter the studies from the search results. Subsequently, we extracted relevant information, such as suppression mechanism, system weights, degrees of freedom (DOF), algorithms for tremor estimation, experimental settings, and the efficacy.
   Results: The results show that the majority of tremor-suppression orthoses are active with 47% prevalence. Active orthoses are also the heaviest with an average weight of 561 +/- 467 g, followed by semi-active 486 +/- 395 g, and passive orthoses 191 +/- 137 g. Most of the orthoses only support one DOF (54.5%). Two-DOF and three-DOF orthoses account for 33 and 18%, respectively. The average efficacy of tremor suppression using wearable orthoses is 83 +/- 13%. Active orthoses are the most efficient with an average efficacy of 83 +/- 8%, following by the semi-active 77 +/- 19%, and passive orthoses 75 +/- 12%. Among different experimental setups, bench testing shows the highest efficacy at 95 +/- 5%, this value dropped to 86 +/- 8% when evaluating with tremor-affected subjects. The majority of the orthoses (92%) measured voluntary and/or tremorous motions using biomechanical sensors (e.g., IMU, force sensor). Only one system was found to utilize EMG for tremor extraction.
   Conclusions: Our review showed an improvement in efficacy of using robotic orthoses in tremor suppression. However, significant challenges for the translations of these systems into clinical or home use remain unsolved. Future challenges include improving the wearability of the orthoses (e.g., lightweight, aesthetic, and soft structure), and user control interfaces (i.e., neural machine interface). We also suggest addressing non-technical challenges (e.g., regulatory compliance, insurance reimbursement) to make the technology more accessible.
RI SON, NGUYEN HOAI/AAJ-1817-2020
OI SON, NGUYEN HOAI/0000-0002-9343-9164
SN 1662-5161
PD APR 30
PY 2021
VL 15
AR 622535
DI 10.3389/fnhum.2021.622535
UT WOS:000649944200001
PM 33994975
ER

PT J
AU Singh, A
   Singh, S
   Mani, KR
   Das, KJM
   Kumar, S
AF Singh, Archana
   Singh, Shalini
   Mani, Karthick Raj
   Das, K. J. Maria
   Kumar, Shaleen
TI Comparing Setup Errors Using Portal Imaging in Patients With Gynecologic
   Cancers by Two Methods of Alignment
SO JOURNAL OF MEDICAL IMAGING AND RADIATION SCIENCES
AB Aims: Alignment tattoos on a lax abdomen contribute to misalignment of patients undergoing abdomino-pelvic radiotherapy (RT). The present study was undertaken to assess setup reproducibility in gynecologic cancer patients positioned identically but aligned for treatment to machine isocenter by rwo different ways.
   Materials and methods: A prospective study in 35 women treated with radical RT for gynecologic malignancy was undertaken. A RT planning contrast-enhanced computed tomography scan in the supine position using an foot and ankle positioning device was done, and three reference points tattooed on the reference plane, anteriorly at the mons pubis and one on each side laterally at a fixed table top-to-vertical height of 10 cm, whereas a fourth point was tattooed at the xiphoid in the anterior midline. Patients were aligned using either a field center, that is, conventional method (Arm I, n = 18) or by a new setup isocenter (Arm II, n = 17) defined by a cranial offset of 4 cm to the reference plane for daily treatment. Anterior and right lateral digitally reconstructed radiograph setup fields were created at the treatment isocenters and compared with orthogonal megavoltage portal images (PI) taken during initial 3 days of RT and subsequently twice weekly. Setup deviations-rotations and translations were analysed in mediolateral (ML), craniocaudal, and anteroposterior direction. No online and offline corrections were performed. Population systematic error and random error were calculated and planning target volume margins required were estimated using van Herk's formula.
   Results: Arm I had 209 PI while Arm II had 188 PI. Patients in arm II had a lesser systematic error in the ML direction. Patients with large pelvic girth (>95 cm) were susceptible for greater movements during treatment, more so in Arm I, major shifts (>5 mm) with respect to Arm II in the ML direction (37% vs. 22%, P = .001). A larger planning target volume expansion was required in Arm I (1.6 cm) compared with Arm II (0.9 cm). The margin expansion required from clinical target volume in anteroposterior direction was about 0.6 cm and about a cm in the craniocaudal direction in both the arm.
   Conclusions: Alignment of patient with anterior tattoo at the relatively immobile portion of lower abdomen (mons pubis) Arm II (setup) is superior to a more cranial location over the flabby abdomen during radiation treatment.
RI Mani, Karthick Raj/ABI-3759-2020
SN 1939-8654
PD SEP
PY 2020
VL 51
IS 3
BP 394
EP 403
DI 10.1016/j.jmir.2020.04.001
UT WOS:000576693800009
PM 32444331
ER

PT J
AU Khademi, A
   Reiche, B
   DiGregorio, J
   Arezza, G
   Moody, AR
AF Khademi, April
   Reiche, Brittany
   DiGregorio, Justin
   Arezza, Giordano
   Moody, Alan R.
TI Whole volume brain extraction for multi-centre, multi-disease FLAIR MRI
   datasets
SO MAGNETIC RESONANCE IMAGING
AB Automatic segmentation of the brain from magnetic resonance images (MRI) is a fundamental step in many neuroimaging processing frameworks. There are mature technologies for this task for T1- and T2-weighted MRI; however, a widely-accepted brain extraction method for Fluid-Attenuated Inversion Recovery (FLAIR) MRI has yet to be established. FLAIR MRI are becoming increasingly important for the analysis of neurodegenerative diseases and tools developed for this sequence would have clinical value. To maximize translation opportunities and for large scale research studies, algorithms for brain extraction in FLAIR MRI should generalize to multi-centre (MC) data. To this end, this work proposes a fully automated, whole volume brain extraction methodology for MC FLAIR MRI datasets. The framework is built using a novel standardization framework which reduces acquisition artifacts, standardizes the intensities of tissues and normalizes the spatial coordinates of brain tissue across MC datasets. Using the standardized datasets, an intuitive set of features based on intensity, spatial location and gradients are extracted and classified using a random forest (RF) classifier to segment the brain tissue class. A series of experiments were conducted to optimize classifier parameters, and to determine segmentation accuracy for standardized and unstandardized (original) data, as a function of scanner vendor, feature type and disease type. The models are trained, tested and validated on 156 image volumes (similar to 8000 image slices) from two multi-centre, multi-disease datasets, acquired with varying imaging parameters from 30 centres and three scanner vendors. The image datasets, denoted as CAIN and ADNI for vascular and dementia disease, respectively, represent a diverse collection of MC data to test the generalization capabilities of the proposed design. Results demonstrate the importance of standardization for segmentation of MC data, as models trained on standardized data yielded a drastic improvement in brain extraction accuracy compared to the original, unstandardized data (CAIN: DSC = 91% and ADNI: DSC = 86% vs. CAIN: 78% and ADNI: 65%). It was also found that models created from one scanner vendor based on unstandardized data yielded poor segmentation results in data acquired from other scanner vendors, which was improved through standardization. These results demonstrate that to create consistency in segmentations from multi-institutional datasets it is paramount that MC variability be mitigated to improve stability and to ensure generalization of machine learning algorithms for MRI.
OI Khademi, April/0000-0002-4932-0385
SN 0730-725X
EI 1873-5894
PD FEB
PY 2020
VL 66
BP 116
EP 130
DI 10.1016/j.mri.2019.08.022
UT WOS:000517669600013
PM 31472262
ER

PT J
AU Liu, Y
   Ma, GY
   Liu, JC
   Zheng, HT
   Huang, GM
   Song, QT
   Pang, ZF
   Du, JJ
AF Liu, Yong
   Ma, Guoyuan
   Liu, Jichang
   Zheng, Haotian
   Huang, Gemu
   Song, Qingtao
   Pang, Zhaofei
   Du, Jiajun
TI SLC7A5 is a lung adenocarcinoma-specific prognostic biomarker and
   participates in forming immunosuppressive tumor microenvironment
SO HELIYON
AB Background: Amino acid metabolism participates in forming immunosuppressive tumor microenvironment. Amino acid transporters (AATs), as a gate for admission, remains to be studied.Materials and methods: We identified LUAD-specific prognostic AATs, SLC7A5 by differential expression analysis, logistic regression, machine learning, Kaplan-Meier analysis, AUC value filtrating and Cox regression. Then dif-ferential expression and distribution of SLC7A5 were depicted. Copy number variation, DNA methylation, tran-scriptional factors and ceRNA network were investigated to explore potential mechanism causing differential expression. The prognostic and clinical relation were evaluated by Kaplan-Meier analysis, Cox regression analysis. GSEA and GSVA were used to analyze altered pathways between SLC7A5 high-and low-groups. The expression of HLA-related genes and immune checkpoint genes, and immune cells infiltration were detected. SLC7A5 expres-sion in immune cells was evaluated by single-cell sequencing data. IPS and an independent immunotherapy cohort assessed response rates of patients with distinct SLC7A5 expression. Proliferation assay and wound healing assay validated the effects of SLC7A5 on proliferation and migration of LUAD cells. Western blotting and cell viability assays were performed to detect mTORC1 pathway activity and sensitivity to rapamycin.Results: SLC7A5 was a LUAD-specific prognostic AAT and had significant differential expression in transcription and translation level. Methylation levels of cg00728300, cg00858400, cg12408911, cg08710629 were negative correlation with SLC7A5 expression. FOXP3 and TFAP2A were possible transcription factors and miR-30a-5p, miR-184, miR-195-5p may target SLC7A5 mRNA. SLC7A5 high-expression indicated poor prognosis and was an independent prognostic factor. mTORC1, cell cycle, DNA damage repair, response to reactive oxygen, angiogenesis, epithelial-mesenchymal transition (EMT) and various growth factors signaling pathways were activated in SLC7A5 high-expression group. Interestingly, SLC7A5 high-expression group had less immune-related genes expression and immune cells infiltration. Single-cell sequencing data also suggested SLC7A5 was down -regulated in various T cells, especially effector T cells. Moreover, high SLC7A5 expression indicated poor immunotherapy efficacy and higher sensitivity to inhibitors of mTORC1 pathway, cell cycle and angiogenesis. SLC7A5 deficiency abrogated proliferation, migration and mTORC1 pathway activity.Conclusions: In summary, as a LUAD-specific prognostic AAT, SLC7A5 is involved in activation of multiple oncogenic pathways and indicates poor prognosis. Moreover, SLC7A5 may participate in forming immunosup-pressive TME and is associated with low response of immunotherapy. SLC7A5 is promising to be a new diagnostic and prognostic biomarker and therapeutic target in LUAD.
OI , Jiajun/0000-0001-7174-243X
EI 2405-8440
PD OCT
PY 2022
VL 8
IS 10
AR e10866
DI 10.1016/j.heliyon.2022.e10866
EA OCT 2022
UT WOS:000869716100010
PM 36217463
ER

PT J
AU Kulikova, EJ
   Penskaya, EN
AF Kulikova, E. Ju.
   Penskaya, E. N.
TI Literary and aesthetic paradoxes of Viktor Burenin
SO SIBIRSKII FILOLOGICHESKII ZHURNAL
AB Burenin's journalistic and literary experience includes more than half a century. During this period, a paradoxical portrait of a critic prevailed in the minds of several generations. On the one hand, Burenin is the embodiment of cynicism, anger and nihilism in relation to the literary workshop. Only some people in the Russian journal-literary environment of the late 19th early 20th centuries caused such a strong rejection and disgust of contemporaries. On the other hand, Viktor Burenin was one of the most prominent figures in Russian journalism and literature of 1870-1910s. It is impossible to imagine the landscape of Russian culture of the last third of the 19th century without Burenin's invasion of journalism into literary and theatre spheres. Burenin established the rules of conduct, set the tone, and defined the style of mental life for many years ahead. He was popular, and that was close to true fame. Having created his style in journalism, he became recognizable in translations, in numerous plays and parodies intended for the stage. Burenin can be considered as some kind of a "factory" for the production of works of different kind, volume, genre. It is difficult to count everything that was created by Burenin during half a century: a few thousand texts scattered and lost in periodicals in the last third of 19th - the first decades of the 20th century. The texts were signed not only with his name but also were hiding behind masks-aliases, that became the "calling card" of Burenin: count Alexis Asminov, Vladimir Monumentov, Vyborgskii Poustynnik. All these roles created and maintained a unique status of journalism as a theatre platform for actors who share their experience with readers and writers. Burenin translated and wrote "his" plays and, in addition, acted as a literary critic and columnist. He worked out a list of his "victims"-those writers and artists whom he doggedly pursued. Exposing parodies of their works, often cruel and unjust, Burenin nevertheless contributed to the purification of art from the routine and die, freeing the way for a new aesthetic system. His great working ability provided the fame and created a smooth rhythm of the journalistic machine. Moscovite, also the inhabitant of Petersburg and later Leningrad, Viktor Burenin (1841-1926) saw a lot in his lifetime, through several eras. He was a witness and contemporary of the reforms from the Crimean disaster to the First World War and Revolution.
RI Kulikova, Elena/K-6809-2017
OI Kulikova, Elena/0000-0003-0695-7447
SN 1813-7083
PD MAR
PY 2018
IS 1
BP 152
EP 167
DI 10.17223/18137083/62/11
UT WOS:000432688900011
ER

PT J
AU Zhang, R
   Liu, JL
   Huang, Y
   Wang, MY
   Shi, QK
   Chen, J
   Zeng, Z
AF Zhang, Rui
   Liu, Jialin
   Huang, Yong
   Wang, Miye
   Shi, Qingke
   Chen, Jun
   Zeng, Zhi
TI Enriching the international clinical nomenclature with Chinese daily
   used synonyms and concept recognition in physician notes
SO BMC MEDICAL INFORMATICS AND DECISION MAKING
AB Background: It has been shown that the entities in everyday clinical text are often expressed in a way that varies from how they are expressed in the nomenclature. Owing to lots of synonyms, abbreviations, medical jargons or even misspellings in the daily used physician notes in clinical information system (CIS), the terminology without enough synonyms may not be adequately suitable for the task of Chinese clinical term recognition.
   Methods: This paper demonstrates a validated system to retrieve the Chinese term of clinical finding (CTCF) from CIS and map them to the corresponding concepts of international clinical nomenclature, such as SNOMED CT. The system focuses on the SNOMED CT with Chinese synonyms enrichment (SCCSE). The literal similarity and the diagnosis-related similarity metrics were used for concept mapping. Two CTCF recognition methods, the rule-and terminology-based approach (RTBA) and the conditional random field machine learner (CRF), were adopted to identify the concepts in physician notes. The system was validated against the history of present illness annotated by clinical experts. The RTBA and CRF could be combined to predict new CTCFs besides SCCSE persistently.
   Results: Around 59,000 CTCF candidates were accepted as valid and 39,000 of them occurred at least once in the history of present illness. 3,729 of them were accordant with the description in referenced Chinese clinical nomenclature, which could cross map to other international nomenclature such as SNOMED CT. With the hybrid similarity metrics, another 7,454 valid CTCFs (synonyms) were succeeded in concept mapping. For CTCF recognition in physician notes, a series of experiments were performed to find out the best CRF feature set, which gained an F-score of 0.887. The RTBA achieved a better F-score of 0.919 by the CTCF dictionary created in this research.
   Conclusions: This research demonstrated that it is feasible to help the SNOMED CT with Chinese synonyms enrichment based on physician notes in CIS. With continuous maintenance of SCCSE, the CTCFs could be precisely retrieved from free text, and the CTCFs arranged in semantic hierarchy of SNOMED CT could greatly improve the meaningful use of electronic health record in China. The methodology is also useful for clinical synonyms enrichment in other languages.
OI Wang, Miye/0000-0002-4137-0310
SN 1472-6947
PD MAY 2
PY 2017
VL 17
AR 54
DI 10.1186/s12911-017-0455-z
UT WOS:000401289900001
PM 28464923
ER

PT C
AU Kniaz, VV
AF Kniaz, Vladimir V.
BE Bruzzone, L
   Bovolo, F
   Benediktsson, JA
TI Deep Learning for Dense Labeling of Hydrographic Regions in Very High
   Resolution Imagery
SO IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXV
SE Proceedings of SPIE
CT Conference on Image and Signal Processing for Remote Sensing XXV
CY SEP 09-11, 2019
CL Strasbourg, FRANCE
SP SPIE
AB Automatic dense labeling of multispectral satellite images facilitates faster map update process. Water objects are essential elements of a geographic map. While modern dense labeling methods perform robust segmentation of such objects like roads, buildings, and vegetation, dense labeling of hydrographic regions remains a challenging problem. Water objects change their surface albedo, color, and reflection in different weather and different seasons. Moreover, rivers and lakes can change their boundaries after floods or d roughts. Robust documentation of such seasonal changes is an essential task in the field of analysis of satellite imagery. Due to the high variance in water object appearance, their segmentation is usually performed manually by a human operator. Recent advances in machine learning have made possible robust segmentation of static objects such as buildings and roads. To the best of our knowledge, there is little research in the modern literature regarding dense labeling of water regions. This paper is focused on the development of a deep-learning-based method for dense labeling of hydrographic in aerial and satellite imagery.
   We use the GeoGAN framework(1) and MobileNetV2(2) as the starting point for our research. The GeoGAN framework uses an aerial image as an input to generate pixel-level annotations of five object c lasses: building, low vegetation, high vegetation, road, and car. The GeoGAN framework leverages two deep learning approaches to ensure robust labeling: a generator with skip connections(3) and Generative Adversarial Networks.(4) A generator with skip connections performs image -> label translation using feed-forward connections between convolutional and deconvolutional layers of the same depth. A GAN framework consists of two competing networks: a generator and a discriminator. The adversarial loss improves the quality of the resulting dense labeling. We made the following contributions to the GeoGAN framework: (1) new MobileNetV2-based generator, (2) adversarial loss function. We term the resulting framework as HydroGAN.
   We evaluate our HydroGAN model using a new HydroViews dataset focused on dense labeling of areas that are subject to severe flooding during the spring season. The evaluation results are encouraging and demonstrate that our HydroGAN model competes with the state-of-the-art models for dense labeling of aerial and satellite imagery.
   The evaluation demonstrates that our model can generalize from the training data to previously unseen samples. The developed HydroGAN model is capable of performing dense labeling of water objects in different seasons. We made our model publicly available*.
RI Kniaz, Vladimir V./P-4300-2014
OI Kniaz, Vladimir V./0000-0003-2912-9986
SN 0277-786X
EI 1996-756X
BN 978-1-5106-3014-7
PY 2019
VL 11155
AR UNSP 111550W
DI 10.1117/12.2533161
UT WOS:000526177000028
ER

PT J
AU Theakstone, AG
   Brennan, PM
   Jenkinson, MD
   Mills, SJ
   Syed, K
   Rinaldi, C
   Xu, Y
   Goodacre, R
   Butler, HJ
   Palmer, DS
   Smith, BR
   Baker, MJ
AF Theakstone, Ashton G.
   Brennan, Paul M.
   Jenkinson, Michael D.
   Mills, Samantha J.
   Syed, Khaja
   Rinaldi, Christopher
   Xu, Yun
   Goodacre, Royston
   Butler, Holly J.
   Palmer, David S.
   Smith, Benjamin R.
   Baker, Matthew J.
TI Rapid Spectroscopic Liquid Biopsy for the Universal Detection of Brain
   Tumours
SO CANCERS
AB Simple Summary Due to the non-specific symptoms of brain cancer (e.g., headaches or memory changes), gliomas will often remain undetected until they are larger or at a higher grade, reducing the patient's likelihood of a good clinical outcome. Earlier detection and diagnosis of brain tumours is vital to improve patient outcomes, leading to safer surgeries and earlier treatments. A liquid biopsy for brain tumour would prove revolutionary however in order to detect disease earlier the liquid biopsy needs to be able to detect smaller tumours; and current liquid biopsies perform worse when detecting smaller or earlier stage tumours. Here, for the first time, we confirm the applicability of a validated spectroscopic liquid biopsy approach to detect both small and low-grade gliomas proving that the spectroscopic liquid biopsy approach is insensitive to tumour volume unlike other liquid biopsies. Background: To support the early detection and diagnosis of brain tumours we have developed a rapid, cost-effective and easy to use spectroscopic liquid biopsy based on the absorbance of infrared radiation. We have previously reported highly sensitive results of our approach which can discriminate patients with a recent brain tumour diagnosis and asymptomatic controls. Other liquid biopsy approaches (e.g., based on tumour genetic material) report a lower classification accuracy for early-stage tumours. In this manuscript we present an investigation into the link between brain tumour volume and liquid biopsy test performance. Methods: In a cohort of 177 patients (90 patients with high-grade glioma (glioblastoma (GBM) or anaplastic astrocytoma), or low-grade glioma (astrocytoma, oligoastrocytoma and oligodendroglioma)) tumour volumes were calculated from magnetic resonance imaging (MRI) investigations and patients were split into two groups depending on MRI parameters (T1 with contrast enhancement or T2/FLAIR (fluid-attenuated inversion recovery)). Using attenuated total reflection (ATR)-Fourier transform infrared (FTIR) spectroscopy coupled with supervised learning methods and machine learning algorithms, 90 tumour patients were stratified against 87 control patients who displayed no symptomatic indications of cancer, and were classified as either glioma or non-glioma. Results: Sensitivities, specificities and balanced accuracies were all greater than 88%, the area under the curve (AUC) was 0.98, and cancer patients with tumour volumes as small as 0.2 cm(3) were correctly identified. Conclusions: Our spectroscopic liquid biopsy approach can identify gliomas that are both small and low-grade showing great promise for deployment of this technique for early detection and diagnosis.
RI Theakstone, Ashton/ABF-5735-2021; Xu, Yun/ABG-2114-2021
OI Theakstone, Ashton/0000-0001-8754-5094; Xu, Yun/0000-0003-3228-5111;
   Brennan, Paul/0000-0002-7347-830X
EI 2072-6694
PD AUG
PY 2021
VL 13
IS 15
AR 3851
DI 10.3390/cancers13153851
UT WOS:000681935900001
PM 34359751
ER

PT C
AU Vasyl, L
   Victoria, V
   Dmytro, D
   Roman, H
   Zoriana, R
AF Vasyl, Lytvyn
   Victoria, Vysotska
   Dmytro, Dosyn
   Roman, Holoschuk
   Zoriana, Rybchak
GP IEEE
TI Application Of Sentence Parsing For Determining Keywords In Ukrainian
   Texts
SO PROCEEDINGS OF THE 2017 12TH INTERNATIONAL SCIENTIFIC AND TECHNICAL
   CONFERENCE ON COMPUTER SCIENCES AND INFORMATION TECHNOLOGIES (CSIT
   2017), VOL. 1
CT 12th International Scientific and Technical Conference on Computer
   Sciences and Information Technologies (CSIT)
CY SEP 05-08, 2017
CL Lviv, UKRAINE
SP IEEE, Natl Univ, Lvic Polytechn, IEEE Ukraine Sect
AB The article presents the use of generative grammars in linguistic modeling. A description of sentence syntax modeling is used to automate the process of analysis and synthesis of natural language texts. The article reveals the features of synthesizing sentences of different languages with the use of generative grammars. The article examines influence of norms and rules of a language on the process of constructing grammars. The use of generative grammars has great potential in the development and creation of automated systems for text content processing, linguistic support for linguistic computer systems etc. In natural languages there are situations where notions, which are dependent on the context, are described as independent of context, i. e. in terms of context-free grammars. This description is complicated by the formation of new categories and rules. The article features the process of introducing new restrictions on these grammar classes through the introduction of new rules. Uncut grammars were received if the number of characters in the right part of the rules were not less than the number of characters in the left one. Then by replacing the only character a context-sensitive grammar was received. A grammar with only one character in the left part of the rule is called a context-free grammar. No further natural restrictions may be applied to the left part of a rule. Based on the importance of automatic processing of text content in modern information media ( e. g., information retrieval systems, machine translation, semantic, statistical, optical and acoustic analysis and speech synthesis, automated editing, extracting knowledge from text content, abstracting and annotating text content, indexing text content, teaching and didactic, management of linguistic corpora, various tools for lexicography, etc.), specialists are actively looking for new models, ways of their description and methods of automatic processing of text content. One of such methods lies in developing general principles of syntactic lexicographical systems formation and developing mentioned systems of processing text content for specific languages based in these principles. Any parsing tools consist of two parts: a knowledge base of concrete natural language and parsing algorithm, i. e. a set of standard operators of text content processing based on this knowledge. The source of grammatical knowledge is data of morphological analysis and various tables filled with concepts and linguistic units. They are the result of an empirical study of the text content in natural language by experts aiming at highlighting the basic laws for parsing.
RI Vysotska, Victoria/P-7714-2016; Dosyn, Dmytro/P-8184-2016; Lytvyn,
   Vasyl/AAY-9371-2020; Holoshchuk, Roman/Q-8779-2017
OI Vysotska, Victoria/0000-0001-6417-3689; Dosyn,
   Dmytro/0000-0003-4040-4467; Lytvyn, Vasyl/0000-0002-9676-0180;
   Holoshchuk, Roman/0000-0002-1811-3025
BN 978-1-5386-1639-0
PY 2017
BP 326
EP 331
UT WOS:000425922100075
ER

PT J
AU Alhamada, H
   Simon, S
   Philippson, C
   Vandekerkhove, C
   Jourani, Y
   Pauly, N
   Dubus, A
   Reynaert, N
AF Alhamada, Husein
   Simon, Stephane
   Philippson, Catherine
   Vandekerkhove, Christophe
   Jourani, Younes
   Pauly, Nicolas
   Dubus, Alain
   Reynaert, Nick
TI Shielding disk position in intra-operative electron radiotherapy
   (IOERT): A Monte Carlo study
SO PHYSICA MEDICA-EUROPEAN JOURNAL OF MEDICAL PHYSICS
AB Purpose: In IOERT breast treatments, a shielding disk is frequently used to protect the underlying healthy structures. The disk is usually composed of two materials, a low-Z material intended to be oriented towards the beam and a high-Z material. As tissues are repositioned around the shield before treatment, the disk is no longer visible and its correct alignment with respect to the beam is guaranteed. This paper studies the dosimetric characteristics of four possible clinical positioning scenarios of the shielding disk. A new alignment method for the shielding disk in the beam is introduced. Finally, it suggests a new design for the shielding disk.
   Methods: As the first step, the IOERT machine "Mobetron 1000" was modeled by using Monte Carlo simulation, tuning the MC model until an excellent match with the measured PDDs and profiles was achieved. Four possible shielding disk positioning scenarios were considered, determining the dosimetric impact. Furthermore, in our center, to prevent beam misalignment, we have developed a shielding disk equipped with guiding rods. Having ascertained a correct alignment between the disk and the beam, we can propose a new internal design of the shielding disk that can improve the dose distribution with a better coverage of the treated area.
   Results: All MC simulations were performed with a 12 MeV beam, the maximum energy of Mobetron 1000 and a 5.5 cm diameter flat tip applicator, this applicator being the most clinically used. The simulations were compared with measurements performed in a water phantom and showed good results within 2.2% of root mean square difference (RMSD). The misplacement positions of the shielding disk have dosimetric impacts in the treatment volume and a small translation could have a significant influence on healthy tissues. The D-scenario is the worst which could happens when the shielding disk is flipped upside down, giving up to 144% dose instead of 90% at the surface of the Pb/Al shielding disk. A new shielding design used, together with our alignment tool, is able to give a more homogeneous dose in the target area.
   Conclusions: The accuracy of shielding disk position can still be problematic in IOERT dosimetry. Any method that can ascertain the good alignment between the shielding disk and the beam is beneficial for the dose distribution and is a prerequisite for an optimized shield internal design that could improve the coverage of the treated area and the protection of healthy tissues.
OI Reynaert, Nick/0000-0001-9221-9103
SN 1120-1797
EI 1724-191X
PD JUL
PY 2018
VL 51
BP 1
EP 6
DI 10.1016/j.ejmp.2018.05.023
UT WOS:000437650300001
PM 30278980
ER

PT J
AU Frimpong, S
   Thiruvengadam, M
AF Frimpong, Samuel
   Thiruvengadam, Magesh
TI Rigid multi-body kinematics of shovel crawler-formation interactions
SO INTERNATIONAL JOURNAL OF MINING RECLAMATION AND ENVIRONMENT
AB Large capacity shovels are deployed in surface mining operations for achieving economic bulk production targets. These shovels use crawler tracks for effective terrain engagement in these environments. Shovel reliability, maintainability, availability and efficiency depend on the service life of the crawler tracks. In rugged and challenging terrains, crawler wear, tear, cracks and failure are extensive resulting in prolonged downtimes with severe economic implications. In particular, crawler shoe wear, tear, cracks and fatigue failures can be expensive in terms of maintenance costs and production losses. No fundamental research has been undertaken to understand the crawler-formation interactions in challenging and rugged terrains in surface mining operations. This study forms the foundations for providing long-term solutions to crawler failure problems. The kinematic equations governing the crawler-formation interactions have been formulated to characterise the crawler motions during shovel production. These equations capture the motions governing the link pin joint, oil sand terrain joint and driving constraints based on the multi-body rigid theory. Crawler propel is achieved by using prescribed velocities along a translational degree of freedom (DOF) and a translational and rotational DOF. The crawler kinematic solutions show that the 3-D crawler-terrain model results in 132 DOFs and requires dynamic modelling to obtain the unknown degrees of freedom. A 3-D virtual prototype model is built to capture the crawler-formation interaction in MSC ADAMS based on the rigid body crawler kinematics. The virtual prototype simulator is supplied with mass properties of crawler shoe, mass, stiffness and damping characteristics of oil sand and external loads due to machine weight and contact forces to obtain the time variation of position, velocity and acceleration for the crawler-terrain engagement for given driving constraints. The results from the driving constraints yield a non-linear longitudinal motion of the crawler track assembly. The crawler track lateral and vertical displacements during translation-only motion fluctuates with maximum magnitudes of 0.7 and 3.6cm. Similarly the fluctuating longitudinal, lateral and vertical velocities and accelerations have maximum magnitudes of 0.22, 0.046 and 0.56m/s and 7.41, 1.73, and 34.9m/s(2), respectively. This research provides a strong foundation for further study on developing flexible crawler track model for predicting crawler shoes dynamic stress distributions, cracks development and propagation and fatigue analysis during shovel operations.
OI Thiruvengadam, Magesh/0009-0009-6077-0510
SN 1748-0930
EI 1748-0949
PD JUL 3
PY 2016
VL 30
IS 4
BP 347
EP 369
DI 10.1080/17480930.2015.1093761
UT WOS:000371923200006
ER

PT J
AU Keim-Malpass, J
   Clark, MT
   Lake, DE
   Moorman, JR
AF Keim-Malpass, Jessica
   Clark, Matthew T.
   Lake, Douglas E.
   Moorman, J. Randall
TI Towards development of alert thresholds for clinical deterioration using
   continuous predictive analytics monitoring
SO JOURNAL OF CLINICAL MONITORING AND COMPUTING
AB Patients who deteriorate while on the acute care ward and are emergently transferred to the Intensive Care Unit (ICU) experience high rates of mortality. To date, risk scores for clinical deterioration applied to the acute care wards rely on static or intermittent inputs of vital sign and assessment parameters. We propose the use of continuous predictive analytics monitoring, or data that relies on real-time physiologic monitoring data captured from ECG, documented vital signs, laboratory results, and other clinical assessments to predict clinical deterioration. A necessary step in translation to practice is understanding how an alert threshold would perform if applied to a continuous predictive analytic that was trained to detect clinical deterioration. The purpose of this study was to evaluate the positive predictive value of 'risk spikes', or large abrupt increases in the output of a statistical model of risk predicting clinical deterioration. We studied 8111 consecutive patient admissions to a cardiovascular medicine and surgery ward with continuous ECG data. We first trained a multivariable logistic regression model for emergent ICU transfer in a test set and tested the characteristics of the model in a validation set of 4059 patient admissions. Then, in a nested analysis we identified large, abrupt spikes in risk (increase by three units over the prior 6 h; a unit is the fold-increase in risk of ICU transfer in the next 24 h) and reviewed hospital records of 91 patients for clinical events such as emergent ICU transfer. We compared results to 59 control patients at times when they were matched for baseline risk including the National Warning Score (NEWS). There was a 3.4-fold higher event rate for patients with risk spikes (positive predictive value 24% compared to 7%,p = 0.006). If we were to use risk spikes as an alert, they would fire about once per day on a 73-bed acute care ward. Risk spikes that were primarily driven by respiratory changes (ECG-derived respiration (EDR) or charted respiratory rate) had highest PPV (30-35%) while risk spikes driven by heart rate had the lowest (7%). Alert thresholds derived from continuous predictive analytics monitoring are able to be operationalized as a degree of change from the person's own baseline rather than arbitrary threshold cut-points, which can likely better account for the individual's own inherent acuity levels. Point of care clinicians in the acute care ward settings need tailored alert strategies that promote a balance in recognition of clinical deterioration and assessment of the utility of the alert approach.
RI Moorman, J Randall/ABG-9946-2020
OI Keim-Malpass, Jessica/0000-0002-7035-8556; Clark,
   Matthew/0000-0002-4161-7897
SN 1387-1307
EI 1573-2614
PD AUG
PY 2020
VL 34
IS 4
BP 797
EP 804
DI 10.1007/s10877-019-00361-5
UT WOS:000549615200022
PM 31327101
ER

PT J
AU Stewart, R
   Goodship, V
   Guild, F
   Green, M
   Farrow, J
AF Stewart, R
   Goodship, V
   Guild, F
   Green, M
   Farrow, J
TI Investigation and demonstration of the durability of air plasma
   pre-treatment on polypropylene automotive bumpers
SO INTERNATIONAL JOURNAL OF ADHESION AND ADHESIVES
AB Higher utilisation of low-density materials such as polymers and polymer composites is a pre-requisite for the lightweight vehicle of the future. Of the commodity polymers polypropylene (PP) is by far the most attractive for the automotive industry. Additionally, PP can be utilised as a glass or mineral filled composite which may be used for semi-structural applications.
   A major problem is that PP (along with other polyolefins) has a non-polar surface chemistry which means the wetting characteristics of components made from this material are poor. Ultimately, this will result in poor adhesion of paints, coatings or adhesive bonding products. This problem has been overcome in the majority of instances by treating the surface of the substrate in order to alter the surface chemistry.
   PP has found extensive use as films and flat sheets and hence certain techniques such as flame and corona have been favoured despite both having problems of heterogeneous or patchy treatment across a surface. However, for complex 3-D automotive shapes, such as bumpers, these methods are less useful. While flame treatment for example is widely used it has several disadvantages in a commercial volume production environment which all centre around the potential for the part to undergo overtreatment, incipient melting or melting during machine stops etc., as well as the hazards associated with combustible gas. An alternative method is atmospheric plasma pre-treatment.
   This work has investigated the effect of a forced air plasma pre-treatment on surface chemistry and bond strength of a commercial grade polypropylene material.
   The plasma head was attached to a robot arm which makes it highly suited to continuous production environments and can treat complex surfaces. A range of translation speeds were investigated and the surface chemistry and topography of the treated surfaces were examined using atomic force microscopy and X-ray photoelectron spectroscopy. The processing was further optimised by single lap-shear testing.
   An optimised set of parameters was used to pre-treat and bond a full size automotive bumper assembly with a polyurethane (PU) adhesive. Bumpers were then subjected to a standard automotive range of climate conditioning as well as soaking at -20degreesC and 70degreesC before front centre impact testing. Parts pre-treated and bonded using this pre-treatment and adhesive system, successfully passed all the required standard automotive impact tests.
   For added benefit, it was also found that the open time of the pre-treatment was 1 week depending on storage conditions. (C) 2004 Elsevier Ltd. All rights reserved.
RI Guild, Felicity/AAX-7554-2020
SN 0143-7496
PD APR
PY 2005
VL 25
IS 2
BP 93
EP 99
DI 10.1016/j.ijadhadh.2004.04.001
UT WOS:000225139300001
ER

PT J
AU Elsharkawy, M
   Sharafeldeen, A
   Taher, F
   Shalaby, A
   Soliman, A
   Mahmoud, A
   Ghazal, M
   Khalil, A
   Alghamdi, NS
   Razek, AAKA
   Alnaghy, E
   El-Melegy, MT
   Sandhu, HS
   Giridharan, GA
   El-Baz, A
AF Elsharkawy, Mohamed
   Sharafeldeen, Ahmed
   Taher, Fatma
   Shalaby, Ahmed
   Soliman, Ahmed
   Mahmoud, Ali
   Ghazal, Mohammed
   Khalil, Ashraf
   Alghamdi, Norah Saleh
   Razek, Ahmed Abdel Khalek Abdel
   Alnaghy, Eman
   El-Melegy, Moumen T.
   Sandhu, Harpal Singh
   Giridharan, Guruprasad A.
   El-Baz, Ayman
TI Early assessment of lung function in coronavirus patients using
   invariant markers from chest X-rays images
SO SCIENTIFIC REPORTS
AB The primary goal of this manuscript is to develop a computer assisted diagnostic (CAD) system to assess pulmonary function and risk of mortality in patients with coronavirus disease 2019 (COVID-19). The CAD system processes chest X-ray data and provides accurate, objective imaging markers to assist in the determination of patients with a higher risk of death and thus are more likely to require mechanical ventilation and/or more intensive clinical care.To obtain an accurate stochastic model that has the ability to detect the severity of lung infection, we develop a second-order Markov-Gibbs random field (MGRF) invariant under rigid transformation (translation or rotation of the image) as well as scale (i.e., pixel size). The parameters of the MGRF model are learned automatically, given a training set of X-ray images with affected lung regions labeled. An X-ray input to the system undergoes pre-processing to correct for non-uniformity of illumination and to delimit the boundary of the lung, using either a fully-automated segmentation routine or manual delineation provided by the radiologist, prior to the diagnosis. The steps of the proposed methodology are: (i) estimate the Gibbs energy at several different radii to describe the inhomogeneity in lung infection; (ii) compute the cumulative distribution function (CDF) as a new representation to describe the local inhomogeneity in the infected region of lung; and (iii) input the CDFs to a new neural network-based fusion system to determine whether the severity of lung infection is low or high. This approach is tested on 200 clinical X-rays from 200 COVID-19 positive patients, 100 of whom died and 100 who recovered using multiple training/testing processes including leave-one-subject-out (LOSO), tenfold, fourfold, and twofold cross-validation tests. The Gibbs energy for lung pathology was estimated at three concentric rings of increasing radii. The accuracy and Dice similarity coefficient (DSC) of the system steadily improved as the radius increased. The overall CAD system combined the estimated Gibbs energy information from all radii and achieved a sensitivity, specificity, accuracy, and DSC of 100%, 97% +/- 3%, 98% +/- 2%, and 98% +/- 2%, respectively, by twofold cross validation. Alternative classification algorithms, including support vector machine, random forest, naive Bayes classifier, K-nearest neighbors, and decision trees all produced inferior results compared to the proposed neural network used in this CAD system. The experiments demonstrate the feasibility of the proposed system as a novel tool to objectively assess disease severity and predict mortality in COVID-19 patients. The proposed tool can assist physicians to determine which patients might require more intensive clinical care, such a mechanical respiratory support.
RI Elsharkawy, Mohamed/AAV-9439-2020; Taher, Fatma/AAA-2912-2021; MAHMOUD,
   ALI/HNO-9565-2023; Sharafeldeen, Ahmed/AAH-6875-2021; El-Baz,
   Ayman/AAC-6689-2019
OI Elsharkawy, Mohamed/0000-0001-9242-9709; Taher,
   Fatma/0000-0001-8358-9081; Sharafeldeen, Ahmed/0000-0002-6838-8211;
   El-Baz, Ayman/0000-0001-7264-1323; Ghazal, Mohammed/0000-0002-9045-6698;
   Khalil, Ashraf/0000-0003-1584-8525; , Norah/0000-0001-6421-6001
SN 2045-2322
PD JUN 8
PY 2021
VL 11
IS 1
AR 12095
DI 10.1038/s41598-021-91305-0
UT WOS:000663012100046
PM 34103587
ER

PT J
AU Qiu, QT
   Duan, JH
   Duan, ZY
   Meng, XJ
   Ma, CS
   Zhu, J
   Lu, J
   Liu, TH
   Yin, Y
AF Qiu, Qingtao
   Duan, Jinghao
   Duan, Zuyun
   Meng, Xiangjuan
   Ma, Changsheng
   Zhu, Jian
   Lu, Jie
   Liu, Tonghai
   Yin, Yong
TI Reproducibility and non-redundancy of radiomic features extracted from
   arterial phase CT scans in hepatocellular carcinoma patients: impact of
   tumor segmentation variability
SO QUANTITATIVE IMAGING IN MEDICINE AND SURGERY
AB Background: The reproducibility and non-redundancy of radiomic features are challenges in accelerating the clinical translation of radiomics. In this study, we focused on the robustness and non-redundancy of radiomic features extracted from computed tomography (CT) scans in hepatocellular carcinoma (HCC) patients with respect to different tumor segmentation methods.
   Methods: Arterial enhanced CT images were retrospectively randomly obtained from 106 patients. As a training data set, 26 HCC patients were used to calculate the features' reproducibility and redundancy. Another data set (55 HCC patients and 25 healthy volunteers) was used for classification. The GrowCut and GraphCut semiautomatic segmentation methods were implemented in 3D Slicer software by two independent observers, and manual delineation was performed by five abdominal radiation oncologists to acquire the gross tumor volume (GTV). Seventy-one radiomic features were extracted from GTVs using Imaging Biomarker Explorer (IBEX) software, including 17 tumor intensity statistical features, 16 shape features and 38 textural features. For each radiomic feature, intraclass correlation coefficient (ICC) and hierarchical clustering were used to quantify its reproducibility and redundancy. Features with ICC values greater than 0.75 were considered reproducible. To generate the number of non-redundancy feature subgroups, the R-2 statistic method was used. Then, a classification model was built using a support vector machine (SVM) algorithm with 10-fold cross validation, and area under ROC curve (AUC) was used to evaluate the utility of non-redundant feature extraction by hierarchical clustering.
   Results: The percentages of excellent reproducible features in the manual delineation group, GraphCut and GrowCut segmentation group were 69% [49], 73% [52] and 79% [56], respectively. Sixty-five percent [46] of the features showed strong robustness for all segmentation methods. The optimal number of cluster subgroup were 9, 13 and 11 for manual delineation, GraphCut and GrowCut segmentation, respectively. The optimal cluster subgroup number was 6 for all groups when the collectively high reproducibility features were selected for clustering. The receiver operating characteristic (ROC) analysis of radiomics classification model with and without feature reduction for healthy liver and HCC had an AUC value of 0.857 and 0.721 respectively.
   Conclusions: Our study demonstrates that variations exist in the reproducibility of quantitative imaging features extracted from tumor regions segmented using different methods. The reproducibility and non-redundancy of the radiomic features rely greatly on the tumor segmentation in HCC CT images. We recommend that the most reliable and uniform radiomic features should be selected in the clinical use of radiomics. Classification experiments with feature reduction showed that radiomic features were effective in identifying healthy liver and HCC.
RI Qiu, Qingtao/AAS-9287-2020
SN 2223-4292
EI 2223-4306
PD MAR
PY 2019
VL 9
IS 3
BP 453
EP +
DI 10.21037/qims.2019.03.02
UT WOS:000462730000010
PM 31032192
ER

PT J
AU Mueller, U
   Harzi, A
   Loescher, R
   Buelhoff, M
   Eckert, JA
   Kretzer, JP
AF Mueller, Ulrike
   Harzi, Amal
   Loescher, Raphael
   Buelhoff, Matthias
   Eckert, Johannes A.
   Kretzer, Jan Philippe
TI Wear and damage in retrieved humeral inlays of reverse total shoulder
   arthroplasty-where, how much, and why?
SO JOURNAL OF SHOULDER AND ELBOW SURGERY
AB Background: Polyethylene (PE) wear and material degradation have been reported as complications in reverse total shoulder replacements (rTSAs). In this regard, scapular notching is associated with more clinical complications. Therefore, the purposes of the study were to quantify the linear and volumetric wear, as a measure for the amount of removed material, and to qualitatively assess the PE damage modes to describe the material degradation in retrieved rTSA humeral PE inlays that contribute to failure of shoulder replacements. Furthermore, this study aimed to evaluate the effect of scapular notching on PE wear and rim damage of the humeral components.
   Methods: The total study population of 39 humeral inlays contains 2 cohorts that were used for the damage mode analysis and for the wear analysis, respectively. The extent and presence of wear damage modes in 5 defined zones were assessed by a grading system for all PE joint replacements. For quantitative wear analysis the most frequent design (n = 17) was chosen. Using a coordinate-measuring machine and postprocessing software, volumetric wear measurements for the retrieved humeral PE inlays were undertaken. Furthermore, prerevision radiographs were analyzed for scapular notching. Finally, retrieval findings were correlated with clinical and radiographic data to consider the effect of notching and to identify risk of failures for these prostheses.
   Results: Damage on the rim of the humeral PE inlays was more frequent and severe than on the intended articulation surface. Irrespective of the damage mode, the inferior rim zone sustained the greatest amount of wear damage followed by the posterior zone. Burnishing, scratching, pitting, and embedded particles are most likely to occur in the articular surface area, whereas surface deformation, abrasion, delamination and gross material degradation are predominantly present in the inferior and posterior rim zones. The retrieved inlays exhibited a mean volumetric wear rate of 296.9 mm(3)/yr +/- 87.0 mm(3)/yr. However, if the notched and non-notched components were compared, a significant higher volumetric wear rate (296.5 +/- 106.1 mm(3)/yr) was found for the notched components compared to the non-notched group (65.7 +/- 7.4 mm(3)/yr). Generally, there was a significantly greater incidence of damage and greater amount of wear if scapular notching occurred.
   Conclusion: The notched components showed a 5-fold increase in PE wear rate. Therefore, scapular notching has a strong effect on PE damage and wear. If scapular notching can be clinically avoided, the PE wear performance is in a similar magnitude as found for hip and knee replacements. (C) 2020 Journal of Shoulder and Elbow Surgery Board of Trustees. All rights reserved.
OI Eckert, Johannes/0000-0003-0240-0238
SN 1058-2746
EI 1532-6500
PD AUG
PY 2021
VL 30
IS 8
BP E517
EP E530
DI 10.1016/j.jse.2020.10.015
EA JUL 2021
UT WOS:000676022900005
PM 33220411
ER

PT J
AU Shahab, QS
   Young, IM
   Dadario, NB
   Tanglay, O
   Nicholas, PJ
   Lin, YH
   Fonseka, RD
   Yeung, JT
   Bai, MY
   Teo, C
   Doyen, S
   Sughrue, ME
AF Shahab, Qazi S.
   Young, Isabella M.
   Dadario, Nicholas B.
   Tanglay, Onur
   Nicholas, Peter J.
   Lin, Yueh-Hsin
   Fonseka, R. Dineth
   Yeung, Jacky T.
   Bai, Michael Y.
   Teo, Charles
   Doyen, Stephane
   Sughrue, Michael E.
TI A connectivity model of the anatomic substrates underlying Gerstmann
   syndrome
SO BRAIN COMMUNICATIONS
AB Shahab et al. report a connectivity model of the neuroanatomic substrates which are likely impaired in Gerstmann's Syndrome. A frontoparietal network converging on three specific parcellations in the anteromedial portion of the intraparietal sulcus was found to be involved in calculation, writing, finger gnosis and left-right orientation.
   The Gerstmann syndrome is a constellation of neurological deficits that include agraphia, acalculia, left-right discrimination and finger agnosia. Despite a growing interest in this clinical phenomenon, there remains controversy regarding the specific neuroanatomic substrates involved. Advancements in data-driven, computational modelling provides an opportunity to create a unified cortical model with greater anatomic precision based on underlying structural and functional connectivity across complex cognitive domains. A literature search was conducted for healthy task-based functional MRI and PET studies for the four cognitive domains underlying Gerstmann's tetrad using the electronic databases PubMed, Medline, and BrainMap Sleuth (2.4). Coordinate-based, meta-analytic software was utilized to gather relevant regions of interest from included studies to create an activation likelihood estimation (ALE) map for each cognitive domain. Machine-learning was used to match activated regions of the ALE to the corresponding parcel from the cortical parcellation scheme previously published under the Human Connectome Project (HCP). Diffusion spectrum imaging-based tractography was performed to determine the structural connectivity between relevant parcels in each domain on 51 healthy subjects from the HCP database. Ultimately 102 functional MRI studies met our inclusion criteria. A frontoparietal network was found to be involved in the four cognitive domains: calculation, writing, finger gnosis, and left-right orientation. There were three parcels in the left hemisphere, where the ALE of at least three cognitive domains were found to be overlapping, specifically the anterior intraparietal area, area 7 postcentral (7PC) and the medial intraparietal sulcus. These parcels surround the anteromedial portion of the intraparietal sulcus. Area 7PC was found to be involved in all four domains. These regions were extensively connected in the intraparietal sulcus, as well as with a number of surrounding large-scale brain networks involved in higher-order functions. We present a tractographic model of the four neural networks involved in the functions which are impaired in Gerstmann syndrome. We identified a 'Gerstmann Core' of extensively connected functional regions where at least three of the four networks overlap. These results provide clinically actionable and precise anatomic information which may help guide clinical translation in this region, such as during resective brain surgery in or near the intraparietal sulcus, and provides an empiric basis for future study.
RI Fonseka, R. Dineth/HMU-9672-2023
OI Fonseka, R. Dineth/0000-0002-7748-5101; Dadario,
   Nicholas/0000-0002-8657-187X
EI 2632-1297
PD MAY 2
PY 2022
VL 4
IS 3
AR fcac140
DI 10.1093/braincomms/fcac140
UT WOS:000814338900003
PM 35706977
ER

PT C
AU Centurelli, F
   Scotti, G
   Tommasino, P
   Trifiletti, A
   Romano, F
   Cimmino, R
   Saitto, A
AF Centurelli, F.
   Scotti, G.
   Tommasino, P.
   Trifiletti, A.
   Romano, F.
   Cimmino, R.
   Saitto, A.
BE Angeli, GZ
   Dierickx, P
TI Feed Array Metrology And Correction Layer For Large Antenna Systems in
   ASIC Mixed Signal Technology
SO MODELING, SYSTEMS ENGINEERING, AND PROJECT MANAGEMENT FOR ASTRONOMY VI
SE Proceedings of SPIE
CT Conference on Modeling, Systems Engineering, and Project Management for
   Astronomy VI
CY JUN 22-24, 2014
CL Montreal, CANADA
SP SPIE, American Astronom Soc, Australian Astronom Observ, Assoc Univ Res Astron, Canadian Astronom Soc, Canadian Space Agcy, European Astronom Soc, European So Observ, Natl Radio Astron Observ, Royal Astronom Soc, Sci & Technol Facilities Council
AB The paper deals with a possible use of the feed array present in a large antenna system, as a layer for measuring the antenna performance with a self-test procedure and a possible way to correct residual errors of the Antenna geometry and of the antenna distortions.
   Focus has been concentrated on a few key critical elements of a possible feed array metrology program. In particular, a preliminary contribution to the design and development of the feed array from one side, and the subsystem dedicated to antenna distortion monitoring and control from the other, have been chosen as the first areas of investigation.
   Scalability and flexibility principles and synergic approach with other coexistent technologies have been assumed of paramount importance to ensure ease of integrated operation and therefore allowing in principle increased performance and efficiency.
   The concept is based on the use of an existing feed array grid to measure antenna distortion with respect to the nominal configuration. Measured data are then processed to develop a multilayer strategy to control the mechanical movable devices (when existing) and to adjust the residual fine errors through a software controlled phase adjustment of the existing phase shifter
   The signal from the feed array is converted passing through a FPGA/ASIC level to digital data channels. The kind of those typically used for the scientific experiments. One additional channel is used for monitoring the antenna distortion status.
   These data are processed to define the best correction strategy, based on a software managed control system capable of operating at three different levels of the antenna system: reflector rotation layer, sub reflector rotation and translation layer (assuming the possibility of controlling a Stewart machine), phase shifter of the phased array layer.
   The project is at present in the design phase, a few elements necessary for a sound software design of the control subsystem have been developed at a technological demonstrator level while the ASIC board for generating the digital data stream has been fully developed. A prototype for control accurately the position of the sub-reflector up to a diameter of 5 meters (similar to the sub reflector size of a large antenna) using a Stewart mechanism is being planned.
   The selection strategy of the correction modes will depend on the dynamics of the phased array (i.e. the available bits of the A/D conversion). The reaction time allowed for the correction, depending on the error type and the inertia of the sub systems. Typically, the compensation can be divided among all the adjusting elements.
RI Centurelli, Francesco/AAE-1590-2020; Scotti, Giuseppe/ABE-8233-2021
OI Centurelli, Francesco/0000-0003-3880-2546; Scotti,
   Giuseppe/0000-0002-5650-8212
SN 0277-786X
EI 1996-756X
BN 978-0-8194-9618-8
PY 2014
VL 9150
AR 91500F
DI 10.1117/12.2051302
UT WOS:000343032800014
ER

PT J
AU Bai, RR
   Zhang, CY
   Wang, L
   Yao, CS
   Ge, JM
   Duan, HL
AF Bai, Renren
   Zhang, Chengyun
   Wang, Ling
   Yao, Chuansheng
   Ge, Jiamin
   Duan, Hongliang
TI Transfer Learning: Making Retrosynthetic Predictions Based on a Small
   Chemical Reaction Dataset Scale to a New Level
SO MOLECULES
AB Effective computational prediction of complex or novel molecule syntheses can greatly help organic and medicinal chemistry. Retrosynthetic analysis is a method employed by chemists to predict synthetic routes to target compounds. The target compounds are incrementally converted into simpler compounds until the starting compounds are commercially available. However, predictions based on small chemical datasets often result in low accuracy due to an insufficient number of samples. To address this limitation, we introduced transfer learning to retrosynthetic analysis. Transfer learning is a machine learning approach that trains a model on one task and then applies the model to a related but different task; this approach can be used to solve the limitation of few data. The unclassified USPTO-380K large dataset was first applied to models for pretraining so that they gain a basic theoretical knowledge of chemistry, such as the chirality of compounds, reaction types and the SMILES form of chemical structure of compounds. The USPTO-380K and the USPTO-50K (which was also used by Liu et al.) were originally derived from Lowe's patent mining work. Liu et al. further processed these data and divided the reaction examples into 10 categories, but we did not. Subsequently, the acquired skills were transferred to be used on the classified USPTO-50K small dataset for continuous training and retrosynthetic reaction tests, and the pretrained accuracy data were simultaneously compared with the accuracy of results from models without pretraining. The transfer learning concept was combined with the sequence-to-sequence (seq2seq) or Transformer model for prediction and verification. The seq2seq and Transformer models, both of which are based on an encoder-decoder architecture, were originally constructed for language translation missions. The two algorithms translate SMILES form of structures of reactants to SMILES form of products, also taking into account other relevant chemical information (chirality, reaction types and conditions). The results demonstrated that the accuracy of the retrosynthetic analysis by the seq2seq and Transformer models after pretraining was significantly improved. The top-1 accuracy (which is the accuracy rate of the first prediction matching the actual result) of the Transformer-transfer-learning model increased from 52.4% to 60.7% with greatly improved prediction power. The model's top-20 prediction accuracy (which is the accuracy rate of the top 20 categories containing actual results) was 88.9%, which represents fairly good prediction in retrosynthetic analysis. In summary, this study proves that transferring learning between models working with different chemical datasets is feasible. The introduction of transfer learning to a model significantly improved prediction accuracy and, especially, assisted in small dataset based reaction prediction and retrosynthetic analysis.
OI Bai, Renren/0000-0002-3511-5794
EI 1420-3049
PD MAY
PY 2020
VL 25
IS 10
AR 2357
DI 10.3390/molecules25102357
UT WOS:000539293400090
PM 32438572
ER

PT J
AU Raoulis, VA
   Zibis, A
   Chiotelli, MD
   Kermanidis, AT
   Banios, K
   Schuster, P
   Hantes, ME
AF Raoulis, Vasilios A.
   Zibis, Aristidis
   Chiotelli, Maria Dimitra
   Kermanidis, Alexis T.
   Banios, Konstantinos
   Schuster, Philipp
   Hantes, Michael E.
TI Biomechanical evaluation of three patellar fixation techniques for MPFL
   reconstruction: Load to failure did not differ but interference screw
   stabilization was stiffer than suture anchor and suture-knot fixation
SO KNEE SURGERY SPORTS TRAUMATOLOGY ARTHROSCOPY
AB Purpose The purpose of this study was to compare the maximum load to failure and stiffness of three medial patella-femoral ligament (MPFL) reconstruction techniques: (i) suture anchor fixation (SA), (ii) interference screw fixation (SF), and (iii) suture knot (SK) patellar fixation. The null hypothesis was that the comparison between these three different patella fixation techniques would show no difference in the ultimate failure load and stiffness.
   Methods Reconstruction of the MPFL with gracilis tendon autograft was performed in 12 pairs of fresh-frozen cadaveric knees (24 knees total; mean age, 63.6 +/- 8.0 years). The specimens were randomly distributed into 3 groups of 8 specimens; SA reconstruction was completed with two 3.0-mm metal suture anchors; (SF) fixation was accomplished by two 6-mm bio-composite interference screws; SK fixation at the lateral side of the patella was accomplished after drilling two semi-patellar tunnels with a diameter of 4.5 mm. The reconstructions were subjected to cyclic loading for 10 cycles to 30 N and tested to failure at a constant displacement rate of 15 mm/min using a materials-testing machine (MTS 810 Universal Testing System). The final load of failure (N), stiffness (N / mm) and failure mode was recorded in each specimen and followed by statistical analysis. Results There was no significant difference in mean ultimate failure load among the three groups. The SK group failed at a mean (+/- SD) ultimate load of 253.5 +/- 38.2 N, the SA group failed at 243 +/- 41.9 N and the SF group at 263.2 +/- 9.06 N. The SF group had a mean stiffness of 37.8 +/- 5.7 N/mm. This was significantly higher (p < 0.05) than the mean stiffness value achieved for the SK group 21.4 +/- 9.5 N/mm and the SA group 18.7 +/- 3.4 N/mm. The most common mode of failure in the SA group was anchor pullout, and in the SK group was failure at the graft-suture interface. All the reconstructions in the SF group failed due to tendon graft slippage from the tunnel.
   Conclusion Load to failure was not significantly different between the 3 techniques. However, screw fixation was found to be significantly stronger than the anchor and the suture knot fixation in terms of rigidity of the reconstruction. From a clinical point of view, all methods of fixation can be used reliably for MPFL reconstruction, since they were found to be stronger than the native MPFL.
RI RAOULIS, VASILEIOS/ABE-2807-2020
OI RAOULIS, VASILEIOS/0000-0003-4585-7545; Hantes,
   Michael/0000-0001-9494-6048; Schuster, Philipp/0000-0002-2418-6642;
   Chiotelli, Maria Dimitra/0000-0001-5975-7989; Zibis,
   Aristeidis/0000-0001-7122-4317
SN 0942-2056
EI 1433-7347
PD NOV
PY 2021
VL 29
IS 11
BP 3697
EP 3705
DI 10.1007/s00167-020-06389-4
EA JAN 2021
UT WOS:000604168600008
PM 33386885
ER

PT J
AU Muneer, I
   Nawab, RMA
AF Muneer, Iqra
   Nawab, Rao Muhammad Adeel
TI Develop corpora and methods for cross-lingual text reuse detection for
   English Urdu language pair at lexical, syntactical, and phrasal levels
SO LANGUAGE RESOURCES AND EVALUATION
AB In recent years, Cross-Lingual Text Reuse Detection (CLTRD) has attracted the attention of the research community because large digital repositories and efficient Machine Translation systems are readily and freely available, which makes it easier to reuse text across the languages and very difficult to detect it. In the previous studies, the problem of CLTRD for the English-Urdu language pair has been explored at the sentence/passage and document level, and benchmark corpora and methods have been developed. However, there is a lack of benchmark corpora and methods for the CLTRD for the English-Urdu language pair at the lexical, syntactical, and phrasal levels. To fulfill this research gap, this study presents three large benchmark corpora for detecting the Cross-Lingual Text Reuse (CLTR) at three levels of rewrite (Wholly Derived (WD), Partially Derived (PD), and Non Derived (ND)). The CLEU-Lex, CLEU-Syn and CLEU-Phr corpora contain 66,485 (WD = 22,236, PD = 20,315 and ND = 23,934), 60,267 (WD = 20,007, PD = 16,979 and ND = 23,281) and 60,106 (WD = 23,862, PD = 15,878 and ND = 20,366) CLTR pairs respectively. As a secondary major contribution, we have applied the Cross-Lingual Word Embedding (CLWE), Cross-Lingual Semantic Tagger (CLST), and Cross-Lingual Sentence Transformer (CLSTR) based methods on our three proposed corpora for the CLTRD. Our extensive experimentation showed that for the binary classification task, the best results on the CLEU-Lex corpus were obtained using the cross-lingual sentence transformer (F-1 = 0.80). For the CLEU-Syn and CLEU-Phr corpora, the best results were obtained using the cross-lingual sentence transformer and a combination of the CLWE, CLST and CLSTR methods (F-1 = 0.92 on CLEU-Syn and F-1 = 0.94 on CLEU-Phr). For the ternary classification task, the best results on the CLEU-Lex corpus were obtained using the cross-lingual sentence transformer method (F-1 = 0.69). For the CLEU-Syn corpus, the best results were obtained using a combination of the CLWE, CLST, and CLSTR methods (F-1 = 0.82). For the CLEU-Phr corpus the best results were obtained using cross-lingual sentence transformer and combination of CLWE, CLST, and CLSTR methods (F-1 = 0.78). To foster and promote research in Urdu (a low-resourced language) all the three proposed corpora are free and publicly available for research purposes.
SN 1574-020X
EI 1574-0218
PD DEC
PY 2022
VL 56
IS 4
BP 1103
EP 1130
DI 10.1007/s10579-022-09613-4
EA SEP 2022
UT WOS:000852130500002
ER

PT C
AU Perez-Meana, H
AF Perez-Meana, Hector
BE Fujita, H
   Sasaki, J
TI Plenary Lecture 1 Face Recognition Using Frequency Domain Feature
   Extraction Methods
SO SELECTED TOPICS IN APPLIED COMPUTER SCIENCE
SE International Conference on Applied Computer Science
CT 10th WSEAS International Conference on Applied Computer Science
CY OCT 04-06, 2010
CL Iwate Prefectural Univ, Iwate, JAPAN
HO Iwate Prefectural Univ
AB The development of security systems based on biometric features has been a topic of active research during the last three decades, because the recognition of the people identity to access control is a fundamental issue in these days. Terrorist attacks happened during the last decade have demonstrated that it is indispensable to have reliable security systems in offices, banks, airports, etc.; increasing in such way the necessity to develop more reliable methods for people recognition. The biometrics systems consist of a group of automated methods for recognition or verification of people identity using the physical characteristics or personal behavior of the person under analysis. In particular the face recognition has been a topic of active research because the face is the most direct way to recognize the people. In addition, the data acquisition of this method consists, simply, of taking a picture with or without collaboration of the person under analysis, doing it one of the biometric methods with larger acceptance among the users.
   The face recognition is a very complex activity of the human brain. For example, we can recognize hundred of faces learned throughout our life and to identify familiar faces at the first sight, even after several years of separation, with relative easy. However it is not a simple task for a computer. Thus to develop high performance face recognition systems, we must to develop accurate feature extraction and classification methods, because, as happens with any pattern recognition algorithm, the performance of a face recognition algorithm strongly depends on the feature extraction method and the classification systems used to carry out the face recognition task. Thus during the last decades several feature extraction methods for using in face recognition systems have been proposed during the last decades, which achieve high accurate recognition. Among the situations that drastically decrease the accuracy and that must be considered to develop high performance face recognition method we have: partial occlusion, illumination variations, size change, rotation and translation of the capture image, etc. To solve these problems several efficient feature extraction methods have been proposed, several of them using frequency domain transforms such as discrete Gabor transform, discrete Fourier transform, Discrete cosine transform, etc. These methods achieve recognition rates higher than 90%.
   In this talk, we analyze several frequency domain feature extraction methods based on the Discrete Gabor transform, Discrete Fourier Transform, Discrete Wavelet Transform, Discrete Cosine Transform, Discrete Walsh-Hadamard Transform and Eigenphases. These feature extraction methods are used with different classifiers such as artificial neural networks (ANN), Gaussian Mixture Models (GMM) and Support vector machines (SVM). The evaluation results were obtained using well known public domain databases such as "AR Face Database".
SN 1792-4863
BN 978-960-474-231-8
PY 2010
BP 15
EP 15
UT WOS:000290364500001
ER

PT J
AU Chiu, YC
   Chen, HIH
   Zhang, TH
   Zhang, SY
   Gorthi, A
   Wang, LJ
   Huang, YF
   Chen, YD
AF Chiu, Yu-Chiao
   Chen, Hung-I Harry
   Zhang, Tinghe
   Zhang, Songyao
   Gorthi, Aparna
   Wang, Li-Ju
   Huang, Yufei
   Chen, Yidong
TI Predicting drug response of tumors from integrated genomic profiles by
   deep neural networks
SO BMC MEDICAL GENOMICS
CT International Conference on Intelligent Biology and Medicine (ICIBM) -
   Medical Genomics
CY JUN 10-12, 2018
CL Los Angeles, CA
AB BackgroundThe study of high-throughput genomic profiles from a pharmacogenomics viewpoint has provided unprecedented insights into the oncogenic features modulating drug response. A recent study screened for the response of a thousand human cancer cell lines to a wide collection of anti-cancer drugs and illuminated the link between cellular genotypes and vulnerability. However, due to essential differences between cell lines and tumors, to date the translation into predicting drug response in tumors remains challenging. Recently, advances in deep learning have revolutionized bioinformatics and introduced new techniques to the integration of genomic data. Its application on pharmacogenomics may fill the gap between genomics and drug response and improve the prediction of drug response in tumors.ResultsWe proposed a deep learning model to predict drug response (DeepDR) based on mutation and expression profiles of a cancer cell or a tumor. The model contains three deep neural networks (DNNs), i) a mutation encoder pre-trained using a large pan-cancer dataset(The Cancer Genome Atlas; TCGA) to abstract core representations of high-dimension mutation data, ii) a pre-trained expression encoder, and iii) a drug response predictor network integrating the first two subnetworks. Given a pair of mutation and expression profiles, the model predicts IC50 values of 265 drugs. We trained and tested the model on a dataset of 622 cancer cell lines and achieved an overall prediction performance of mean squared error at 1.96 (log-scale IC50 values). The performance was superior in prediction error or stability than two classical methods (linear regression and support vector machine) and four analog DNN models of DeepDR, including DNNs built without TCGA pre-training, partly replaced by principal components, and built on individual types of input data. We then applied the model to predict drug response of 9059 tumors of 33 cancer types. Using per-cancer and pan-cancer settings, the model predicted both known, including EGFR inhibitors in non-small cell lung cancer and tamoxifen in ER+ breast cancer, and novel drug targets, such as vinorelbine for TTN-mutated tumors. The comprehensive analysis further revealed the molecular mechanisms underlying the resistance to a chemotherapeutic drug docetaxel in a pan-cancer setting and the anti-cancer potential of a novel agent, CX-5461, in treating gliomas and hematopoietic malignancies.ConclusionsHere we present, as far as we know, the first DNN model to translate pharmacogenomics features identified from in vitro drug screening to predict the response of tumors. The results covered both well-studied and novel mechanisms of drug resistance and drug targets. Our model and findings improve the prediction of drug response and the identification of novel therapeutic options.
RI GUPTA, APARNA/HLV-8653-2023; Chen, Yi/HPD-0595-2023; Huang,
   Yufei/AAU-2891-2021
OI Gorthi, Aparna/0000-0001-5935-4098; Chiu, Yu-Chiao/0000-0003-1647-8634
SN 1755-8794
PD JAN 31
PY 2019
VL 12
SU 1
AR 18
DI 10.1186/s12920-018-0460-9
UT WOS:000457463500012
PM 30704458
ER

PT J
AU Jutzi, TB
   Krieghoff-Henning, EI
   Brinker, TJ
AF Jutzi, Tanja B.
   Krieghoff-Henning, Eva, I
   Brinker, Titus J.
TI The Rise of Artificial Intelligence - High Prediction Accuracy in Early
   Detection of Pigmented Melanoma
SO AKTUELLE DERMATOLOGIE
AB The incidence of malignant melanoma is increasing worldwide. If detected early, melanoma is highly treatable, so early detection is vital.
   Skin cancer early detection has improved significantly in recent decades, for example by the introduction of screening in 2008 and dermoscopy. Nevertheless, in particular visual detection of early melanomas remains challenging because they show many morphological overlaps with nevi. Hence, there continues to be a high medical need to further develop methods for early skin cancer detection in order to be able to reliably diagnose melanomas at a very early stage.
   Routine diagnostics for melanoma detection include visual whole body inspection, often supplemented by dermoscopy, which can significantly increase the diagnostic accuracy of experienced dermatologists. A procedure that is additionally offered in some practices and clinics is whole-body photography combined with digital dermoscopy for the early detection of malignant melanoma, especially for monitoring high-risk patients.
   In recent decades, numerous noninvasive adjunctive diagnostic techniques were developed for the examination of suspicious pigmented moles, that may have the potential to allow improved and, in some cases, automated evaluation of these lesions. First, confocal laser microscopy should be mentioned here, as well as electrical impedance spectroscopy, multiphoton laser tomography, multispectral analysis, Raman spectroscopy or optical coherence tomography. These diagnostic techniques usually focus on high sensitivity to avoid malignant melanoma being overlooked. However, this usually implies lower specificity, which may lead to unnecessary excision of benign lesions in screening. Also, some of the procedures are time-consuming and costly, which also limits their applicability in skin cancer screening. In the near future, the use of artificial intelligence might change skin cancer diagnostics in many ways. The most promising approach may be the analysis of routine macroscopic and dermoscopic images by artificial intelligence.
   For the classification of pigmented skin lesions based on macroscopic and dermoscopic images, artificial intelligence, especially in form of neural networks, has achieved comparable diagnostic accuracies to dermatologists under experimental conditions in numerous studies. In particular, it achieved high accuracies in the binary melanoma/nevus classification task, but it also performed comparably well to dermatologists in multiclass differentiation of various skin diseases. However, proof of the basic applicability and utility of such systems in clinical practice is still pending. Prerequisites that remain to be established to enable translation of such diagnostic systems into dermatological routine are means that allow users to comprehend the system's decisions as well as a uniformly high performance of the algorithms on image data from other hospitals and practices.
   At present, hints are accumulating that computer-aided diagnosis systems could provide their greatest benefit as assistance systems, since studies indicate that a combination of human and machine achieves the best results. Diagnostic systems based on artificial intelligence are capable of detecting morphological characteristics quickly, quantitatively, objectively and reproducibly, and could thus provide a more objective analytical basis - in addition to medical experience.
OI Brinker, Titus Josef/0000-0002-3620-5919
SN 0340-2541
EI 1438-938X
PD MAR
PY 2022
VL 48
IS 03
BP 84
EP 91
DI 10.1055/a-1514-2013
UT WOS:000769699900011
PM 36580975
ER

PT J
AU Liu, C
   Nguyen, MA
   Alvarez-Ciara, A
   Franklin, M
   Bennett, C
   Domena, JB
   Kleinhenz, NC
   Colmenares, GAB
   Duque, S
   Chebbi, AF
   Bernard, B
   Olivier, JH
   Prasad, A
AF Liu, Chuan
   Nguyen, Michelle A.
   Alvarez-Ciara, Anabel
   Franklin, Melissa
   Bennett, Cassie
   Domena, Justin B.
   Kleinhenz, Noah C.
   Colmenares, Gabriel A. Blanco
   Duque, Sebastian
   Chebbi, Aisha F.
   Bernard, Brianna
   Olivier, Jean-Hubert
   Prasad, Abhishek
TI Surface Modifications of an Organic Polymer-Based Microwire Platform for
   Sustained Release of an Anti-Inflammatory Drug
SO ACS APPLIED BIO MATERIALS
AB Brain machine interfaces (BMIs), introduced into the daily lives of individuals with injuries or disorders of the nervous system such as spinal cord injury, stroke, or amyotrophic lateral sclerosis, can improve the quality of life. BMIs rely on the capability of microelectrode arrays to monitor the activity of large populations of neurons. However, maintaining a stable, chronic electrode-tissue interface that can record neuronal activity with a high signal-to-noise ratio is a key challenge that has limited the translation of such technologies. An electrode implant injury leads to a chronic foreign body response that is well-characterized and shown to affect the electrode-tissue interface stability. Several strategies have been applied to modulate the immune response, including the application of immunomodulatory drugs applied both systemically and locally. While the use of passive drug release at the site of injury has been exploited to minimize neuroinflammation, this strategy has all but failed as a bolus of antiinflammatory drugs is released at predetermined times that are often inconsistent with the ongoing innate inflammatory process. Common strategies do not focus on the proper anchorage of soft hydrogel scaffolds on electrode surfaces, which often results in delamination of the porous network from electrodes. In this study, we developed a microwire platform that features a robust yet soft biocompatible hydrogel coating, enabling long-lasting drug release via formation of drug aggregates and dismantlement of hydrophilic biodegradable three-dimensional polymer networks. Facile surface chemistry is developed to functionalize polyimide-coated electrodes with the covalently anchored porous hydrogel network bearing large numbers of highly biodegradable ester groups. Exponential long-lasting drug release is achieved using such hydrogels. We show that the initial state of dexamethasone (Dex) used to formulate the hydrogel precursor solution plays a cardinal role in engineering hydrophilic networks that enable a sustained and long-lasting release of the anti-inflammatory agent. Furthermore, utilization of a high loading ratio that exceeds the solubility of Dex leads to the encapsulation of Dex aggregates that regulate the release of this anti-inflammatory agent. To validate the anti-inflammatory effect of the hydrogel-functionalized Dex-loaded microwires, an in vivo preliminary study was performed in adult male rats (n = 10) for the acute time points of 48 h and 7 days post implant. Quantitative real-time polymerase chain reaction (qRT-PCR) was used to assess the mRNA expression of certain inflammatory-related genes. In general, a decrease in fold-change expression was observed for all genes tested for Dex-loaded wires compared with controls (functionalized but no drug). The engineering of hybrid microwires enables a sustained release of the anti-inflammatory agent over extended periods of time, thus paving the way to fabricate neuroprosthetic devices capable of attenuating the foreign body response.
RI Liu, Chuan/M-6816-2014
OI Liu, Chuan/0000-0002-4387-7836; Domena, Justin/0000-0003-1276-7711;
   Blanco Colmenares, Gabriel/0000-0002-1073-840X
SN 2576-6422
PD JUL 20
PY 2020
VL 3
IS 7
BP 4613
EP 4625
DI 10.1021/acsabm.0c00506
UT WOS:000604597100068
PM 35025460
ER

PT J
AU Frost, E
   Quinn, T
AF Frost, Elton, Jr.
   Quinn, Terrence
TI Data Preconditioning for Predictive and Interpretative
   Algorithms:Importance in Data-Driven Analytics and Methods for
   Application
SO PETROPHYSICS
AB Although much has been written about methods for estimating and interpreting log measurements, new emphasis must be given to data quality as data-mining methods become more prevalent. These methods are highly dependent on the quality of the original acquired data sets. Wireline and logging-while-drilling (LWD) technologies have advanced to a level where today's analysts frequently assume acquired measurements are correct. Unless problems are encountered in integrating the data, the problem may not be recognized in subsequent data-driven analytics (DDA). Assumptions regarding the quality of modern data are generally valid, but often fail when conditions within the borehole degrade to the point of falling outside the physical measurement limitations of the instruments, or when mixing older data from different tool designs with modern data.
   When wellbore conditions reach a point where data degradation occurs, the information must be corrected for enviromnental and borehole geometry effects or, in extreme cases, the data must be reconstructed. Data reconstruction/estimation can take many forms including translation application of regional trends, transformation of one type of measurement into another, extrapolation of offset well data to the well of interest, use of offset openhole data combined with casedhole data in predicting the measurements on adjoining wells when only the casedhole data are available, extraction of measurements from seismic etc. Methods involved in these endeavors vary from empirical algorithms to regional trend analysis, to statistical inference, to neural nets, to data mining and other forms of machine learning. Successful application of any method requires data that are representative of the formations when acquired under optimal conditions to assure their optimum use in defining the proper representation of the data to be derived. New evaluation technologies (log, lab, algorithms) must be assessed prior to pinning data quality on these new technologies.
   Interpretation algorithms applied to the data are no different, in that data quality is assumed for the analysis models to function correctly. Proprietary internal and third-party external interpretation packages all have problems when the data quality suffers.
   Proper data reconstruction requires an understanding of the quality of the acquired data (calibrations, accuracy etc.), the instrument configuration in the toolstring, the acquisition methodology and the condition of the wellbore environment when the data were acquired.
   We will examine the application of several methods used in data preconditioning and data reconstruction, along with some novel statistical methods the authors employ, with examples in various environments. Validation of the reconstructed data sets is also demonstrated. With the digital oil field (DOF) accessing single and multiple data stores involving data extraction for geological to engineering applications, quality and meaningful log and petrophysical data must be a part of these data stores. Recommendations for data-quality assurance before being employed in mining databases are provided.
SN 1529-9074
PD DEC
PY 2018
VL 59
IS 6
BP 873
EP 890
DI 10.30632/PJV59N6-2018a11
UT WOS:000452543500014
ER

PT J
AU Zuranski, AM
   Alvarado, JIM
   Shields, BJ
   Doyle, AG
AF Zuranski, Andrzej M.
   Alvarado, Jesus I. Martinez
   Shields, Benjamin J.
   Doyle, Abigail G.
TI Predicting Reaction Yields via Supervised Learning
SO ACCOUNTS OF CHEMICAL RESEARCH
AB CONSPECTUS: Numerous disciplines, such as image recognition and language translation, have been revolutionized by using machine learning (ML) to leverage big data. In organic synthesis, providing accurate chemical reactivity predictions with supervised ML could assist chemists with reaction prediction, optimization, and mechanistic interrogation.
   To apply supervised ML to chemical reactions, one needs to define the object of prediction (e.g., yield, enantioselectivity, solubility, or a recommendation) and represent reactions with descriptive data. Our group's effort has focused on representing chemical reactions using DFT-derived physical features of the reacting molecules and conditions, which serve as features for building supervised ML models. In this Account, we present a review and perspective on three studies conducted by our group where ML models have been employed to predict reaction yield. First, we focus on a small reaction data set where 16 phosphine ligands were evaluated in a single Ni-catalyzed Suzuki-Miyaura cross-coupling reaction, and the reaction yield was modeled with linear regression.
   In this setting, where the regression complexity is strongly limited by the amount of available data, we emphasize the importance of identifying single features that are directly relevant to reactivity. Next, we focus on models trained on two larger data sets obtained with highthroughput experimentation (HTE). With hundreds to thousands of reactions available, more complex models can be explored, for example, models that algorithmically perform feature selection from a broad set of candidate features. We examine how a variety of ML algorithms model these data sets and how well these models generalize to out-of-sample substrates. Specifically, we compare the ML models that use DFT-based featurization to a baseline model that is obtained with features that carry no physical information, that is, random features, and to a naive non-ML model that averages yields of reactions that share the same conditions and substrate combinations. We find that for only one of the two data sets, DFT-based featurization leads to a significant, although moderate, out-of-sample prediction improvement. The source of this improvement was further isolated to specific features which allowed us to formulate a testable mechanistic hypothesis that was validated experimentally. Finally, we offer remarks on supervised ML model building on HTE data sets focusing on algorithmic improvements in model training.
   Statistical methods in chemistry have a rich history, but only recently has ML gained widespread attention in reaction development. As the untapped potential of ML is explored, novel tools are likely to arise from future research. Our studies suggest that supervised ML can lead to improved predictions of reaction yield over simpler modeling methods and facilitate mechanistic understanding of reaction dynamics. However, further research and development is required to establish ML as an indispensable tool in reactivity modeling.
OI Martinez Alvarado, Jesus/0000-0002-7061-8792; Doyle,
   Abigail/0000-0002-6641-0833; Shields, Benjamin/0000-0002-3825-0211
SN 0001-4842
EI 1520-4898
PD APR 20
PY 2021
VL 54
IS 8
BP 1856
EP 1865
DI 10.1021/acs.accounts.0c00770
EA MAR 2021
UT WOS:000643542200005
PM 33788552
ER

PT J
AU Sharma, RK
   Duda, T
AF Sharma, Rameshwar K.
   Duda, Teresa
TI Membrane guanylate cyclase, a multimodal transduction machine: history,
   present, and future directions
SO FRONTIERS IN MOLECULAR NEUROSCIENCE
AB A sequel to these authors' earlier comprehensive reviews which covered the field of mammalian membrane guanylate cyclase (MGC) from its origin to the year 2010, this article contains 13 sections. The first is historical and covers MGC from the year 1963-1987, summarizing its colorful developmental stages from its passionate pursuit to its consolidation. The second deals with the establishment of its biochemical identity. MGC becomes the transducer of a hormonal signal and founder of the peptide hormone receptor family, and creates the notion that hormone signal transduction is its sole physiological function. The third defines its expansion. The discovery of ROS-GC subfamily is made and it links ROS-GC with the physiology of phototransduction. Sections ROS-GC, a Ca2+-Modulated Two Component Transduction System to Migration Patterns and Translations of the GCAP Signals Into Production of Cyclic GMP are Different cover its biochemistry and physiology. The noteworthy events are that augmented by GCAPs, ROS-GC proves to be a transducer of the free Ca2+ signals generated within neurons; ROS-GC becomes a two-component transduction system and establishes itself as a source of cyclic GMP the second messenger of phototransduction. Section ROS-GC1 Gene Linked Retinal Dystrophies demonstrates how this knowledge begins to be translated into the diagnosis and providing the molecular definition of retinal dystrophies. Section Controlled By Low and High Levels of [Ca2+](i), ROS-GC1 is a Bimodal Transduction Switch discusses a striking property of ROS-GC where it becomes a "[Ca2+](i) bimodal switch" and transcends its signaling role in other neural processes. In this course, discovery of the first CD-GCAP (Ca2+-dependent guanylate cyclase activator), the S100B protein, is made. It extends the role of the ROS-GC transduction system beyond the phototransduction to the signaling processes in the synapse region between photoreceptor and cone ON-bipolar cells; in section Ca2+-Modulated Neurocalcin delta ROS-GC1 Transduction System Exists in the Inner Plexiform Layer (IPL) of the Retinal Neurons, discovery of another CD-GCAP NC delta, is made and its linkage with signaling of the inner plexiform layer neurons is established. Section ROS-GC Linkage With Other Than Vision-Linked Neurons discusses linkage of the ROS-GC transduction system with other sensory transduction processes: Pineal gland, Olfaction and Gustation. In the next, section Evolution of a General Ca2+-Interlocked ROS-GC Signal Transduction Concept in Sensory and Sensory-Linked Neurons, a theoretical concept is proposed where "Ca2+-interlocked ROS-GC signal transduction" machinery becomes a common signaling component of the sensory and sensory-linked neurons. Closure to the review is brought by the conclusion and future directions.
SN 1662-5099
PD JUL 2
PY 2014
VL 7
AR 56
DI 10.3389/fnmol.2014.00056
UT WOS:000348063200001
PM 25071437
ER

PT J
AU Khan, ZU
   Pi, DC
AF Khan, Zaheer Ullah
   Pi, Dechang
TI DeepSSPred: A Deep Learning Based Sulfenylation Site Predictor Via a
   Novel nSegmented Optimize Federated Feature Encoder
SO PROTEIN AND PEPTIDE LETTERS
AB Background: S-sulfenylation (S-sulphenylation, or sulfenic acid) proteins, are special kinds of post-translation modification, which plays an important role in various physiological and pathological processes such as cytokine signaling, transcriptional regulation, and apoptosis. De-spite these aforementioned significances, and by complementing existing wet methods, several computational models have been developed for sulfenylation cysteine sites prediction. However, the performance of these models was not satisfactory due to inefficient feature schemes, severe im-balance issues, and lack of an intelligent learning engine. Objective: In this study, our motivation is to establish a strong and novel computational predictor for discrimination of sulfenylation and non-sulfenylation sites. Methods: In this study, we report an innovative bioinformatics feature encoding tool, named DeepSSPred, in which, resulting encoded features is obtained via nSegmented hybrid feature, and then the resampling technique called synthetic minority oversampling was employed to cope with the severe imbalance issue between SC-sites (minority class) and non-SC sites (majority class). State of the art 2D-Convolutional Neural Network was employed over rigorous 10-fold jackknife cross-validation technique for model validation and authentication. Results: Following the proposed framework, with a strong discrete presentation of feature space, machine learning engine, and unbiased presentation of the underline training data yielded into an excellent model that outperforms with all existing established studies. The proposed approach is 6% higher in terms of MCC from the first best. On an independent dataset, the existing first best study failed to provide sufficient details. The model obtained an increase of 7.5% in accuracy, 1.22% in Sn, 12.91% in Sp and 13.12% in MCC on the training data and12.13% of ACC, 27.25% in Sn, 2.25% in Sp, and 30.37% in MCC on an independent dataset in comparison with 2nd best method. These empirical analyses show the superlative performance of the proposed model over both training and Independent dataset in comparison with existing literature studies Conclusion: In this research, we have developed a novel sequence-based automated predictor for SC-sites, called DeepSSPred. The empirical simulations outcomes with a training dataset and inde-pendent validation dataset have revealed the efficacy of the proposed theoretical model. The good performance of DeepSSPred is due to several reasons, such as novel discriminative feature encod-ing schemes, SMOTE technique, and careful construction of the prediction model through the tuned 2D-CNN classifier. We believe that our research work will provide a potential insight into a further prediction of S-sulfenylation characteristics and functionalities. Thus, we hope that our de-veloped predictor will significantly helpful for large scale discrimination of unknown SC-sites in particular and designing new pharmaceutical drugs in general.
OI Pi, Dechang/0000-0002-6593-4563
SN 0929-8665
EI 1875-5305
PY 2021
VL 28
IS 6
BP 708
EP 721
UT WOS:000691406900011
PM 33267753
ER

PT J
AU Jutzi, T
   Krieghoff-Henning, EI
   Brinker, TJ
AF Jutzi, Tanja
   Krieghoff-Henning, Eva I.
   Brinker, Titus J.
TI The Rise of Artificial Intelligence-High Prediction Accuracy in Early
   Detection of Pigmented Melanoma
SO LARYNGO-RHINO-OTOLOGIE
AB The incidence of malignant melanoma is increasing worldwide. If detected early, melanoma is highly treatable, so early detection is vital.Skin cancer early detection has improved significantly in recent decades, for example by the introduction of screening in 2008 and dermoscopy. Nevertheless, in particular visual detection of early melanomas remains challenging because they show many morphological overlaps with nevi. Hence, there continues to be a high medical need to further develop methods for early skin cancer detection in order to be able to reliably diagnosemelanomas at a very early stage.Routine diagnostics for melanoma detection include visual whole body inspection, often supplemented by dermoscopy, which can significantly increase the diagnostic accuracy of experienced dermatologists. A procedure that is additionally offered in some practices and clinics is wholebody photography combined with digital dermoscopy for the early detection of malignant melanoma, especially for monitoring high-risk patients.In recent decades, numerous noninvasive adjunctive diagnostic techniques were developed for the examination of suspicious pigmented moles, that may have the potential to allow improved and, in some cases, automated evaluation of these lesions. First, confocal laser microscopy should be mentioned here, as well as electrical impedance spectroscopy, multiphoton laser tomography, multispectral analysis, Raman spectroscopy or optical coherence tomography. These diagnostic techniques usually focus on high sensitivity to avoid malignant melanoma being overlooked. However, this usually implies lower specificity, which may lead to unnecessary excision of benign lesions in screening. Also, some of the procedures are time-consuming and costly, which also limits their applicability in skin cancer screening. In the near future, the use of artificial intelligence might change skin cancer diagnostics in many ways. The most promising approach may be the analysis of routine macroscopic and dermoscopic images by artificial intelligence.For the classification of pigmented skin lesions based on macroscopic and dermoscopic images, artificial intelligence, especially in form of neural networks, has achieved comparable diagnostic accuracies to dermatologists under experimental conditions in numerous studies. In particular, it achieved high accuracies in the binary melanoma/nevus classification task, but it also performed comparably well to dermatologists in multiclass differentiation of various skin diseases. However, proof of the basic applicability and utility of such systems in clinical practice is still pending. Prerequisites that remain to be established to enable translation of such diagnostic systems into dermatological routine are means that allow users to comprehend the system's decisions as well as a uniformly high performance of the algorithms on image data from other hospitals and practices.At present, hints are accumulating that computer-aided diagnosis systems could provide their greatest benefit as assistance systems, since studies indicate that a combination of human and machine achieves the best results. Diagnostic systems based on artificial intelligence are capable of detecting morphological characteristics quickly, quantitatively, objectively and reproducibly, and could thus provide a more objective analytical basis - in addition to medical experience.
OI Brinker, Titus Josef/0000-0002-3620-5919
SN 0935-8943
EI 1438-8685
DI 10.1055/a-1949-3639
EA DEC 2022
UT WOS:000914918800002
PM 36580975
ER

PT J
AU Xiao, LX
   Zou, GY
   Cheng, R
   Wang, PP
   Ma, KX
   Cao, HM
   Zhou, WY
   Jin, XY
   Xu, ZC
   Huang, Y
   Lin, XY
   Nie, H
   Jiang, QH
AF Xiao, Lixing
   Zou, Guoying
   Cheng, Rui
   Wang, Pingping
   Ma, Kexin
   Cao, Huimin
   Zhou, Wenyang
   Jin, Xiyun
   Xu, Zhaochun
   Huang, Yan
   Lin, Xiaoyu
   Nie, Huan
   Jiang, Qinghua
TI Alternative splicing associated with cancer stemness in kidney renal
   clear cell carcinoma
SO BMC CANCER
AB BackgroudCancer stemness is associated with metastases in kidney renal clear cell carcinoma (KIRC) and negatively correlates with immune infiltrates. Recent stemness evaluation methods based on the absolute expression have been proposed to reveal the relationship between stemness and cancer. However, we found that existing methods do not perform well in assessing the stemness of KIRC patients, and they overlooked the impact of alternative splicing. Alternative splicing not only progresses during the differentiation of stem cells, but also changes during the acquisition of the stemness features of cancer stem cells. There is an urgent need for a new method to predict KIRC-specific stemness more accurately, so as to provide help in selecting treatment options.MethodsThe corresponding RNA-Seq data were obtained from the The Cancer Genome Atlas (TCGA) data portal. We also downloaded stem cell RNA sequence data from the Progenitor Cell Biology Consortium (PCBC) Synapse Portal. Independent validation sets with large sample size and common clinic pathological characteristics were obtained from the Gene Expression Omnibus (GEO) database. we constructed a KIRC-specific stemness prediction model using an algorithm called one-class logistic regression based on the expression and alternative splicing data to predict stemness indices of KIRC patients, and the model was externally validated. We identify stemness-associated alternative splicing events (SASEs) by analyzing different alternative splicing event between high- and low- stemness groups. Univariate Cox and multivariable logistic regression analysisw as carried out to detect the prognosis-related SASEs respectively. The area under curve (AUC) of receiver operating characteristic (ROC) was performed to evaluate the predictive values of our model.ResultsHere, we constructed a KIRC-specific stemness prediction model with an AUC of 0.968,and to provide a user-friendly interface of our model for KIRC stemness analysis, we have developed KIRC Stemness Calculator and Visualization (KSCV), hosted on the Shiny server, can most easily be accessed via web browser and the url https://jiang-lab.shinyapps.io/kscv/. When applied to 605 KIRC patients, our stemness indices had a higher correlation with the gender, smoking history and metastasis of the patients than the previous stemness indices, and revealed intratumor heterogeneity at the stemness level. We identified 77 novel SASEs by dividing patients into high- and low- stemness groups with significantly different outcome and they had significant correlations with expression of 17 experimentally validated splicing factors. Both univariate and multivariate survival analysis demonstrated that SASEs closely correlated with the overall survival of patients.ConclusionsBasing on the stemness indices, we found that not only immune infiltration but also alternative splicing events showed significant different at the stemness level. More importantly, we highlight the critical role of these differential alternative splicing events in poor prognosis, and we believe in the potential for their further translation into targets for immunotherapy.
OI Jiang, Qinghua/0000-0002-1827-0389
EI 1471-2407
PD JUN 15
PY 2021
VL 21
IS 1
AR 703
DI 10.1186/s12885-021-08470-8
UT WOS:000664811500001
PM 34130646
ER

PT J
AU Boddula, MR
   Adamson, GJ
   Gupta, A
   McGarry, MH
   Lee, TQ
AF Boddula, Madhav R.
   Adamson, Gregory J.
   Gupta, Akash
   McGarry, Michelle H.
   Lee, Thay Q.
TI Restoration of Labral Anatomy and Biomechanics After Superior Labral
   Anterior-Posterior Repair Comparison of Mattress Versus Simple Technique
SO AMERICAN JOURNAL OF SPORTS MEDICINE
AB Background: Both simple and mattress repair techniques have been utilized with success for type II superior labral anterior-posterior (SLAP) lesions; however, direct anatomic and biomechanical comparisons of these techniques have yet to be clearly demonstrated.
   Hypothesis: For type II SLAP lesions, the mattress suture repair technique will result in greater labral height and better position on the glenoid face and exhibit stronger biomechanical characteristics, when cyclically loaded and loaded to failure through the biceps, compared with the simple suture repair technique.
   Study Design: Controlled laboratory study.
   Methods: Six matched pairs of cadaveric shoulders were dissected, and a clock face was created on the glenoid from 9 o'clock (posterior) to 3 o'clock (anterior). For the intact specimen, labral height and labral distance from the glenoid edge were measured using a MicroScribe. A SLAP lesion was then created from 10 o'clock to 2 o'clock. Lesions were repaired with two 3.0-mm BioSuture-Tak anchors placed at 11 o'clock and 1 o'clock. For each pair, a mattress repair was used for one shoulder, and a simple repair was used for the contralateral shoulder. After repair, labral height and labral distance from the glenoid edge were again measured. The specimens were then cyclically loaded and loaded to failure through the biceps using an Instron machine. A paired t test was used for statistical analysis.
   Results: After mattress repair, a significant increase in labral height occurred compared with intact from 2.5 +/- 0.3 mm to 4.3 +/- 0.3 mm at 11 o'clock (P = .013), 2.7 +/- 0.5 mm to 4.2 +/- 0.7 mm at 12:30 o'clock (P = .007), 3.1 +/- 0.5 mm to 4.2 +/- 0.7 mm at 1 o'clock (P = .006), and 2.8 +/- 0.7 mm to 3.7 +/- 0.8 mm at 1:30 o'clock (P = .037). There was no significant difference in labral height between the intact condition and after simple repair at any clock face position. Labral height was significantly increased in the mattress repairs compared with simple repairs at 11 o'clock (mean difference, 2.0 mm; P = .008) and 12:30 o'clock (mean difference, 1.3 mm; P = .044). Labral distance from the glenoid edge was not significantly different between techniques. No difference was observed between the mattress and simple repair techniques for all biomechanical parameters, except the simple technique had a higher load and energy absorbed at 2-mm displacement.
   Conclusion: The mattress technique created a greater labral height while maintaining similar biomechanical characteristics compared with the simple repair, with the exception of load and energy absorbed at 2-mm displacement, which was increased for the simple technique.
   Clinical Relevance: Mattress repair for type II SLAP lesions creates a higher labral bumper compared with simple repairs, while both techniques resulted in similar biomechanical characteristics.
RI Adamson, Gregory/L-2794-2019
SN 0363-5465
PD APR
PY 2012
VL 40
IS 4
BP 875
EP 881
DI 10.1177/0363546511433407
UT WOS:000302285100019
PM 22302203
ER

PT J
AU Kenry
   Liu, B
AF Kenry
   Liu, Bin
TI Enhancing the Theranostic Performance of Organic Photosensitizers with
   Aggregation-Induced Emission
SO ACCOUNTS OF MATERIALS RESEARCH
AB Photodynamic therapy has been actively explored recently to combat various physiological disorders and diseases, including bacterial infections, inflammation, and cancer. Metabolic As a noninvasive modality with high spatiotemporal selectivity, photodynamic therapy tnginaer,no leverages photosensitizers, light, and reactive oxygen species to induce cytotoxicity and cell death. Specifically, upon light irradiation, photosensitizers harvest the incident light energy to generate highly reactive singlet oxygen species through photochemical reactions to disrupt the integrity of certain cellular components of the target cells. The extent to which the target cells can be damaged depends largely on the characteristics of photosensitizers. As such, the selection and design of photosensitizers are essential to ensuring effective and safe photodynamic therapy. Unfortunately, organic photosensitizers typically used in photodynamic therapy tend to suffer from a considerable reduction in singlet oxygen production when these molecules aggregate, significantly limiting the efficacy of photodynamic therapy. To address this issue, a different class of organic photosensitizers with aggregation-induced emission (ME) characteristics, which exhibit bright fluorescence and enhanced photosensitizing activity only when they exist in an aggregated state, has been increasingly formulated for disease theranostic applications.
   In general, AIE photosensitizers can be designed on the basis of several major strategies. For example, AIE photosensitizers with efficient singlet oxygen generation can be formulated by minimizing their singlet-triplet energy gap via tuning the distribution of the highest occupied molecular orbital and the lowest unoccupied molecular orbital of the molecules. Simultaneously, through donor- acceptor engineering, AIE photosensitizers can be designed to have increased molar absorptivity, red-shifted absorption and emission wavelengths, and improved body clearance. In situ dick synthesis can also be adopted to formulate ME photosensitizers with suppressed dark toxicity. These design approaches can be optimized using artificial intelligence or machine learning, leading to higher throughput discovery of AIE photosensitizers with exceptional performance. Intriguingly, the therapeutic impact of AIE photosensitizers can be further strengthened by modulating their performance-related features, notably targeting specificity, target accumulation and retention, tissue penetration depth, stimulus responsivity, and theranostic modality. By precisely controlling these elements, multifunctional and biocompatible ME photosensitizers with superior performance can be realized.
   Herein, we describe our recent efforts in designing and formulating organic AIE photosensitizers with improved theranostic efficacy and safety to treat bacterial infections and cancer. We first introduce different principles that can be adopted to guide the design of AIE photosensitizers. We then present various ways to strengthen the different performance-associated features of AIE photosensitizers. These include enhancement of the targeting specificity, target accumulation, and retention of AIE photosensitizers through metabolic engineering, enhancement of the tissue penetration depth of AIE photosensitizers through chemiexcitation and ionizing irradiation, enhancement of AIE photosensitization by suppressing intrinsic oxidative resistance, enhancement of the responsivity of AIE photosensitizers through stimulus-responsive building blocks, and enhancement of the overall theranostic performance of AIE photosensitizers through combinatorial therapy. Finally, we identify current challenges, potential opportunities, and future research directions for this emerging field. Through this Account, we seek to stimulate further interest and active collaborations in the development, theranostic applications, and clinical translation of organic AIE photosensitizers to treat different diseases.
RI , Kenry/P-9488-2019
OI , Kenry/0000-0003-0405-1369
EI 2643-6728
DI 10.1021/accountsmr.2c00039
EA MAY 2022
UT WOS:000820840500001
ER

PT J
AU Liu, PY
   Lai, PL
   Lin, CL
AF Liu, Po-Yi
   Lai, Po-Liang
   Lin, Chun-Li
TI A biomechanical investigation of different screw head designs for
   vertebral derotation in scoliosis surgery
SO SPINE JOURNAL
AB BACKGROUND CONTEXT: The posterior pedicle screw-rod system, which is widely used to correct spinal deformities, achieves a good correction rate in the frontal and coronal planes but not in the axial plane. Direct vertebral derotation (DVD) was developed to correct axial plane deformities. However, the design of screw head and body connection, in terms of monoaxial, polyaxial, and uniplanar screw, may influence the efficiency of DVD.
   PURPOSE: This study compared the efficiency of a newly designed uniplanar screw with that of monoaxial and polyaxial screws in the DVD maneuver.
   STUDY DESIGN: A porcine spine model and monoaxial, polyaxial, and uniplanar screws were used to examine the biomechanics of the DVD maneuver.
   METHODS: Six T7-T13 porcine thoracic spine segments were used as test specimens in this study. Pedicle screws were inserted in the left pedicles of the T9-T11 spinal segments and then connected with a rod. Three types of pedicle screws with different screw head designs (monoaxial, polyaxial, and uniplanar) were employed in this study. The material testing system (MTS) machine generated a rotational moment through the derotational tube on the T10 (apical body) pedicle screw, which simulated the motion applied during the surgical vertebral derotational procedure. The pedicle strain and the kinematics of the vertebral body and derotational tube were recorded to evaluate the derotational efficiency of different pedicle screw head designs.
   RESULTS: The variances of the derotation for the monoaxial, polyaxial, and uniplanar screws were 2.22 degrees +/- 1.43 degrees, 32.23 degrees +/- 2.26 degrees, and 4.75 degrees +/- 1.60 degrees, respectively; the derotation efficiency was 0.65, 0.51, and 0.12, respectively, when the torques of the spinal constructs reached 3 Nm. The rotational variance of the polyaxial screw was statistically greater than that of the monoaxial and uniplanar screws (p<.05). The maximum micro-strains of the pedicles for the monoaxial, polyaxial, and uniplanar screws were 1,067.45 +/- 550.35, 747.68 +/- 393.56, and 663.55 +/- 271.04, respectively, with no statistically significant differences (p>.05).
   CONCLUSIONS: The screw head design played an important role in the efficiency and variance of the derotation during the DVD maneuver. The derotational efficiency of the newly designed uniplanar screw was closer to that of the monoaxial screw group than to that of the polyaxial screw group. The polyaxial screw was inferior to DVD owing to a derotational variance between the derotational tube and the apical body that was correlated with the range of motion of the screw head. In the present study, the pedicle strain was similar in all groups. However, the pedicle strain of the uniplanar screw group was lower than that of the monoaxial screw group and was similar to that of the polyaxial screw group when the angle of rotation of the apical body increased. (C) 2017 Elsevier Inc. All rights reserved.
RI Mattivi, Fulvio/B-7645-2011; Grando, Maria Stella/D-5448-2011; Nicolini,
   Giorgio/C-3748-2008
OI Mattivi, Fulvio/0000-0003-4935-5876; Grando, Maria
   Stella/0000-0002-6889-1968; Nicolini, Giorgio/0000-0003-3225-8551; Lin,
   Chun-Li/0000-0002-5958-0949
SN 1529-9430
EI 1878-1632
PD AUG
PY 2017
VL 17
IS 8
BP 1171
EP 1179
DI 10.1016/j.spinee.2017.04.010
UT WOS:000406917200016
PM 28414169
ER

PT J
AU Zhu, LX
   Wang, QX
   Yang, C
AF Zhu, Lixin
   Wang, Qinxi
   Yang, Chen
TI Serine/Arginine Repetitive Matrix 2 Antisense RNA 1 Negatively Regulates
   miR-370-3p and Promotes Hyperplasia, Migration, and Aggression of the
   Colon Cancer Cell Line
SO JOURNAL OF BIOMATERIALS AND TISSUE ENGINEERING
AB The purpose of this study is to explore the effect and possible machine-processing of the long non-coding RNA (lncRNA) SRRM2-AS1 in the development and pathogenesis of colorectal cancer. lncRNA plays an important role in tumorigenesis and development. LncRNA can regulate gene transcription and translation, cell proliferation, differentiation and apoptosis by affecting gene expression pathways of various coding proteins. SRRM2-AS1 is a kind of lncRNA. Studies have confirmed that the expression of SRRM2-AS1 is increased in colon adenocarcinoma tissues of colon cancer patients and is closely related to the prognosis of patients. However, the influence and molecular mechanism of SRRM2-AS1 on the malignant biological behavior of colon cancer cells are not yet clear. SRRM2-AS1 may interact with miR-370-3p. Studies have confirmed that overexpression of miR-370-3p can inhibit the proliferation and epithelial-mesenchymal transition of colon cancer cells in vitro. However, it is not yet clear whether SRRM2-AS1 can target miR-370-3p to affect the occurrence and development of tumors. In this study, RT-qPCR was employed to detect levels of SRRM2-AS1 and miRNA-370-3p in carcinoma tissues and corresponding paracarcinoma tissues from 41 patients with colon cancer. SW1116 colon cancer cells were cultured in vitro and separated into 4 groups: (1) si-NC group, (2) si-SRRM2-AS1 group, (3) si-SRRM2-AS1 +anti-miRNA-NC group, and (4) si-SRRM2-AS1 +anti-miRNA-370-3p group. The CCK-8 assay and colony formation experiment was employed to gauge cell proliferation. The scratch test was used to detect cell migration while the transwell assay was used to detect cell invasion. Finally, Western blot analysis was employed to detect levels of Ki67, E-cadherin, and N-cadherin proteins in colorectal cancer cells. The dual-luciferase reporter gene experiment verified that SRRM2-AS1 regulates miRNA-370-3p. The study found that compared to paracarcinoma tissue, levels of SRRM2-AS1 in colon cancer tissues was increased (P < 0.05). Compared to the si-NC group, the SW1116 cell OD value, number of colonies formed, scratch healing rate, number of invasive cells, and expression levels of Ki67 and N-cadherin protein in the si-SRRM2-AS1 group were all decreased (P < 0.05). However, E-cadherin protein levels were elevated (P < 0.05). SRRM2-AS1 negatively regulates levels of miRNA-370-3p in SW1116 cells. Compared to the si-SRRM2-AS1 + anti-miRNA-NC group, SW1116 cell OD value, number of colonies formed, scratch healing rate, number of invasive cells, and Ki67 and N-cadherin protein levels were increased (P < 0.05) in the si-SRRM2-AS1 + anti-miRNA-370-3p group. Conversely, E-cadherin protein levels were decreased (P < 0.05). These findings indicate that SRRM2-AS1 is predominately expressed in cancerous colon tissues. Attenuating expression of SRRM2-AS1 may curb the hyperplasia of colon carcinoma cell line SW1116 and promote cell apoptosis by regulating miRNA-370-3p expression.
SN 2157-9083
EI 2157-9091
PD JUN
PY 2021
VL 11
IS 6
BP 1059
EP 1065
DI 10.1166/jbt.2021.2507
UT WOS:000663632500005
ER

PT J
AU Ciszek, R
   Ndode-Ekane, XE
   Gomez, CS
   Casillas-Espinosa, PM
   Ali, I
   Smith, G
   Puhakka, N
   Lapinlampi, N
   Andrade, P
   Kamnaksh, A
   Immonen, R
   Paananen, T
   Hudson, MR
   Brady, RD
   Shultz, SR
   O'Brien, TJ
   Staba, RJ
   Tohka, J
   Pitkanen, A
AF Ciszek, Robert
   Ndode-Ekane, Xavier Ekolle
   Gomez, Cesar Santana
   Casillas-Espinosa, Pablo M.
   Ali, Idrish
   Smith, Gregory
   Puhakka, Noora
   Lapinlampi, Niina
   Andrade, Pedro
   Kamnaksh, Alaa
   Immonen, Riikka
   Paananen, Tomi
   Hudson, Matthew R.
   Brady, Rhys D.
   Shultz, Sandy R.
   O'Brien, Terence J.
   Staba, Richard J.
   Tohka, Jussi
   Pitkanen, Asla
TI Informatics tools to assess the success of procedural harmonization in
   preclinical multicenter biomarker discovery study on post-traumatic
   epileptogenesis
SO EPILEPSY RESEARCH
AB The Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx) is a National Institutes for Neurological Diseases and Stoke funded Centers-Without-Walls international multidisciplinary study aimed at preventing epileptogenesis. The preclinical biomarker discovery in EpiBios4Rx applies a multicenter study design to allow the number of animals that are required for adequate statistical power for the analysis to be studied in an efficient manner. Further, the use of multiple centers mimics the clinical trial situation, and therefore potentially the chance of successful clinical translation of the outcomes of the study. Its successful implementation requires harmonization of procedures and data analyses between the three contributing centers in Finland, Australia, and USA. The objective of the present analysis was to develop metrics for analysis of the success of harmonization of procedures to guide further data analyses and plan the future multicenter preclinical studies. The interim analysis of data is based on the analysis of data from 212 rats with lateral fluid-percussion injury or sham-operation included in the biomarker discovery by April 30, 2018. The details of protocols, including production of injury, post-injury follow-up, blood sampling, electroencephalogram recording, and magnetic resonance imaging have been presented in the accompanying manuscripts in this Supplement. Implementation of protocols in EpiBios4Rx project participant centers was visualized in 2D using t-distributed stochastic neighborhood embedding (t-SNE). The protocols applied to each rat were presented as feature vectors of procedure related variables (e.g., impact pressure, anesthesia time). The total number of protocol features linked to each rat was 112. The missing data was accounted in visualization by utilizing imputation and adding the number of missing values as a third dimension to 2D t-SNE plot, resulting in a 3D overview of protocol data. Intraclass correlation coefficient (ICC) using Euclidean distances and area under receiver operating characteristic curve (AUC) of k-nearest neighbor classifier (KNN) were utilized to quantify the degree of clustering by center. Both subsets of data with incomplete protocol vectors omitted and missing protocol data imputed were assessed. Our data show that a visible clustering by center was observed in all t-SNE plots, except for day 7 neuroscores. Both ICC and AUC indicated clustering by center in all protocol variable subsets, excluding unimputed day 7 neuroscores (ICC 0.04 and AUC 0.6). ICC for imputed set of all protocol related variables was 0.1 and KNN AUC 0.92. In conclusion, both ICC and AUC indicated differences in protocol between EpiBios4Rx participating centers, which needs to be taken into account in data analysis. Importantly, the majority of observed differences are recoverable as they relate to insufficient updates in record keeping. While AUC score of KNN is a more sensitive measure for protocol harmonization than ICC for data that displays complex splintered clustering, ICC and AUC provide complementary measures to assess the degree of procedural harmonization. This experience should be helpful for other groups planning such multicenter post-traumatic epileptogenesis studies in the future.
RI Andrade, Pedro/N-8410-2018; Tohka, Jussi/D-2385-2013; Andrade,
   Pedro/AAE-8150-2022; Staba, Richard/AAN-3149-2021; Santana-Gomez,
   Cesar/AAY-1014-2021; O'Brien, Terence/L-8102-2013; O'Brien,
   Terence/AAU-5525-2021; Hudson, Matthew/AAH-9168-2020; Ali,
   Idrish/P-4989-2018
OI Andrade, Pedro/0000-0001-9118-8341; Tohka, Jussi/0000-0002-1048-5860;
   Andrade, Pedro/0000-0001-9118-8341; Santana-Gomez,
   Cesar/0000-0001-9163-5615; O'Brien, Terence/0000-0002-7198-8621; Ali,
   Idrish/0000-0002-0971-4654; Lapinlampi, Niina/0000-0003-3210-6312;
   Hudson, Matthew/0000-0001-5016-735X; Ndode-Ekane, Xavier
   Ekolle/0000-0001-5682-5632
SN 0920-1211
EI 1872-6844
PD FEB
PY 2019
VL 150
BP 17
EP 26
DI 10.1016/j.eplepsyres.2018.12.010
UT WOS:000461404200003
PM 30605864
ER

PT J
AU Potter, R
   Van Limbergen, E
   Gerstner, N
   Wambersie, A
AF Potter, R
   Van Limbergen, E
   Gerstner, N
   Wambersie, A
TI Survey of the use of the ICRU 38 in recording and reporting cervical
   cancer brachytherapy
SO RADIOTHERAPY AND ONCOLOGY
AB Background: A survey on the practice of reporting intracavitary cervix cancer brachytherapy (LDR and HDR) in clinical practice (CP) and in literature (LIT) was performed on the occasion of a workshop, 'ICRU 38: The Basis for a Revision', which took place at the Annual GEC ESTRO meeting in Naples in 1998.
   Materials and methods: The answers (n = 85) to a specific questionnaire which had been sent to ail ESTRO members (n = 1600), were evaluated. Tn parallel, a systematic survey on the literature reporting cervix cancer brachytherapy since 1985 was performed using the MEDLINE database. The main recommendations for reporting as given in the ICRU 38 were addressed for both surveys: technique; total reference air kerma (TRAK); dose specification to the target volume '60 Gy reference volume', to organs at risk 'ICRU rectum and bladder point' and other reference points and time-dose pattern. In addition: some other items were investigated such as mg h, Point A, B, and in vivo dosimetry in bladder and rectum.
   Results: Issues related to technique (source, machine and applicator type) and to time-dose pattern are reported in the majority of patients in CP and LIT. The same applies for the following parameters: Point A is indicated in 76% (LDR) to 89% (HDR) in CP, in 60% (LDR) to 96% (HDR) in LIT. Rectum and bladder ICRU points are recorded in 55% (HDR) to 90% (LDR) and 58% (HDR) to 84% (LDR), respectively, in CP. On the other hand, TRAK is given in 14% (HDR) to 43% (LDR) in CP, in 0% (HDR) to 10% (LDR) in LIT. '60 Gy reference volume' is recorded in 18% (HDR) to 51% (LDR) in CP, in 0% (HDR) to 17% (LDR) in LIT. Rectum and bladder ICRU points are reported in 18% (LDR) to 28% (HDR) and 14% (HDR) to 29% (LDR), respectively, in LIT. Other reference points and in vivo dosimetry measurements are given in a low percentage. Dose rate and overall treatment time is reported in 100-44%.
   Conclusion: Recording and reporting in CP and in LIT meets the recommendations as given in ICRU 38 to different degrees. Specific items such as TRAK and the 'Reference volume' have only limited penetration into CP and LIT, which applies in particular to centers using HDR brachytherapy. The discrepancies between CP and LIT may be due to the well-known delay between change in CP and its translation into LIT. In order to arrive at a more common language for the better exchange of clinical results, it seems to be necessary to adapt some terms and recommendations. In particular, comprehensive concepts are needed for reporting dose to points and volumes in the target and in critical organs, according to the new potential from imaging and computer technology and from modern radiobiological insights, bridging the gap between LDR and HDR brachytherapy. (C) 2001 Elsevier Science Ireland Ltd. All rights reserved.
OI Potter, Richard/0000-0002-0703-572X
SN 0167-8140
PD JAN
PY 2001
VL 58
IS 1
BP 11
EP 18
DI 10.1016/S0167-8140(00)00266-8
UT WOS:000166966100004
PM 11165676
ER

PT J
AU Badaro, G
   Hajj, H
   Habash, N
AF Badaro, Gilbert
   Hajj, Hazem
   Habash, Nizar
TI A Link Prediction Approach for Accurately Mapping a Large-scale Arabic
   Lexical Resource to English WordNet
SO ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION
   PROCESSING
AB Success of Natural Language Processing (NLP) models, just like all advanced machine learning models, rely heavily on large -scale lexical resources. For English, English WordNet (EWN) is a leading example of a large-scale resource that has enabled advances in Natural Language Understanding (NLU) tasks such as word sense disambiguation, question answering, sentiment analysis, and emotion recognition. EWN includes sets of cognitive synonyms called synsets, which are interlinked by means of conceptual-semantic and lexical relations and where each synset expresses a distinct concept. However, other languages are still lagging behind in having large-scale and rich lexical resources similar to EWN. In this article, we focus on enabling the development of such resources for Arabic. While there have been efforts in developing an Arabic WordNet (AWN), the current version of AWN has its limitations in size and in lacking transliteration standards, which are important for compatibility with Arabic NLP tools. Previous efforts for extending AWN resulted in a lexicon, called ArSenL, that overcame the size and the transliteration standard limitation but was limited in accuracy due to the heuristic approach that only considered surface matching between the English definitions from the Standard Arabic Morphological Analyzer (SAMA) and EWN synset terms, and that resulted in inaccurate mapping of Arabic lemmas to EWN's synsets. Furthermore, there has been limited exploration of other expansion methods due to expensive manual validation needed. To address these limitations of simultaneously having large-scale size with high accuracy and standard representations, the mapping problem is formulated as a link prediction problem between a large-scale Arabic lexicon and EWN, where a word in one lexicon is linked to a word in another lexicon if the two words are semantically related. We use a semi-supervised approach to create a training dataset by finding common terms in the large-scale Arabic resource and AWN. This set of data becomes implicitly linked to EWN and can be used for training and evaluating prediction models. We propose the use of a two-step Boosting method, where the first step aims at linking English translations of SAMA's terms to EWN's synsets. The second step uses surface similarity between SAMA's glosses and EWN's synsets. The method results in a new large-scale Arabic lexicon that we call ArSenL 2.0 as a sequel to the previously developed sentiment lexicon ArSenL. A comprehensive study covering both intrinsic and extrinsic evaluations shows the superiority of the method compared to several baseline and state-of-the-art link prediction methods. Compared to previously developed ArSenL, ArSenL 2.0 included a larger set of sentimentally charged adjectives and verbs. It also showed higher linking accuracy on the ground truth data compared to previous ArSenL. For extrinsic evaluation, ArSenL 2.0 was used for sentiment analysis and showed, here, too, higher accuracy compared to previous ArSenL.
OI Habash, Nizar/0000-0002-1831-3457
SN 2375-4699
EI 2375-4702
PD NOV
PY 2020
VL 19
IS 6
AR 80
DI 10.1145/3404854
UT WOS:000595547800006
ER

PT J
AU ZIENKIEWICZ, OC
AF ZIENKIEWICZ, OC
TI COMPUTATIONAL MECHANICS TODAY
SO INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING
CT 2ND WORLD CONGRESS OF THE INTERNATIONAL ASSOC OF COMPUTATIONAL MECHANICS
CY AUG, 1990
CL STUTTGART, GERMANY
SP INT ASSOC COMP MECH
AB Computational Mechanics or Computational Applied Science is today the base on which most of the achievements of engineering and physics are built.  Its concern is the solution of complex mathematical theories in numerical terms, without which the translation of these into practical artifacts would be impossible.  Indeed, by providing such quantitative measures it enhances the understanding of the physical phenomena and stimulates further development of theory and physical experiment.
   Most of the theory underlying physical phenomena is cast in terms of, often involved, differential equations for which closed forms of solution are seldom possible.  Numerical approximation or discretization processes are necessary for quantitative solution.  Here the first steps were taken at the start of this century by the pioneering work of Richardson introducing finite difference approximations.  The invention of relaxation methods by Southwell during the Second World War allowed many practical solutions to be achieved.  However, it was the advent of the electronic digital computer that marked the turning point in Computational Mechanics.  The dramatic escalation of the power of these machines, which still continues today, allowed the development of the field of Computational Mechanics as we know it.
   It is through this computer power that such methods of approximations as finite elements, finite differences, boundary solutions and spectral processes became a practical reality, though each was anticipated in the pre computer area.  It is not surprising therefore, that the mathematical foundations and the full development of such methods have been accomplished only relatively recently.
   Today we see the field of activity subdivided between those specializing in the development of the various computational approximation processes and those seeking optimal numerical solutions for their particular field of application.  It is the objective of this Congress and indeed of the International Association of Computational Mechanics to provide a forum at which an interdisciplinary exchange of information can take place between the various sections and disciplines of the whole field.  Indeed, this is the way progress can best be achieved.  Recent history indicates that substantial advances are as frequently made due to a method seeking a new application as through a problem requiring a solution.
   In recent history we have seen on occasion a liaison of a particular computational approximation method to a field of application occurring through historical accident.  Here the intimate association of the finite method and the field of SOLID MECHANICS (CSM or Computational Solid Mechanics) and that of the finite differences with FLUID DYNAMICS (CDF or Computational Fluid Dynamics) can be observed as classical examples.  Today the advent of new application fields and a better understanding of the approximation theory are helping to break down the barriers and ensure a more rational matching of objectives and methods.  We shall illustrate the lecture with examples of such recent progress and state some possibilities as yet unexplored.  Indeed, we are sure that the Congress will achieve in much more detail the same aims.
   This presentation stresses the essential unity of the subject and discuss some areas where progress and research are currently active.  Two of such, adaptive error controlled analysis and treatment of hyperbolic (fluid) problems, are singled out due to their wide applications.
SN 0029-5981
PD MAR 15
PY 1992
VL 34
IS 1
BP 9
EP 33
DI 10.1002/nme.1620340104
UT WOS:A1992HG41500003
ER

PT J
AU Wu, DD
   Yuan, YH
   Liu, PM
   Wu, Y
   Gao, MY
AF Wu, Dandan
   Yuan, Yihui
   Liu, Pengming
   Wu, Yan
   Gao, Meiying
TI Cellular responses in Bacillus thuringiensis CS33 during bacteriophage
   BtCS33 infection
SO JOURNAL OF PROTEOMICS
AB Bacillus thuringiensis (Bt) has been widely used for 50 years as a biopesticide for controlling insect pests. However, bacteriophage infection can cause failures in 50%-80% of the batches during Bt fermentation, resulting in severe losses. In the present work, the physiological and biochemical impacts of Bt strain CS33 have been studied during bacteriophage infection. This study adopted a gel-based proteomics approach to probe the sequential changed proteins in phage-infected Bt cells. To phage, it depressed the host energy metabolism by suppressing the respiration chain, the TCA cycle, and the utilization of PHB on one hand; on the other hand, it hijacked the host translational machine for its own macromolecular synthesis. To host, superinfection exclusion might be triggered by the changes of S-layer protein and flagella related proteins, which were located on the cell surface and might play as the candidates for the phage recognition. More importantly, the growth rate, cell mass, and ICPs yield were significantly decreased. The low yield of ICPs was mainly due to the suppressed utilization of PHB granules. Further functional study on these altered proteins may lead to a better understanding of the pathogenic mechanisms and the identification of new targets for phage control.
   Biological significance B. thuringiensis (Bt) has been widely used for 50 years as a safe biopesticide for controlling agricultural and sanitary insect pests. However, bacteriophage infection can cause severe losses during B. thuringiensis fermentation. The processes and consequences of interactions between bacteriophage and Bt were still poorly understood, and the molecular mechanisms involved were more unknown. This study adopted a gel-based proteomics approach to probe the physiological and biochemical impacts of Bt strain CS33 after phage-infection. The interactions between phage BtCS33 and its host Bt strain CS33 occurred mainly on four aspects. First, phage synthesized its nucleic acids through metabolic regulation by increasing the amount of NDK. Second, it is reasonable to infer that a phage resistance or superinfection exclusion was triggered by several increased or decreased proteins (SLP, FED, FlaB), which were located on the cell surface and might play as candidates for the phage recognition. Third, combining the decreased flavoproteins (SdhA and EtfB) and the down regulated Fe-S cluster biosynthesis pathway together, it can be suggested that the respiration chain was weakened after phage infection. Additionally, three key enzymes (AcnB, FumC and AdhA) involved in the TCA cycle were all decreased, indicating the TCA cycle was seriously inhibited after infection. Fourth, the growth rate, cell mass and ICPs yield of the host were significantly decreased. To the best of our knowledge, this work represents the first systematic study on the interactions of an insecticidal bacterium with its phage, and has contributed novel information to understand the molecular events in the important biological pesticide producer, B. thuringiensis, in response to phage challenge. (C) 2014 Elsevier B.V. All rights reserved.
SN 1874-3919
EI 1876-7737
PD APR 14
PY 2014
VL 101
BP 192
EP 204
DI 10.1016/j.jprot.2014.02.016
UT WOS:000335104500015
PM 24565692
ER

PT J
AU Ouweltjes, W
   Spoelstra, M
   Ducro, B
   de Haas, Y
   Kamphuis, C
AF Ouweltjes, Wijbrand
   Spoelstra, Mirjam
   Ducro, Bart
   de Haas, Yvette
   Kamphuis, Claudia
TI A data-driven prediction of lifetime resilience of dairy cows using
   commercial sensor data collected during first lactation
SO JOURNAL OF DAIRY SCIENCE
AB Reliable prediction of lifetime resilience early in life can contribute to improved management decisions of dairy farmers. Several studies have shown that time series sensor data can be used to predict lifetime resilience rankings. However, such predictions generally require the translation of sensor data into biologically meaningful sensor features, which involve proper feature definitions and a lot of preprocessing. The objective of this study was to investigate the hypothesis that data driven random forest algorithms can equal or improve the prediction of lifetime resilience scores compared with ordinal logistic regression, and that these algorithms require considerably less effort for data preprocessing. We studied this by developing prediction models that forecast lifetime resilience of a cow early in her productive life using sensor data from the first lactation. We used an existing data set from a Dutch experimental herd, with data of culled cows for which birth dates, insemination dates, calving dates, culling dates, and health treatments were available to calculate lifetime resilience scores. Moreover, 4 types of first-lactation sensor data, converted to daily aggregated values, were available: milk yield, body weight, activity, and rumination. For each sensor, 14 sensor features were calculated, of which part were based on absolute daily values and part on relative to herd average values. First, we predicted lifetime resilience rank with stepwise logistic regression using sensor features as predictors and a P-value of <0.2 as the cut-off. Next, we applied a random forest with the 6 features that remained in the final logistic regression model. We then applied a random forest with all sensor features, and finally applied a random forest with daily aggregated values as features. All models were validated with stratified 10-fold cross-validation with 90% of the records in the training set and 10% in the validation set. Model performances expressed in percentage of correctly classified cows (accuracy) and percentage of cows being critically misclassified (i.e., high as low and vice versa) +/- standard deviation were 45.1 +/- 8.1% and 10.8% with the ordinal logistic regression model, 45.7 +/- 8.4% and 16.0% with the random forest using the same 6 features as the logistic regression model, 48.4 +/- 6.7% and 10.0% for the random forest with all sensor features, and 50.5 +/- 6.3% and 8.4% for the random forest with daily sensor values. This random forest also revealed that data collected in early and late stages of first lactation seem to be of particular importance in the prediction compared with that in mid lactation. Accuracies of the models were not significantly different, but the percentage of critically misclassified cows was significantly higher for the second model than for the other models. We concluded that a data-driven random forest algorithm with daily aggregated sensor data as input can be used for the prediction of lifetime resilience classification with an overall accuracy of similar to 50%, and provides at least as good prediction as models with sensor features as input.
RI Ouweltjes, Wijbrand/HSE-6778-2023
OI Spoelstra, Mirjam/0000-0002-4993-2242; Ouweltjes,
   Wijbrand/0000-0001-5455-0110
SN 0022-0302
EI 1525-3198
PD NOV
PY 2021
VL 104
IS 11
BP 11759
EP 11769
DI 10.3168/jds.2021-20413
EA OCT 2021
UT WOS:000718964600004
PM 34454764
ER

PT J
AU KOSTUIK, JP
   MUNTING, E
   VALDEVIT, A
AF KOSTUIK, JP
   MUNTING, E
   VALDEVIT, A
TI BIOMECHANICAL ANALYSIS OF SCREW LOAD SHARING IN PEDICLE FIXATION OF THE
   LUMBAR SPINE
SO JOURNAL OF SPINAL DISORDERS
AB Segmental fixation of the spine by means of rods or plates and pedicle screws posteriorly usually results in a stable and rigid construct. The magnitude of the stresses on the instrumentation particularly at the bone-screw and rod (or plate) interfaces will depend on the load distribution between the bone and the implant as well as the number of sites of bone purchase of the implant. If a fusion is to be obtained in the case of a degenerative spine, the role of the instrumentation is to prevent translation of the motion segments, thereby allowing the compressive loads to be transmitted through the vertebral bodies and the degenerated discs. In the case of a fracture, the instrumentation is made to withstand the bulk of the loads since the structural integrity of a motion segment may have been lost. This study was undertaken to evaluate the effect of different constructs on the stresses in Cotrel-Duboussett (C-D) pedicle (tulip) screws close to their junction with the rod bridging the motion segments to be immobilized. In order to minimize the effect of anatomical and material property variation between spines, adult porcine spines were used, thus providing a reproducible experimental model. Fresh 3-year-old porcine spines were potted in holders after the soft tissues were removed (with the exception of the ligamentous structures). The potted spine was then fixed into the loading frame of a materials testing machine (MTS 858 Bionix Test System) and cycled 500 times with an axial load of 380 N. Following this pre-cycling, 6.5-mm diameter C-D tulip screws mounted with strain gauges (Micro-Measurements CEA-06-125UN350) were inserted and aligned. The constructs tested were an intact spine with an eight-screw assembly, a spine with an L4 corpectomy with an eight-screw assembly and a spine with an L4 corpectomy with a four-screw assembly. Each of these constructs was loaded with and without the presence of cross-linking devices. For each construct, five spines were tested. A computerized data acquisition system converted the strain gauge data to a bending moment for each screw. Statistical tests at the 95% level of significance show that the presence of cross links did not affect the mean bending moments within a given construct. Although the bending moments were greater in the distal screws than in the proximal screws in the four-screw corpectomy model, the overall bending moments of the corpectomy models were greater than those encountered in the intact models by a factor of 3. In the four-screw corpectomy model, the bending moments on distal screws were larger than on the proximal screws. The proximal screws accounted for approximately 12% of the total bending moment, whereas the corresponding fractions for the distal screws was approximately 75%. In the corpectomy models, the loads of the intermediate screws were distributed proportionate to the number of distal screws; thus failure is less likely to occur in vivo with four distal screws than with two screws since the proportionate load increased from 60 to 190 Nm, respectively.
SN 0895-0385
PD OCT
PY 1994
VL 7
IS 5
BP 394
EP 401
UT WOS:A1994PK36800005
PM 7819639
ER

PT J
AU Koller, H
   Resch, H
   Tauber, M
   Zenner, J
   Augat, P
   Penzkofer, R
   Acosta, F
   Kolb, K
   Kathrein, A
   Hitzl, W
AF Koller, Heiko
   Resch, Herbert
   Tauber, Mark
   Zenner, Juliane
   Augat, Peter
   Penzkofer, Rainer
   Acosta, Frank
   Kolb, Klaus
   Kathrein, Anton
   Hitzl, Wolfgang
TI A biomechanical rationale for C1-ring osteosynthesis as treatment for
   displaced Jefferson burst fractures with incompetency of the transverse
   atlantal ligament
SO EUROPEAN SPINE JOURNAL
AB Nonsurgical treatment of Jefferson burst fractures (JBF) confers increased rates of C1-2 malunion with potential for cranial settling and neurologic sequels. Hence, fusion C1-2 was recognized as the superior treatment for displaced JBF, but sacrifies C1-2 motion. Ruf et al. introduced the C1-ring osteosynthesis (C1-RO). First results were favorable, but C1-RO was not without criticism due to the lack of clinical and biomechanical data serving evidence that C1-RO is safe in displaced JBF with proven rupture of the transverse atlantal ligament (TAL). Therefore, our objectives were to perform a biomechanical analysis of C1-RO for the treatment of displaced Jefferson burst fractures (JBF) with incompetency of the TAL. Five specimens C0-2 were subjected to loading with posteroanterior force transmission in an electromechanical testing machine (ETM). With the TAL left intact, loads were applied posteriorly via the C1-RO ramping from 10 to 100 N. Atlantoaxial subluxation was measured radiographically in terms of the anterior antlantodental interval (AADI) with an image intensifier placed surrounding the ETM. Load-displacement data were also recorded by the ETM. After testing the TAL-intact state, the atlas was osteotomized yielding for a JBF, the TAL and left lateral joint capsule were cut and the C1-RO was accomplished. The C1-RO was subjected to cyclic loading, ramping from 20 to 100 N to simulate post-surgery in vivo loading. Afterwards incremental loading (10-100 N) was repeated with subsequent increase in loads until failure occurred. Small differences (1-1.5 mm) existed between the radiographic AADI under incremental loading (10-100 N) with the TAL-intact as compared to the TAL-disrupted state. Significant differences existed for the beginning of loading (10 N, P = 0.02). Under physiological loads, the increase in the AADI within the incremental steps (10-100 N) was not significantly different between TAL-disrupted and TAL-intact state. Analysis of failure load (FL) testing showed no significant differences among the radiologically assessed displacement data (AADI) and that of the ETM (P = 0.5). FL was 297.5 +/- A 108.5 N (range 158.8-449.0 N). The related displacement assessed by the ETM was 5.8 +/- A 2.8 mm (range 2.3-7.9). All specimens succeeded a FL > 150 N, four of them > 250 N and three of them > 300 N. In the TAL-disrupted state loads up to 100 N were transferred to C1, but the radiographic AADI did not exceed 5 mm in any specimen. In conclusion, reconstruction after displaced JBF with TAL and one capsule disrupted using a C1-RO involves imparting an axial tensile force to lift C0 into proper alignment to the C1-2 complex. Simultaneous compressive forces on the C1-lateral masses and occipital condyles allow for the recreation of the functional C0-2 ligamentous tension band and height. We demonstrated that under physiological loads, the C1-RO restores sufficient stability at C1-2 preventing significant translation. C1-RO might be a valid alternative for the treatment of displaced JBF in comparison to fusion of C1-2.
RI Augat, Peter/A-2051-2011; Hitzl, Wolfgang/A-1068-2013
OI Augat, Peter/0000-0003-4805-2128; Hitzl, Wolfgang/0000-0002-7696-1479
SN 0940-6719
EI 1432-0932
PD AUG
PY 2010
VL 19
IS 8
BP 1288
EP 1298
DI 10.1007/s00586-010-1380-3
UT WOS:000280577500007
PM 20386935
ER

PT J
AU Wolpaw, JR
   Birbaumer, N
   McFarland, DJ
   Pfurtscheller, G
   Vaughan, TM
AF Wolpaw, JR
   Birbaumer, N
   McFarland, DJ
   Pfurtscheller, G
   Vaughan, TM
TI Brain-computer interfaces for communication and control
SO CLINICAL NEUROPHYSIOLOGY
AB For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world - a brain-computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or 'locked in', with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10-25 bits/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers.
   With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances. (C) 2002 Elsevier Science Ireland Ltd. All rights reserved.
OI Wolpaw, Jonathan/0000-0003-0805-1315; Birbaumer,
   Niels/0000-0002-6786-5127
SN 1388-2457
EI 1872-8952
PD JUN
PY 2002
VL 113
IS 6
BP 767
EP 791
AR PII S1388-2457(02)00057-3
DI 10.1016/S1388-2457(02)00057-3
UT WOS:000176503000002
PM 12048038
ER

PT J
AU Liebsch, C
   Zimmermann, J
   Graf, N
   Schilling, C
   Wilke, HJ
   Kienle, A
AF Liebsch, Christian
   Zimmermann, Julia
   Graf, Nicolas
   Schilling, Christoph
   Wilke, Hans-Joachim
   Kienle, Annette
TI In vitro validation of a novel mechanical model for testing the
   anchorage capacity of pedicle screws using physiological load
   application
SO JOURNAL OF THE MECHANICAL BEHAVIOR OF BIOMEDICAL MATERIALS
AB Biomechanical in vitro tests analysing screw loosening often include high standard deviations caused by high variabilities in bone mineral density and pedicle geometry, whereas standardized mechanical models made of PU foam often do not integrate anatomical or physiological boundary conditions. The purpose of this study was to develop a most realistic mechanical model for the standardized and reproducible testing of pedicle screws regarding the resistance against screw loosening and the holding force as well as to validate this model by in vitro experiments.
   The novel mechanical testing model represents all anatomical structures of a human vertebra and is consisting of PU foam to simulate cancellous bone, as well as a novel pedicle model made of short carbon fibre filled epoxy. Six monoaxial cannulated pedicle screws (06.5 x 45 mm) were tested using the mechanical testing model as well as human vertebra specimens by applying complex physiological cyclic loading (shear, tension, and bending; 5 Hz testing frequency; sinusoidal pulsating forces) in a dynamic materials testing machine with stepwise increasing load after each 50.000 cycles (100.0 N shear force + 20.0 N per step, 51.0 N tension force + 10.2 N per step, 4.2 N m bending moment + 0.8 N m per step) until screw loosening was detected. The pedicle screw head was fixed on a firmly clamped rod while the load was applied in the vertebral body. For the in vitro experiments, six human lumbar vertebrae (L1-3, BMD 75.4 +/- 4.0 mg/cc HA, pedicle width 9.8 +/- 0.6 mm) were tested after implanting pedicle screws under X-ray control. Relative motions of pedicle screw, specimen fixture, and rod fixture were detected using an optical motion tracking system.
   Translational motions of the mechanical testing model experiments in the point of load introduction (0.9-2.2 mm at 240 N shear force) were reproducible within the variation range of the in vitro experiments (0.6-3.5 nun at 240 N shear force). Screw loosening occurred continuously in each case between 140 N and 280 N, while abrupt failures of the specimen were observed only in vitro. In the mechanical testing model, no translational motion was detected in the screw entry point, while in vitro, translational motions of up to 2.5 mm in inferior direction were found, leading to a slight shift of the centre of rotation towards the screw tip. Translational motions of the screw tip of about 5 mm in superior direction were observed both in vitro and in the mechanical testing model, while they were continuous in the mechanical testing model and rapidly increasing after screw loosening initiation in vitro.
   The overall pedicle screw loosening characteristics were qualitatively and quantitatively similar between the mechanical testing model and the human vertebral specimens as long as there was no translation of the screw at the screw entrance point. Therefore, the novel mechanical testing model represents a promising method for the standardized testing of pedicle screws regarding screw loosening for cases where the screw rotates around a point close to the screw entry point.
RI Wilke, Hans-Joachim/M-6440-2014; Liebsch, Christian/AAY-9918-2020
OI Wilke, Hans-Joachim/0000-0001-6007-8844; Liebsch,
   Christian/0000-0003-1543-4728
SN 1751-6161
EI 1878-0180
PD JAN
PY 2018
VL 77
BP 578
EP 585
DI 10.1016/j.jmbbm.2017.10.030
UT WOS:000418309500067
PM 29096123
ER

PT C
AU Ter Hark, M
AF Ter Hark, Michel
BE Parusnikova, Z
   Cohen, R
TI Popper's Theory of the Searchlight: A Historical Assessment of Its
   Significance
SO RETHINKING POPPER
SE Boston Studies in the Philosophy and History of Science
CT International Conference on Rethinking Popper
CY SEP, 2007
CL Inst Philosophy Czech Acad Sci, Prague, CZECH REPUBLIC
HO Inst Philosophy Czech Acad Sci
AB On the basis of a correspondence between Karl Popper and the Dutch psychologist Adriaan de Groot, it is argued that the former's epistemology of the searchlight is historically rooted in early cognitive psychology of Otto Selz. It is furthermore argued that Popper's later critique of information processing psychology is the fruit of his assimilation of Selz's evolutionarily inspired program. In light of the current interest in evolutionary approaches to the mind, it is argued that this Popper-Selz program is as actual as ever.
   In my recent book Popper, Otto Selz and the Rise of Evolutionary Epistemology (ter Hark 2004), I have attempted both to trace the origins of Popper's epistemology of the searchlight back to early German psychology, in particular the work of Otto Selz, and to point out the way the work of the latter diverges from the mainstream psychology at the time including the programme of the closely related Wurzburger School of psychology. As I also pointed out in my book, and as has been mentioned by others as well, Otto Selz played a formative role in the rise of cognitive science in the early 1950s of the last century in the USA, notably the work of later Nobel laureates Allen Newell and Herbert A. Simon. A mediating role here has been played by the Dutch psychologist and methodologist Adriaan de Groot, who was the first to apply Selz's ideas to the thinking processes of chess masters, and whose book Het Denken van den Schaker (de Groot 1946) was studied by Newell and Simon, in the original language, in 1954, a year before the revolution started. In 1965, an English translation of de Groot's book, On Thought and Chess appeared. As some of the new footnotes make clear, de Groot in his turn has been influenced by the American reception of Selz's ideas. For now, in the computational era, he claims that thought processes, as analyzed by Selz in the pre-computational era, might be simulated by a machine-program. As the further development of cognitive science has shown, the idea of so-called strong artificial intelligence, vigorously defended by Simon by means of his notion of a physical symbol system having the necessary and sufficient means for general intelligent action, has come increasingly under attack (Simon 1996). More and more, the idea of physical symbol systems has made place for an evolutionary or biological approach to the study of intelligence.
   As was pointed out in the final chapter of my book, the evolutionary approach to intelligence has been a pervasive feature of Selz's work, even neglected by de Groot. But not by young Popper. In his work on epistemology and the mind-body problem from the 1960s onwards, the evolutionary approach is again a dominating feature. From this perspective Popper's correspondence with de Groot is particularly interesting. My point of departure in this chapter is a particular letter in which they discuss the significance of Selz both for methodology and psychology, as well as the approach taken by Newell and Simon. What is illustrated by this letter is not only that there has been a typical European approach to the study of cognition, different from the American approach, but also that young Popper showed precocious awareness of the lasting significance of one of the most important contributors to this European tradition.
SN 0068-0346
BN 978-1-4020-9337-1
PY 2009
VL 272
BP 175
EP 184
UT WOS:000268846300015
ER

PT J
AU Michel, PA
   Kronenberg, D
   Neu, G
   Stolberg-Stolberg, J
   Frank, A
   Pap, T
   Langer, M
   Fehr, M
   Raschke, MJ
   Stange, R
AF Michel, Philipp A.
   Kronenberg, Daniel
   Neu, Gertje
   Stolberg-Stolberg, Josef
   Frank, Andre
   Pap, Thomas
   Langer, Martin
   Fehr, Michael
   Raschke, Michael J.
   Stange, Richard
TI Microsurgical reconstruction affects the outcome in a translational
   mouse model for Achilles tendon healing
SO JOURNAL OF ORTHOPAEDIC TRANSLATION
AB Background: Animal models are one of the first steps in translation of basic science findings to clinical practice. For tendon healing research, transgenic mouse models are important to advance therapeutic strategies. However, the small size of the structures complicates surgical approaches, histological assessment, and biomechanical testing. In addition, available models are not standardized and difficult to compare. How surgery itself affects the healing outcome has not been investigated yet. The focus of the study was to develop a procedure that includes a transection and microsurgical reconstruction of the Achilles tendon but, unlike other models, preserves the sciatic nerve. We wanted to examine how distinct parts of the technique influenced healing.
   Methods: For this animal model study, we used 96 wild-type male C57BL/6 mice aged 8-12 weeks. We evaluated different suture techniques and macroscopically confirmed the optimal combination of suture material and technique to minimize tendon gap formation. A key element is the detailed, step-by-step illustration of the surgery. In addition, we assessed histological (Herovici and Alcian blue staining) outcome parameters at 1-16 weeks postoperatively. Microcomputed tomography (micro-CT) was performed to measure the bone volume of heterotopic ossifications (HOs). Biomechanical analyses were carried out using a viscoelastic protocol on the biomechanical testing machine LM1.
   Results: A modified 4-strand suture combined with a cerclage for immobilization without transection of the sciatic nerve reliably eliminated gap formation. The maximal dorsal extension of the hindlimb at the upper ankle joint from the equinus position (limited by the immobilization cerclage) increased over time postoperatively (operaion: 28.8 +/- 2.2 degrees; 1 week: 54 +/- 36 degrees; 6 weeks: 80 +/- 11.7 degrees; 16 weeks: 96 +/- 15.8 degrees, p > 0.05). Histological staining revealed a maturation of collagen fibres within 6 weeks, whereas masses of cartilage were visible throughout the healing period. Micro-CT scans detected the development of HOs starting at 4 weeks and further progression at 6 and 16 weeks (bone volume, 4 weeks: 0.07604 +/- 0.05286 mm(3); 6 weeks: 0.50682 +/- 0.68841 mm(3); 16 weeks: 2.36027 +/- 0.85202 mm(3), p > 0.001). In-depth micro-CT analysis of the different surgical elements revealed that an injury of the tendon is a key factor for the development of HOs. Immobilization alone does not trigger HOs. Biomechanical properties of repaired tendons were greatly altered and remained inferior 6 weeks after surgery.
   Conclusion With this study, we demonstrated that the microsurgical technique greatly influences the short- and longer-term healing outcome. When the sciatic nerve is preserved, the best surgical reconstruction of the tendon defect is achieved by a 4-strand core suture in combination with a tibiofibular cerclage for postoperative immobilization. The cerclage promotes a gradual increase in the range of motion of the upper ankle joint, comparable with an early mobilization rehabilitation protocol. HO, as a key mechanism for poor tendon healing, is progressive and can be monitored early in the model.
   The translational potential of this article: The study enhances the understanding of model dependent factors of healing. The described reconstruction technique provides a reproducible and translational rodent model for future Achilles tendon healing research. In combination with transgenic strains, it can be facilitated to advance therapeutic strategies to improve the clinical results of tendon injuries.
RI Langer, Martin/HKP-1288-2023; Laumann, Marion/AAX-3607-2021; Kronenberg,
   Daniel/ABA-9255-2020; Frank, Andre/ABA-9251-2020
OI Kronenberg, Daniel/0000-0001-6807-7328; Frank,
   Andre/0000-0001-7099-9719; Stange, Richard/0000-0003-0807-3151
SN 2214-031X
PD SEP
PY 2020
VL 24
BP 1
EP 11
DI 10.1016/j.jot.2020.04.003
UT WOS:000577490400002
PM 32489862
ER

PT J
AU Hervieu, A
   Bouthemy, P
   Le Cadre, JP
AF Hervieu, Alexandre
   Bouthemy, Patrick
   Le Cadre, Jean-Pierre
TI Video Trajectory-based Event Recognition using Hidden Markov Models
SO TRAITEMENT DU SIGNAL
AB We address the problem of dynamic event recognition in videos. This is motivated by increasing needs for content-based exploitation of video footage, as encouraged in numerous applications, e.g., retrieving video sequences in large TV archives, creating automatic video summarization of sport TV programs, or detecting specific actions or activities in video-surveillance. It implies to tackle the well-known semantic gap between computed low-level features and high-level concepts. Considering 2D trajectories is attractive since they form computable image features which capture elaborated spatio-temporal information on the viewed actions. Methods for tracking moving objects in an image sequence are now available to get reliable enough 2D trajectories in various situations. These trajectories are given as a set of consecutive positions (x, y) in the image plane over time. If they are embedded in an appropriate modeling framework, high-level information on the dynamic scene can then be reachable.
   We aim at designing a general trajectory classification method that does not exploit strong a priori information on the scene structure, the camera set-up, the 3D object motions, while taking into account both the trajectory shape (geometrical information related to the type of motion and to variations in the motion direction) and the speed changes of the moving object on its trajectory (dynamics-related information). Appropriate local differential features combining curvature and motion magnitude are defined and robustly computed on the motion trajectories. Moreover, these features are not affected by the location of the trajectory in the image plane (invariance to translation), by its direction in the image plane (invariance to rotation) and by the distance of the viewed action to the camera (invariance to scale), and may allow comparison of trajectories from different cameras. A robust enough non-parametric feature extraction framework is also proposed since local differential features computed on the extracted trajectories are prone to be noise corrupted.
   To efficiently process the invariant trajectory characterization, probabilistic networks, and more specifically hidden Markov models (HMM) are used since the inherent properties of this modeling help taking into account the temporal evolution of the spatio-temporal information contained in the trajectories. Classical HMM, relying on Gaussian mixture modes (GMM), are designed to model data of sufficient sizes. Hence they may fail treating small trajectories with only few dozens of observations. An original HMM modeling, based on a uniform quantization of the observation space and dealing efficiently with small trajectories, is proposed, and an efficient HMM state number selection is also developed. To compare trajectories, a similarity measure is defined based on the Rabiner distance between HMM, and used to process the video event retrieval task.
   By considering a feature having those invariances and characteristics (considering both the trajectory shapes and speed evolutions), we consider the trajectory as a dynamical pattern while other methods consider the trajectories as attached to the camera point of view. We have compared our approach with other methods to put forward the properties (spatio-temporal modeling and efficient processing of small trajectories) of the developed method. Methods relying on feature histogram comparisons, on HMM/GMM modeling and on support vector machine (SVM) were considered. A set of comparative experiments on real videos (especially Formula One and ski TV program) with classification ground truth has been conducted and showed that the proposed method supplies accurate results and offers better performances than other methods.
SN 0765-0019
PY 2009
VL 26
IS 3
BP 187
EP 197
UT WOS:000271926600001
ER

PT J
AU You, Y
   Zhang, Z
   Hsieh, CJ
   Demmel, J
   Keutzer, K
AF You, Yang
   Zhang, Zhao
   Hsieh, Cho-Jui
   Demmel, James
   Keutzer, Kurt
TI Fast Deep Neural Network Training on Distributed Systems and Cloud TPUs
SO IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS
AB Since its creation, the ImageNet-1k benchmark set has played a significant role as a benchmark for ascertaining the accuracy of different deep neural net (DNN) models on the image classification problem. Moreover, in recent years it has also served as the principal benchmark for assessing different approaches to DNN training. Finishing a 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU takes 14 days. This training requires 10(18) single precision operations in total. On the other hand, the world's current fastest supercomputer can finish 3 x 10(17) single precision operations per second (according to the Nov 2018 Top 500 results). If we can make full use of the computing capability of the fastest supercomputer, we should be able to finish the training in several seconds. Over the last two years, researchers have focused on closing this significant performance gap through scaling DNN training to larger numbers of processors. Most successful approaches to scaling ImageNet training have used the synchronous mini-batch stochastic gradient descent (SGD). However, to scale synchronous SGD one must also increase the batch size used in each iteration. Thus, for many researchers, the focus on scaling DNN training has translated into a focus on developing training algorithms that enable increasing the batch size in data-parallel synchronous SGD without losing accuracy over a fixed number of epochs. In this paper, we investigate supercomputers' capability of speeding up DNN training. Our approach is to use a large batch size, powered by the Layer-wise Adaptive Rate Scaling (LARS) algorithm, for efficient usage of massive computing resources. Our approach is generic, as we empirically evaluate the effectiveness on five neural networks: AlexNet, AlexNet-BN, GNMT, ResNet-50, and ResNet-50-v2 trained with large datasets while preserving the state-of-the-art test accuracy. Compared to the baseline of a previous study from Goyal et al. [1] , our approach shows higher test accuracy on batch sizes that are larger than 16K. When we use the same baseline, our results are better than Goyal et al. for all the batch sizes (Fig. 20 ). Using 2,048 Intel Xeon Platinum 8160 processors, we reduce the 100-epoch AlexNet training time from hours to 11 minutes. With 2,048 Intel Xeon Phi 7250 Processors, we reduce the 90-epoch ResNet-50 training time from hours to 20 minutes. Our implementation is open source and has been released in the Intel distribution of Caffe, Facebook's PyTorch, and Google's TensorFlow. The difference between this paper and the conference-version of our work [2] includes: (1) we implement our approach on Google's cloud Tensor Processing Unit (TPU) platform, which verifies our previous success on CPUs and GPUs. (2) we scale the batch size of ResNet-50-v2 to 32K and achieve 76.3 percent accuracy, which is better than the 75.3 percent accuracy achieved in our conference paper. (3) we apply our approach to Google's Neural Machine Translation (GNMT) application, which helps us to achieves 4x speedup on the cloud TPUs.
RI Zhang, Zhao/AAX-9485-2021
OI Zhang, Zhao/0000-0001-5921-0035
SN 1045-9219
EI 1558-2183
PD NOV
PY 2019
VL 30
IS 11
BP 2449
EP 2462
DI 10.1109/TPDS.2019.2913833
UT WOS:000492450900006
ER

PT J
AU Hramov, AE
   Maksimenko, VA
   Pisarchik, AN
AF Hramov, Alexander E.
   Maksimenko, Vladimir A.
   Pisarchik, Alexander N.
TI Physical principles of brain-computer interfaces and their applications
   for rehabilitation, robotics and control of human brain states
SO PHYSICS REPORTS-REVIEW SECTION OF PHYSICS LETTERS
AB Brain-computer interfaces (BCIs) development is closely related to physics. In this paper, we review the physical principles of BCIs, and underlying novel approaches for registration, analysis, and control of brain activity. We analyze recent advances in BCI studies focusing on their applications for (i) controlling the movement of robots and exoskeletons, (ii) revealing and preventing brain pathologies, (iii) assessing and controlling psychophysiological states, and (iv) monitoring and controlling normal and pathological cognitive activity.
   We consider the BCI as a hardware/software communication system that allows interaction of humans or animals with their surroundings without the involvement of peripheral nerves and muscles, using control signals generated from brain cerebral activity. Classifying BCIs into three main types (active, reactive and passive), we describe their functional models and neuroimaging methods, as well as novel techniques for signal enhancement and artifact recognition and avoidance, to improve BCI performance in real time. We also review different BCI applications, including communications, external device control, movement control, neuroprostheses, and assessment of human psychophysiological states.
   Then, we describe the most common techniques for the analysis and classification of electroencephalographic (EEG) and magnetoencephalographic (MEG) data. Special attention is paid to modern technology based on machine learning and reservoir computing. We discuss main results on the creation and application of BCIs based on invasive and noninvasive EEG recordings. First, we consider neurointerfaces for controlling the movement of robots and exoskeletons. Second, we describe BCIs for diagnosis and control of pathological brain activity, in particular, epilepsy. We also discuss the results on the development of invasive BCIs for predicting and mitigating absence epileptic seizures. After that, we focus on passive neurointerfaces for assessing and controlling a person's psychophysiological states and cognitive activity. Special attention is given to optogenetic brain interfaces using photostimulation to deliver intervention to specificcell types. We outline the basic principles of optogenetic neurocontrol and extracellular electrophysiology recording. We also describe the state-of-the-art of miniaturized closed-loop optogenetic devices to control normal and pathological brain activities.
   Further, we discuss the new emerging technological trend in the BCI development which consists in using neurointerfaces to improve the interaction between people, so-called brain-to-brain interfaces (BBIs). Such interfaces can increase the efficiency of collaborative processes when working in a group. We propose a BBI which distributes a cognitive load among all team members working on a common task. This BBI allows sharing the workload among the participants according to their current cognitive performance, estimated from their electrical brain activity. The novel results of the brain-to-brain interaction are promising for the development of a new generation of communication systems based on the neurophysiological brain activity of interacting persons, where the BBI estimates physical conditions of each partner and adapts the assigned task accordingly.
   Finally, we trace the main historical epochs in BCI development and applications and highlight possible future directions for this research area, including hybrid BCIs.
   (c) 2021 Elsevier B.V All rights reserved.
RI Pisarchik, Alexander N./E-7545-2011; Hramov, Alexander E/C-5600-2008
OI Pisarchik, Alexander N./0000-0003-2471-2507; Hramov, Alexander
   E/0000-0003-2787-2530
SN 0370-1573
EI 1873-6270
PD JUN 25
PY 2021
VL 918
SI SI
BP 1
EP 133
DI 10.1016/j.physrep.2021.03.002
EA JUN 2021
UT WOS:000659886000001
ER

PT J
AU Parida, S
   Bharadwaj, H
   Heinz, MG
AF Parida, Satyabrata
   Bharadwaj, Hari
   Heinz, Michael G.
TI Spectrally specific temporal analyses of spike-train responses to
   complex sounds: A unifying framework
SO PLOS COMPUTATIONAL BIOLOGY
AB Author summary
   Despite major technological and computational advances, we remain unable to match human auditory perception using machines, or to restore normal-hearing communication for those with sensorineural hearing loss. An overarching reason for these limitations is that the neural correlates of auditory perception, particularly for complex everyday sounds, remain largely unknown. Although neural responses can be measured in humans noninvasively and compared with perception, these evoked responses lack the anatomical and physiological specificity required to reveal underlying neural mechanisms. Single-unit spike-train responses can be measured from preclinical animal models with well-specified pathology; however, the disparate response types (point-process versus continuous-valued signals) have limited application of the same advanced signal-processing analyses to single-unit and evoked responses required for direct comparison. Here, we fill this gap with a unifying framework for analyzing both spike-train and evoked neural responses using advanced spectral analyses of both the slow and rapid response components that are known to be perceptually relevant for speech and music, particularly in challenging listening environments. Numerous benefits of this framework are demonstrated here, which support its potential to advance the translation of spike-train data from animal models to improve clinical diagnostics and technological development for real-world listening.
   Significant scientific and translational questions remain in auditory neuroscience surrounding the neural correlates of perception. Relating perceptual and neural data collected from humans can be useful; however, human-based neural data are typically limited to evoked far-field responses, which lack anatomical and physiological specificity. Laboratory-controlled preclinical animal models offer the advantage of comparing single-unit and evoked responses from the same animals. This ability provides opportunities to develop invaluable insight into proper interpretations of evoked responses, which benefits both basic-science studies of neural mechanisms and translational applications, e.g., diagnostic development. However, these comparisons have been limited by a disconnect between the types of spectrotemporal analyses used with single-unit spike trains and evoked responses, which results because these response types are fundamentally different (point-process versus continuous-valued signals) even though the responses themselves are related. Here, we describe a unifying framework to study temporal coding of complex sounds that allows spike-train and evoked-response data to be analyzed and compared using the same advanced signal-processing techniques. The framework uses a set of peristimulus-time histograms computed from single-unit spike trains in response to polarity-alternating stimuli to allow advanced spectral analyses of both slow (envelope) and rapid (temporal fine structure) response components. Demonstrated benefits include: (1) novel spectrally specific temporal-coding measures that are less confounded by distortions due to hair-cell transduction, synaptic rectification, and neural stochasticity compared to previous metrics, e.g., the correlogram peak-height, (2) spectrally specific analyses of spike-train modulation coding (magnitude and phase), which can be directly compared to modern perceptually based models of speech intelligibility (e.g., that depend on modulation filter banks), and (3) superior spectral resolution in analyzing the neural representation of nonstationary sounds, such as speech and music. This unifying framework significantly expands the potential of preclinical animal models to advance our understanding of the physiological correlates of perceptual deficits in real-world listening following sensorineural hearing loss.
RI Bharadwaj, Hari/GPX-1969-2022
OI Bharadwaj, Hari/0000-0001-8685-9630; Parida,
   Satyabrata/0000-0002-2896-2522; Heinz, Michael/0000-0002-1524-402X
SN 1553-734X
EI 1553-7358
PD FEB
PY 2021
VL 17
IS 2
AR e1008155
DI 10.1371/journal.pcbi.1008155
UT WOS:000621566000003
PM 33617548
ER

PT J
AU Neshat, H
   Cool, DW
   Barker, K
   Gardi, L
   Kakani, N
   Fenster, A
AF Neshat, Hamid
   Cool, Derek W.
   Barker, Kevin
   Gardi, Lori
   Kakani, Nirmal
   Fenster, Aaron
TI A 3D ultrasound scanning system for image guided liver interventions
SO MEDICAL PHYSICS
AB Purpose: Two-dimensional ultrasound (2D US) imaging is commonly used for diagnostic and intraoperative guidance of interventional liver procedures; however, 2D US lacks volumetric information that may benefit interventional procedures. Over the past decade, three-dimensional ultrasound (3D US) has been developed to provide the missing spatial information. 3D US image acquisition is mainly based on mechanical, electromagnetic, and freehand tracking of conventional 2D US transducers, or 2D array transducers available on high-end machines. These approaches share many problems during clinical use for interventional liver imaging due to lack of flexibility and compatibility with interventional equipment, limited field-of-view (FOV), and significant capital cost compared to the benefits they introduce. In this paper, a novel system for mechanical 3D US scanning is introduced to address these issues.
   Methods: The authors have developed a handheld mechanical 3D US system that incorporates mechanical translation and tilt sector sweeping of any standard 2D US transducer to acquire 3D images. Each mechanical scanning function can be operated independently or may be combined to allow for a hybrid wide FOV acquisition. The hybrid motion mode facilitates registration of other modalities (e.g., CT or MRI) to the intraoperative 3D US images by providing a larger FOV in which to acquire anatomical information. The tilting mechanism of the developed mover allows image acquisition in the intercostal rib space to avoid acoustic shadowing from bone. The geometric and volumetric scanning validity of the 3D US system was evaluated on tissue mimicking US phantoms for different modes of operation. Identical experiments were performed on a commercially available 3D US system for direct comparison. To replicate a clinical scenario, the authors evaluated their 3D US system by comparing it to CT for measurement of angle and distance between interventional needles in different configurations, similar to those used for percutaneous ablation of liver tumors.
   Results: The mean geometrical hybrid 3D reconstruction error measured from scanning of a known string phantom was less than 1 nun in two directions and 2.5 mm in the scanning direction, which was comparable or better than the same measurements obtained from a commercially available 3D US system. The error in volume measurements of spherical phantom models depended on depth of the object. For a 20 cm(3) model at a depth of 15 cm, a standard depth for liver imaging, the mean error was 3.6% +/- 4.5% comparable to the 2.3% +/- 1.8% error for the 3D US commercial system. The error in 3D US measurement of the tip distance and angle between two microwave ablation antennas inserted into the phantom was 0.9 +/- 0.5 mm and 1.1 degrees +/- 0.7 degrees, respectively.
   Conclusions: A 3D US system with hybrid scanning motions for large field-of-view 3D abdominal imaging has been developed and validated. The superior spatial information provided by 3D US might enhance image-guidance for percutaneous interventional treatment of liver malignancies. The system has potential to be integrated with other liver procedures and has application in other abdominal
RI Cool, Derek W/A-4568-2011; FENSTER, Aaron/K-4337-2013
OI Cool, Derek/0000-0003-1095-3553
SN 0094-2405
EI 2473-4209
PD NOV
PY 2013
VL 40
IS 11
AR 112903
DI 10.1118/1.4824326
UT WOS:000326991800064
PM 24320470
ER

PT J
AU Jeppesen, JO
   Nielsen, KA
   Perkins, J
   Vignon, SA
   Di Fabio, A
   Ballardini, R
   Gandolfi, MT
   Venturi, M
   Balzani, V
   Becher, J
   Stoddart, JF
AF Jeppesen, JO
   Nielsen, KA
   Perkins, J
   Vignon, SA
   Di Fabio, A
   Ballardini, R
   Gandolfi, MT
   Venturi, M
   Balzani, V
   Becher, J
   Stoddart, JF
TI Amphiphilic bistable rotaxanes
SO CHEMISTRY-A EUROPEAN JOURNAL
AB Two molecular shuttles/ switches-a slow one and a fast one-in the shape of amphiphilic, bistable [2]rotaxanes have been synthesized and characterized. Both [2]rotaxanes contain a hydrophobic, tetraarylmethane and a hydrophilic, dendritic stopper. They are comprised of two pi-electron-rich stations-a monopyrrolotetrathia-fulvalene unit and a 1,5-dioxynaphthalene moiety-which can act as recognition sites for the tetracationic cyclophane, cyclobis(paraquat-p-phenylene), to reside around. In addition, a model [2]rotaxane, incorporating only a monopyrrolotetrathiafulvalene unit in the rod section of the amphiphilic dumbbell component and cyclobis(paraquat-p-phenylene) as the ring component, has been investigated. The dumbbell-shaped components were constructed using conventional synthetic methodologies to assemble 1) the hydrophobic, tetraaryl-methane stopper and 2) the hydrophilic, dendritic stopper. Next, 3) the hydrophobic stopper was fused to the 1,5-dioxynaphthalene moiety and/or the monopyrrolotetrathiafulvalene unit by appropriate alkylations, followed by 4) attachment of the hydrophilic stopper, once again by alkylation to give the dumbbell-shaped compounds. Finally, 5) the [2]rotaxanes were self-assembled by using the dumbbells as templates for the formation of the encircling cyclobis(paraquat-p-phenylene) tetracations. The two [2]rotaxanes differ in their arrangement of the pi-electron-rich units, one in which the SMe group of the monopyrrolotetrathiafulvalene unit points toward the 1,5-dioxynaphthalene moiety (2(.)4PF(6)) and another in which it points away from the 1,5-dioxynaphthalene moiety (3(.)4PF(6)). This seemingly small difference in the orientation of the monopyrrolotetrathiafulvalene unit leads to profound changes in the physical properties of these rotaxanes. The bistable [2]rotaxanes were both isolated as brown solids. H-1 NMR and UV-visible spectroscopy, and electrochemical investigations, reveal the presence of both possible translational isomers at ambient temperature. As a consequence of the existence of both possible translational isomers in these bistable [2]rotaxanes, they exhibit a complex electrochemical behavior, which is further complicated by the presence of folded conformations wherein the monopyrrolotetrathiafulvalene unit is involved in an "alongside" interaction with the tetracationic cyclophane. In the molecular shuttle/switch 2(.)4PF(6) a "knob", in the shape of the SMe group, is situated between the monopyrrolotetrathiafulvalene and the 1,5-dioxynaphthalene recognition sites, making it possible to isolate both translational isomers (2(.)4PF(6)(.)GREEN and 2(.)4PF(6)(.)RED) and to investigate the kinetics of the shuttling of the cyclobis(paraquat-p-phenylene) tetracation between the two recognition sites. The shuttling processes, which are accompanied by clearly detectable color changes, can be followed by H-1 NMR and UV-visible spectroscopy, allowing the rate constants and energies of activation for the translation of the cyclobis(paraquat-p-phenylene) tetracations between the two recognition sites to be determined. In the molecular shuttle/switch 3(.)4PF(6), there is no "knob" situated between the 1,5-dioxynaphthalene and the monopyrrolotetrathiafulvalene recognition sites, resulting in a considerably faster shuttling of the cyclobis(paraquat-p-phenylene) tetracation between these two sites, making the separation of the two possible translational isomers of 3.4PF6 impractical. However, the shuttling of
   At low temperatures, the major translational isomer is 3(.)4PF(6)(.)RED, while 3(.)4PF(6)(.) GREEN is the major isomer at higher temperature. In the bistable [2]rotaxanes shuttling of the cyclobis(paraquat-p-phenylene) tetracations can be driven by electrochemical oxidation of the monopyrrolotetrathiafulvalene unit. In complexes in which one of the two dumbbell stoppers is missing, electrochemical oxidation causes dethreading.
RI Stoddart, James Fraser/H-1518-2011
OI Stoddart, James Fraser/0000-0003-3161-3697; Nielsen,
   Kent/0000-0002-5800-5774
SN 0947-6539
EI 1521-3765
PD JUL 7
PY 2003
VL 9
IS 13
BP 2982
EP 3007
DI 10.1002/chem.200204589
UT WOS:000184140700007
ER

PT J
AU Gergely, A
   Kiss, O
   Reicher, V
   Iotchev, I
   Kovacs, E
   Gombos, F
   Benczur, A
   Galambos, A
   Topal, J
   Kis, A
AF Gergely, Anna
   Kiss, Orsolya
   Reicher, Vivien
   Iotchev, Ivaylo
   Kovacs, Eniko
   Gombos, Ferenc
   Benczur, Andras
   Galambos, Agoston
   Topal, Jozsef
   Kis, Anna
TI Reliability of Family Dogs' Sleep Structure Scoring Based on Manual and
   Automated Sleep Stage Identification
SO ANIMALS
AB Simple Summary: Sleep alterations are known to be severe accompanying symptoms of many human psychiatric conditions, and validated clinical protocols are in place for their diagnosis and treatment. However, sleep monitoring is not yet part of standard veterinary practice, and the possible importance of sleep-related physiological alterations for certain behavioral problems in pets is seriously understudied. Recently, a non-invasive electroencephalography (EEG) method was developed for pet dogs that is well-suited for untrained individuals. This so called polysomnography protocol could easily be implemented in veterinary diagnosis. However, in order to make the procedure more effective and standardized, methodological questions about the validity and reliability of data processing need to be answered. As a first step, the present study tests the effect of several factors on the manual scoring of the different sleep stages (a standard procedure adopted from human studies). Scoring the same recordings but varying the number of EEG channels visible to the scorer (emulating the difference between single channel versus four channel recordings) resulted in significant differences in hypnograms. This finding suggests that using more recording electrodes may provide a more complete picture of dog brain electrophysiological activity. Visual sleep staging by three different expert raters also did not provide a full agreement, but despite this, there were no significant differences between raters in important output values such as sleep structure and the spectral features of the EEG. This suggests that the non-invasive canine polysomnography method is ready to be implemented for veterinary use, but there is room for further refinement of the data processing. Here, we describe which parts of the sleep recording yield the lowest agreement and present the first form of an automated method that can reliably distinguish awake from sleep stages and could thus accelerate the time-consuming manual data processing. The translation of the findings into clinical practice will open the door to the more effective diagnosis and treatment of disorders with sleep-related implications.
   Non-invasive polysomnography recording on dogs has been claimed to produce data comparable to those for humans regarding sleep macrostructure, EEG spectra and sleep spindles. While functional parallels have been described relating to both affective (e.g., emotion processing) and cognitive (e.g., memory consolidation) domains, methodologically relevant questions about the reliability of sleep stage scoring still need to be addressed. In Study 1, we analyzed the effects of different coders and different numbers of visible EEG channels on the visual scoring of the same polysomnography recordings. The lowest agreement was found between independent coders with different scoring experience using full (3 h-long) recordings of the whole dataset, and the highest agreement within-coder, using only a fraction of the original dataset (randomly selected 100 epochs (i.e., 100 x 20 s long segments)). The identification of drowsiness was found to be the least reliable, while that of non-REM (rapid eye movement, NREM) was the most reliable. Disagreements resulted in no or only moderate differences in macrostructural and spectral variables. Study 2 targeted the task of automated sleep EEG time series classification. Supervised machine learning (ML) models were used to help the manual annotation process by reliably predicting if the dog was sleeping or awake. Logistic regression models (LogREG), gradient boosted trees (GBT) and convolutional neural networks (CNN) were set up and trained for sleep state prediction from already collected and manually annotated EEG data. The evaluation of the individual models suggests that their combination results in the best performance: similar to 0.9 AUC test scores.
OI Andras, Benczur/0000-0002-9392-0986
SN 2076-2615
PD JUN
PY 2020
VL 10
IS 6
AR 927
DI 10.3390/ani10060927
UT WOS:000551757400001
PM 32466600
ER

PT J
AU Nyamai, DW
   Bishop, OT
AF Nyamai, Dorothy Wavinya
   Bishop, Ozlem Tastan
TI Identification of Selective Novel Hits against Plasmodium falciparum
   Prolyl tRNA Synthetase Active Site and a Predicted Allosteric Site Using
   In Silico Approaches
SO INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES
AB Recently, there has been increased interest in aminoacyl tRNA synthetases (aaRSs) as potential malarial drug targets. These enzymes play a key role in protein translation by the addition of amino acids to their cognate tRNA. The aaRSs are present in all Plasmodium life cycle stages, and thus present an attractive malarial drug target. Prolyl tRNA synthetase is a class II aaRS that functions in charging tRNA with proline. Various inhibitors against Plasmodium falciparum ProRS (PfProRS) active site have been designed. However, none have gone through clinical trials as they have been found to be highly toxic to human cells. Recently, a possible allosteric site was reported in PfProRS with two possible allosteric modulators: glyburide and TCMDC-124506. In this study, we sought to identify novel selective inhibitors targeting PfProRS active site and possible novel allosteric modulators of this enzyme. To achieve this, virtual screening of South African natural compounds against PfProRS and the human homologue was carried out using AutoDock Vina. The modulation of protein motions by ligand binding was studied by molecular dynamics (MD) using the GROningen MAchine for Chemical Simulations (GROMACS) tool. To further analyse the protein global motions and energetic changes upon ligand binding, principal component analysis (PCA), and free energy landscape (FEL) calculations were performed. Further, to understand the effect of ligand binding on the protein communication, dynamic residue network (DRN) analysis of the MD trajectories was carried out using the MD-TASK tool. A total of ten potential natural hit compounds were identified with strong binding energy scores. Binding of ligands to the protein caused observable global and residue level changes. Dynamic residue network calculations showed increase in betweenness centrality (BC) metric of residues at the allosteric site implying these residues are important in protein communication. A loop region at the catalytic domain between residues 300 and 350 and the anticodon binding domain showed significant contributions to both PC1 and PC2. Large motions were observed at a loop in the Z-domain between residues 697 and 710 which was also in agreement with RMSF calculations that showed increase in flexibility of residues in this region. Residues in this loop region are implicated in ATP binding and thus a change in dynamics may affect ATP binding affinity. Free energy landscape (FEL) calculations showed that the holo protein (protein-ADN complex) and PfProRS-SANC184 complexes were stable, as shown by the low energy with very few intermediates and hardly distinguishable low energy barriers. In addition, FEL results agreed with backbone RMSD distribution plots where stable complexes showed a normal RMSD distribution while unstable complexes had multimodal RMSD distribution. The betweenness centrality metric showed a loss of functional importance of key ATP binding site residues upon allosteric ligand binding. The deep basins in average L observed at the allosteric region imply that there is high accessibility of residues at this region. To further analyse BC and average L metrics data, we calculated the Delta BC and Delta L values by taking each value in the holo protein BC or L matrix less the corresponding value in the ligand-bound complex BC or L matrix. Interestingly, in allosteric complexes, residues located in a loop region implicated in ATP binding had negative Delta L values while in orthosteric complexes these residues had positive Delta L values.
   An increase in contact frequency between residues Ser263, Thr267, Tyr285, and Leu707 at the allosteric site and residues Thr397, Pro398, Thr402, and Gln395 at the ATP binding TXE loop was observed. In summary, this study identified five potential orthosteric inhibitors and five allosteric modulators against PfProRS. Allosteric modulators changed ATP binding site dynamics, as shown by RMSF, PCA, and DRN calculations. Changes in dynamics of the ATP binding site and increased contact frequency between residues at the proposed allosteric site and the ATP binding site may explain how allosteric modulators distort the ATP binding site and thus might inhibit PfProRS. The scaffolds of the identified hits in the study can be used as a starting point for antimalarial inhibitor development with low human cytotoxicity.
RI Bishop, Ozlem Tastan/J-3084-2016
OI Bishop, Ozlem Tastan/0000-0001-6861-7849; NYAMAI,
   DOROTHY/0000-0003-4704-9883
EI 1422-0067
PD JUN
PY 2020
VL 21
IS 11
AR 3803
DI 10.3390/ijms21113803
UT WOS:000543400300065
PM 32471245
ER

PT J
AU Wamba-Taguimdje, SL
   Wamba, SF
   Kamdjoug, JRK
   Wanko, CET
AF Wamba-Taguimdje, Serge-Lopez
   Fosso Wamba, Samuel
   Kala Kamdjoug, Jean Robert
   Tchatchouang Wanko, Chris Emmanuel
TI Influence of artificial intelligence (AI) on firm performance: the
   business value of AI-based transformation projects
SO BUSINESS PROCESS MANAGEMENT JOURNAL
AB Purpose The main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations. Design/methodology/approach This study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations' AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase. Findings AI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes. Research limitations/implications AI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations' performance, the profitability of their investments in AI transformation projects, and some competitive advantage.
   This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company's business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure. Originality/value This article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.
RI Wamba, Samuel Fosso/AAB-4953-2019
OI Wamba, Samuel Fosso/0000-0002-1073-058X; WAMBA-TAGUIMDJE,
   SERGE-LOPEZ/0000-0002-4493-4695; TCHATCHOUANG WANKO, Chris
   Emmanuel/0000-0001-8016-3579
SN 1463-7154
EI 1758-4116
PD NOV 2
PY 2020
VL 26
IS 7
BP 1893
EP 1924
DI 10.1108/BPMJ-10-2019-0411
EA MAY 2020
UT WOS:000531838400001
ER

PT J
AU Itoi, E
   Lee, SB
   Berglund, L
   Berge, LL
   An, KN
AF Itoi, E
   Lee, SB
   Berglund, L
   Berge, LL
   An, KN
TI The effect of a glenoid defect on anteroinferior stability of the
   shoulder after Bankart repair: A cadaveric study
SO JOURNAL OF BONE AND JOINT SURGERY-AMERICAN VOLUME
AB Background: An osseous defect of the glenoid rim is sometimes caused by multiple recurrent dislocations of the shoulder. It is generally thought that a large defect should be treated with bone-grafting, but there is a lack of consensus,vith regard to how large a defect must be in order to necessitate this procedure. Some investigators have proposed that a defect must involve at least one-third of the glenoid surface in order to necessitate bone-grafting. However, it is difficult to determine (1) whether a defect involves one-third of the glenoid surface and (2) whether a defect of this size is critical to the stability of the shoulder after a Bankart repair. The purposes of the present study were (1) to create and quantify various sizes of osseous defects of the glenoid and (2) to determine the effect of such defects on the stability and motion of the shoulder after Bankart repair
   Methods: The glenoids from sixteen dried scapulae were photographed, and the images were scanned into a computer The average shape of the glenoid was determined on the basis of the scans, and this information was used to design custom templates for the purpose of creating various sizes of osseous; defects. Ten fresh-frozen cadaveric shoulders then were obtained from individuals who had been an average of seventy-nine years old at the time of death, and all muscles were removed to expose the joint capsule. With use of a custom multiaxis electromechanical testing machine,vith a six-degrees-of-freedom load-cell, the humeral head was translated ten millimeters in the anteroinferior direction,vith the arm in abduction and external rotation as well as in abduction and internal rotation. With a fifty-newton axial force constantly applied to the humerus in order to keep the humeral head centered in the glenoid fossa, the peak force that was needed to translate the humeral head a normalized distance was determined under eleven sequential conditions: (1) with the capsule intact, (2) after the creation of a simulated Bankart lesion, (3) after the capsule was repaired, (4) after the creation of an anteroinferior osseous defect with a width that was 9 percent of the glenoid length (average,width, 2.8 millimeters), (5) after the capsule was repaired, (6) after the creation of an osseous defect with a width that was 21 percent of the glenoid length (average width, 6.8 millimeters), (7) after the capsule was repaired, (8) after the creation of an osseous defect,vith a width that was 34 percent of the glenoid length (average width, 10.8 millimeters), (9) after the capsule was repaired, (10) after the creation of an osseous defect,vith a,width that was 46 percent of the glenoid length (average width, 14.8 millimeters), and (11) after the capsule was repaired.
   Results: With the arm in abduction and external rotation, the stability of the shoulder after Bankart repair did not change significantly regardless of the size of the osseous defect (p = 0.106). With the arm in abduction and internal rotation, the stability decreased significantly as the size of the osseous defect increased (p < 0.0001): the translation force in shoulders in which the width of the osseous defect was at least 21 percent of the glenoid length (average width, 6.8 millimeters) was significantly smaller than the force in shoulders without an osseous defect. The range of external rotation in shoulders in which the width of the osseous defect was at least 21 percent of the glenoid length was significantly less than that in shoulders,without a defect (p < 0.0001) because of the pretensioning of the capsule caused by closing the gap between the detached capsule and the glenoid rim. The average loss of external rotation was 25 degrees per centimeter of defect.
   Conclusions: An osseous defect with a width that is at least 21 percent of the glenoid length may cause instability and limit the range of motion of the shoulder after Bankart repair.
   Clinical Relevance: The results of the present study suggest that measures to restore the are of glenoid concavity may be beneficial, in terms of both stability and motion, for patients who have a glenoid defect,vith a width that is at least 21 percent of the glenoid length.
SN 0021-9355
EI 1535-1386
PD JAN
PY 2000
VL 82A
IS 1
BP 35
EP 46
DI 10.2106/00004623-200001000-00005
UT WOS:000084759400005
PM 10653082
ER

PT J
AU Li, K
   Shi, T
   Shi, SH
   Fu, GY
   Wang, MY
   Zhang, RW
   Liu, G
AF Li Kuan
   Shi Tuo
   Shi Shihong
   Fu Geyan
   Wang Mingyu
   Zhang Rongwei
   Liu Guang
TI Laser Powder Feeding Additive Manufacturing of Ternary Blade with
   Abnormity Base Plane
SO CHINESE JOURNAL OF LASERS-ZHONGGUO JIGUANG
AB Objective Ternary blades are used in the impellers of aerospace turbojet engines, large-scale marine gas turbines, and automotive advanced turbine engines. The joints connecting the ternary blades to the impeller base are irregularly shaped surfaces. The ternary blade has special, complex, and spatial structure with a large inclination angle, large distortion, and other characteristics that conform to the ternary flow design theory; this design reduces energy loss when fluid travels through the impeller and improves its efficiency. Thus, ternary blade manufacturing has received considerable attention, and its unique structure imposes severe processing requirements. However, ternary blades are mainly processed by Computerized Numerical Control Electrical Discharge Machining (CNC EDM) and CNC milling; the number of materials eliminated during the processing is enormous, and the processing cycle is extensive. Laser additive manufacturing, a revolutionary type of forming technique, was proposed in the 1990s, which is used for efficient and moldless manufacturing of complicated structural elements. The research on laser additive manufacturing of ternary blades has significant practical applications. Currently, most multi-twisted blade formation, both home and abroad, is performed on a flat or cylindrical generatrix, and there are few reports on laser powder feeding additive manufacturing ternary blades on special-shaped base surfaces. The technique of streamline layering is proposed in this study, and the cladding formation of the ternary blade is effectively accomplished; this is based on the optical inner powder feeding cladding nozzle produced independently in the laboratory.
   Methods The three-element blades described in this study are curved in the flow direction and are three-dimensionally twisted, with complex twisting and tilting characteristics. The blade stacking substrate is a concave curved surface rather than a standard horizontal substrate. However, it is challenging to obtain information on each unit' s location and direction. The bottom of the three-element blade is an arc main base surface on the special-shaped base surface. Thus, this study proposes the following streamlined layering methods: first, the primary base surface is divided evenly in the radial direction to obtain circular arc slices perpendicular to the radial direction; second, the arc slices are divided equidistantly along the scanning direction to obtain the division unit; lastly, the vector formed by the centerline of the upper and lower planes of the division unit is used to determine the height and deflection direction. Further, when stacking, the nozzle' s axis aligns with the dividing unit' s direction; the cladding nozzle moves according to the division unit' s position and direction, and the final splicing obtains the formed part of the ternary blade.
   Results and Discussions To obtain segmentation units with varied structural characteristics, the streamline layering technique is developed for stacking the ternary blade' s forming parts [ Fig. 4 ( f )] . The homogeneous transformation matrix of each segmentation unit relative to the base coordinate system is determined through translation and rotation calculation, thus, determining the positional and directional information of each segmentation unit [Fig. 5( c) ] . Further, the laser cladding nozzle in this study is an optical internal powder-feeding nozzle, which is independently developed in the laboratory; the cladding nozzle is installed on a six-axis robotic arm. During cladding, the axis of the cladding nozzle aligns with the dividing unit's direction and fits along the curve. The single-layer arc slice cladding is completed by splicing the trajectory of each segmentation unit, and the multilayer arc slice overlaps to generate the ternary blade laser cladding, forming a portion with an enormous inclination, complex bending, and torsion structure (Fig. 7).
   Conclusions The streamline layering technique is proposed in this study to tackle the ternary blade laser cladding formation trajectory problem. The main base surface is evenly spaced along the radial direction to obtain arc slices perpendicular to the radial direction, which are then evenly spaced along the scanning direction to obtain the segmentation unit. The streamlined layering technique is proposed in this study to solve the problem of ternary blade structural parts layering, bending, and inclination, and to accomplish the accumulation of ternary blade forming parts. The inspection results of the formed parts are as follows: the surface of the ternary blade-formed parts is smooth, and the average surface roughness value is less than 4.065 mu m, which effectively reduces the step effect; it achieves a good metallurgical combination with the irregular base surface, and the average thickness of the formed parts is 5.97 mm. The relative errors of thickness and torsion angle are from -1. 4% to 1. 03% and - 4. 67% , respectively. The forming accuracy is high. The heat accumulation at the laser molten pool is evident as the height of the formed part increases. Further, the microhardness reduces as the microstructure increases; the microhardness of the formed part ranges from 348. 3 to 360. 4 HV, and the metallographic structure is uniformly dense with no evident holes or cracks.
SN 0258-7025
PD JAN
PY 2022
VL 49
IS 2
AR 0202019
DI 10.3788/CJL202249.0202019
UT WOS:000824289700005
ER

PT J
AU Hennoste, T
AF Hennoste, Tiit
TI Literature as resistance in Soviet Estonia in the post-World War II
   period
SO AJALOOLINE AJAKIRI-THE ESTONIAN HISTORICAL JOURNAL
AB The theme of this article is the resistance that took place in Soviet Estonian literature, literary criticism and literary studies in the post-Second World War period. The article accentuates that different modes and objectives of resistance were central in different periods.
   Literary resistance is divided into four groups according to the nature of the pressure and the aims of resistance: first, ideological resistance to Soviet ideology in the name of literature that is free of ideology, or in the name of some other ideology; second, national resistance in the name of the unity of the people and preservation of identity; third, aesthetic resistance to the official literary doctrine; and fourth, resistance in the name of general or personal freedom and authenticity. Writers and literary scholars used different modes of resistance. These were so-called writing for the desk drawer, silence within a text, the use of 'secret codes', self-publication, the selection of themes or modes of writing that were not favoured by the regime and were apolitical and nonideological, and the use of neutral words and concepts instead of concepts and words bearing Soviet ideology.
   Totalitarian control of literature by way of decisions and direct instructions from the Communist Party characterised the Stalinist period (until 1956). All literature had to adhere to the doctrine of socialist realism. Practically the only form of resistance in this period was to keep silent. Some authors remained completely silent, some worked on translations, some wrote for their desk drawer for themselves and presented texts for publication that adhered to the officially sanctioned model. Keeping silent can also be interpreted as resistance in the name of aesthetic authenticity.
   The subsequent period that lasted until the 1970s is characterised by an increase in liberty in society, including literature. The body of norms of socialist realism was relaxed. Literary activities were controlled by writers' organisations according to the guidelines provided by the Communist Party. Different aesthetic and ideological camps of writers emerged and competed with one another. The era of keeping silent and writing for one's desk drawer ended. Public resistance, which was united by the question of relating to literature that preceded the Soviet era, was at the centre of this period. The fight for aesthetic freedom and literature that was free of ideology carried on throughout this period and was finally won by 1968-69. By that time, socialist realism had essentially ended in Estonian literature. In place of it, avant-gardism, modernism and broader realism prevailed. In place of Marxism-Leninism, non-Marxist ways of thinking had become important: first and foremost existentialism, but also Buddhism, Zen Buddhism, Taoism and classical psychoanalysis.
   Secondly, resistance was put up in the name of Estonian national unity and national memory. This was resistance in the name of authors who had been banished from the history of literature and of bringing back the prewar metalanguage. This was concerned with modern writers (symbolists, decadents, impressionists, expressionists) in Estonian literature from the early 20th century. Generally speaking, this struggle was successful.
   The third struggle was waged in the name of creative freedom and the writer's inner authenticity. Here political freedom and independence in general intertwined as ideals, with the Soviet system and any kind of system as the enemy that oppresses human freedom and independence: institutions and the state, machines and rationality, conformism and the middle class way of life.
   The third period of resistance began at the start of the 1970s and continued until perestroika. The so-called tightening of the screws took place throughout the state during this period and Russification was adopted as a new orientation starting in the mid-1970s. On the other hand, a socialist consumer society took shape in Estonia, characterised by Communist Party membership for the sake of one's career and openly double morality. Ideological censorship in literature was intensified, along with the partial steering of literature by way of Party documents. Such new conditions brought new variants of resistance to the fore.
   Nationalist resistance and resistance to Russification came to the fore in the 1970s and 1980s. Open struggle receded into the background. Covert resistance, primarily within individual texts, which had previously been insignificant, became central. This resistance used joint secret codes common to writers and readers (allusions, irony, parodies, and other such devices). The struggle continued in the name of a neutral metalanguage that is not ideologised. Resistance criticism, so to speak, took shape: keeping silent about negative assessments that could potentially have provided the basis for political accusations, and keeping silent about secret codes in texts that the authorities did not have to know about.
   The struggle for words and concepts without ideological connotations at the level of phenomena that were ideologically important for the Soviet regime was a continuing theme: the Republic of Estonia, the blue, black and white colour combination, expatriates, deportation, and other such concepts.
SN 1406-3859
EI 2228-3897
PY 2018
IS 2-3
BP 225
EP 251
DI 10.12697/AA.2018.2-3.06
UT WOS:000469068000006
ER

PT J
AU Harris, S
AF Harris, Stefanie
TI Dis-orienting photography: Making, reading, exhibiting images in Peter
   Henisch's Die 'Kleine Figur Meines Vaters' (2003)
SO MODERN AUSTRIAN LITERATURE
AB Peter Henisch's prose work and biography of his father, Die kleine Figur meines Vaters, has appeared in several revised editions since its original publication in 1975. The book has largely been hailed as paradigmatic of the so-called Vaterliteratur of the 1970s and 1980s, in which the men and women of the generation born in the last years of or shortly after World War 11 questioned the actions of their German and Austrian parents during the conflict. Henisch's own father was a photographer in one of Goebbels's propaganda units, and although he always steadfastly denied any political affiliation, even after the war Henisch senior held fast to his heroic past as a war photographer, carefully archiving his images, even assembling them into albums. Through a series of taped interviews with his dying father, combined with his father's letters and his own childhood memories, Henisch reconstructs the identity of his father in order to, as he states, confirm and distinguish his own identity.
   In subsequent editions in 1980 and in 1987, the text of the novel was reworked, including the addition and deletion of scenes, the re-ordering of passages, and alterations in word choice and emphasis (through the author's use of capital letters). In her translation of Henisch's text into English, Negatives of My Father (1990), based on the 1987 edition of the book, Anne Close Ulmer asserts that Henisch's revisions were motivated by the author's desire to adjust the tone of the work, and especially to sharpen the son's criticism of his father (187-88). The reworking of the text in the 1980s thus makes less ambiguous Henisch junior's stance toward his father's role in the Nazi war machine and, not least, his father's continued pride in his former profession as producer of images of propaganda. His father long-since dead, these revisions were directed to the contemporary reader, asking her to take account of and recognize Austria's continued neglect of its Nazi past (especially in the context of the Waldheim era). In other analyses, Henisch's revisions have been described as a methodological principle. For example Kathy Brzovic has argued that Henisch's constant reworking of the text testifies to the open-endedness of life more generally: "The act of composition is endless insofar as there is no end to the 'material,' to the very stuff of life and literature, to the ongoing human confrontation with the Self and the World that comes with the knowledge of good and evil" (47).
   Although Ulmer's and Brzovic's arguments are legitimate, a signal change occured with the 2003 edition of Die kleine Figur meines Vaters. Henisch's most recent edition of the work reveals that more is at stake than creating a balanced portrayal of his father or reminding the reader of crimes of the past, for the new book departs from the earlier editions not only in its textual reworking but, significantly, in the inclusion of reprints of a selection of his father's photographs. In addition to a brief new foreword by the author for this edition, a photograph now marks the beginning of each of the three sections of the work, and twenty-six photographs are collected in an appendix under an epigraph quoting Walter Henisch: "Fur mich war das Fotografieren immer die Hauptsache. Alles andere war nebensachlich" (25 1). This revision of the text results in a work that not only describes a photographer and photographic practices (and this often in distinction to a defined literary method), but also enacts complex image-text relationships within the pages of the work. As Henisch writes in his preface, although he had intended to make only small, textual revisions, "Da tauchten die Bilder wieder auf, vorerst die Bilder in meinem Kopf. Dauber hinaus aber eine Anzahl all die Jahre fur verloren gehaltener Fotos. Die nicht ohne Ruck-wirkung auf den Text bleiben konmen" (6). The publication of the photographs as a visual element of the text (as opposed to their previous inclusion only as verbal description), as well as the heavy emphasis on media in the textual revisions for the 2003 edition, reveal that the emphasis of the work is now primarily on the urgent question of specific media practices and representational strategies.' In other words if, as Ulmer has argued, the earlier revisions were in part a strategy to intensify the critique of the father, and thus in accord with the interrogational and confrontational attitude of much Vaterliteratur, this latest revision shifts its address to future generations, asking them to reflect on recording processes themselves and the vast archive from which we construct our understanding of the past.
   In this essay I consider this shift of emphasis in Henisch's work through an examination of the role of the photographic images in the book and a consideration of the textual modifications that draw our attention to the construction of social and political reality through verbal and visual media. The new edition of Peter Henisch's novel coincided with the staging of an exhibition of Walter Henisch's photographs by Images in Peter Henisch's Die kleine Figur meines Vaters
SN 0026-7503
PY 2007
VL 40
IS 3
BP 73
EP +
UT WOS:000249676800005
ER

PT J
AU Feng, ZB
   Zhou, Y
   Jiang, R
   Han, XQ
   Xu, XY
   Liu, B
AF Feng Zebin
   Zhou Yi
   Jiang Rui
   Han XiaoQuan
   Xu Xiangyu
   Liu Bin
TI Recognition of Energy Model of Excimer Laser by Gate Recurrent Unit
SO CHINESE JOURNAL OF LASERS-ZHONGGUO JIGUANG
AB Objective Excimer lasers are widely used in industrial, medical, and scientific fields because of their short wavelength, high power, and narrow line width. Especially rare gas halogen excimer laser, because of its high peak output power, high single pulse energy, and ultraviolet wavelength, has become the main laser source in the semiconductor lithography industry. Its energy is one of the three key parameters (energy, linewidth, and wavelength) of excimer laser for photolithography, which directly determines the processing accuracy, yield, and key dimensions of semiconductor lithography. When studying the energy of an excimer laser, the closer the model approaches the actual law of light output energy, the more conducive to the study. The output energy model of an excimer laser is the basis for studying and controlling the energy characteristics of the laser. Discharge process of excimer laser is a complex nonlinear process, which leads to the accuracy of laser discharge energy model based on discharge dynamics is difficult to meet the needs of simulation research and control algorithm design. In this paper, the method based on deep learning was applied to identify the energy mode of excimer laser to avoid the inaccuracy of theoretical modeling.
   Methods The development of deep learning theory has become more and more complete. It has become a tool and has been widely applied. Among them, recurrent neural network (RNN) is an important branch in the field of deep learning. It has been widely used in language recognition, machine translation, text analysis and other fields. In recent years, circulating neural networks abroad, especially its variant gate recurrent unit (GRU), has been applied to model recognition, trend prediction and other fields. In this paper, the gated recurrent unit network was used to identify the discharge energy model of the excimer laser. Firstly, based on the characteristics of the excimer laser energy, the discharge voltage and discharge interval were selected as the input of the established gating recurrent unit network. Then, according to the characteristics of the gated recurrent unit network and the excimer laser energy, a neural network suitable for energy model identification of excimer laser was established. When using the GRU network to identify the laser light energy model, a burst pulse energy sequence was used as a time sequence. Finally, the back propagation through time (BPTT) was used to train the established GRU network.
   Results and Discussions Using GRU to learn the energy model of excimer laser requires a lot of data. The data was taken from a KrF excimer laser that produces laser of 248 nm, which worked at a repetition frequency of 4 kHz. Since the wavelength of the excimer laser also affects the energy data, in the course of the experiment, the wavelength was controlled at 248.327 nm using feedback technology. Energy data of the laser was collected under discharge high voltages of 1400 V, 1450 V, 1550 V, and 1600 V, respectively. In order to make full use of the data, at each training, the data under different discharge voltages was randomly selected to train GRU. The termination condition was set as 100000 trainings or the maximum error less than 0.15 mJ. The maximum error of model under each high voltage was less than 0.15 mJ (Fig. 6). Since the energy center value was 10 mJ, the relative error was less than 1.5%. The change of the maximum error in the training process indicates that the GRU neural network converges during the training process (Fig. 7). The data outside the training set was used to validate the model. The model obtained by training was used to calculate the laser light energy when the high voltage was 1550 V, and the comparison between the obtained energy value and the energy value collected on the actual laser after processing (1) is shown in Fig. 8. The energy obtained through the GRU neural network has a good coincidence with the energy of the actual pulse. Another verification data set was collected at laser working with repetition frequency of 1, 2, 3, and 4 kHz. The maximum error between the model data and the actual laser data was less than 0. 13 mJ under different repetition frequencies, that is, the relative error was less than 1.5% (Fig. 10).
   Conclusions The energy model of excimer laser is a complex nonlinear model, which is difficult to get an accurate model from the theory. However, the actual research and application work need an accurate laser output energy model. In this paper, through the method of deep learning, GRU neural network was to identify the energy model. The verification results show that the maximum error between the pulse energy generated by the laser energy model identified by GRU neural network and the actual energy was less than 1.5 Yo. The maximum error 1.5 Yo is less than 2. 74% of the required energy stability in dose control, which meets the simulation requirements of the model control effect. This method can accurately identify the laser energy model. Using the identified model can be more convenient for the simulation of energy control algorithm, so as to improve the energy stability control and dose accuracy control of excimer laser.
SN 0258-7025
PD MAY
PY 2021
VL 48
IS 9
AR 0901004
DI 10.3788/CJL202148.0901004
UT WOS:000686546000004
ER

PT J
AU Dai, YY
   Yu, D
   Li, YH
   Chen, AM
   Jin, MX
AF Dai Yuyin
   Yu Dan
   Li Yinghua
   Chen Anmin
   Jin Mingxing
TI Effect of Different Numbers of Spatial Confinement Walls on
   Laser-Induced Cu Plasma Spectra
SO CHINESE JOURNAL OF LASERS-ZHONGGUO JIGUANG
AB Objective Laser-induced breakdown spectroscopy is a powerful spectroscopy technique for the analysis of various materials. As we all know, laser pulse ablates the sample and generates plasma, and the shock wave generated by the plasma propagates at supersonic speed. When the shock wave encounters the wall of confinement cavity, it will be reflected. The reflected shock wave compresses the plasma, increasing the collision rate of particles in the plasma. It will increase the number of atoms in the high-energy state and enhance the plasma's spectral intensity. As a result, many researchers have conducted extensive research on spatial confinement. However, no research group has yet investigated the effect of the number of spatial confinement walls on the spectra of laser-induced plasmas. For this reason, it is necessary to compare and study the plasma spectral emission characteristics for the different spatial confinement walls.
   Methods We focused nanosecond laser on the surface of the Cu target to generate plasmas, and analysed the spectra of the generated plasmas. To avoid laser irradiation at the same position on the target surface, the Cu target was placed on a 3D translation stage and moved along with it. The focusing lens collected the plasma emission signal, which was then transmitted via optical fibre to a spectrometer equipped with an ICCD. To ensure time synchronisation between the laser and spectral signals, the output laser triggered the photodiode. The cavities with different spatial confinement walls (2, 3, 4, and cylindrical walls) were machined using aluminium alloy. Each spectrum was an average of 20 laser shots, and the whole experiment processes were carried out in the air.
   Results First, the spectra for different numbers of confinement walls at a delay time of 12.5 mu s were compared. The results show that the number of confinement walls has a significant enhancement effect on the intensity of the three Cu (I) spectral lines at a delay time of 12.5 mu s. Second, the time-resolved spectral intensity of Cu (I) at 521.82 nm was measured for various numbers of confinement walls. Within the acquisition delay range of 9-22 mu s, the intensities of the Cu (I) for the number of confinement walls of 2, 3, 4, and cylindrical wall are stronger than those for the number of confinement wall of 0 (Fig. 3). The laser irradiates on the Cu target surface at the centre of the confinement cavity to generate plasma, and the shock wave generated by the Cu plasma rapidly expands (the expansion speed of the shock wave is much faster than the diffusion speed of the plasma). During the plasma expansion process, the shock wave will be reflected by the cavity wall, and the reflected shock wave will compress the Cu plasma to a smaller volume. Because energy cannot be rapidly diffused in plasma, it is absorbed by low-level atoms and transitions to the high-level, causing the intensity of the Cu (I) line to increase within the acquisition delay range of 9-22 mu s. Third, the comparison of best enhancement factors of Cu (I) at 521.82 nm for the different numbers of confinement walls at a delay time of 12.5 mu s was analyzed. It is found that when the confinement cavity is the cylindrical wall, the best enhancement factor of Cu (I) at 521.82 nm is the highest (Fig. 4). At this point, the shock wave will be reflected by the cylindrical wall, resulting in increased shock wave energy and a greater degree of coupling between the shock wave and plasma plume (Fig. 5). Fourth, the time-resolved SBR of Cu (I) at 521.82 nm for various confinement walls was measured. It is found that when the confinement cavity is the cylindrical wall, Cu (I) at 521.82 nm has the maximum SBR (Fig. 6). Finally, the time-resolved plasma temperature for the different numbers of confinement walls was calculated by the Boltzmann diagram method. Plasma temperature changes are similar to changes in spectral intensity and SBR, and the electron temperature with a confinement cavity of a cylindrical wall is the highest (Fig. 7). As previously stated, when the confinement cavity is the cylindrical wall, the spectral intensity, SBR, and electron temperature are the highest; that is to say, the spatial confinement effect is the best at this time.
   Conclusions In this paper, the influence of the number of spatial confinement walls on laser-induced Cu plasma spectra was studied in an atmospheric environment. The experiment discovered that the spectral intensity, SBR, and electron temperature of Cu plasma increased with increasing spatial confinement walls; when the confinement cavity was the cylindrical wall, the spectral intensity, SBR, and electron temperature of the plasma were the highest. The spatial confinement effect resulted from the shock wave reflected from the confinement cavity wall compressing the plasma plume. As the number of confinement walls increased, the energy of the shock wave used to confine the plasma continued to grow, and the coupling degree of the shock wave and plasma plume was also increasing, resulting in the continuous enhancement of the compression effect of the plasma. In conclusion it can be seen that a sufficient number of spatial confinement walls can effectively increase the spectral intensity, SBR, and electron temperature, there by improving spectral signal and sensitivity.
SN 0258-7025
PD JUN
PY 2022
VL 49
IS 6
AR 0611001
DI 10.3788/CJL202249.0611001
UT WOS:000833974700009
ER

PT J
AU Zadeh, LA
AF Zadeh, Lotfi A.
TI Is there a need for fuzzy logic?
SO INFORMATION SCIENCES
AB "Is there a need for fuzzy logic?" is an issue which is associated with a long history of spirited discussions and debate. There are many misconceptions about fuzzy logic. Fuzzy logic is not fuzzy. Basically, fuzzy logic is a precise logic of imprecision and approximate reasoning. More specifically, fuzzy logic may be viewed as an attempt at formalization/mechanization of two remarkable human capabilities. First, the capability to converse, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, conflicting information, partiality of truth and partiality of possibility - in short, in an environment of imperfect information. And second, the capability to perform a wide variety of physical and mental tasks without any measurements and any computations [L.A. Zadeh, From computing with numbers to computing with words - from manipulation of measurements to manipulation of perceptions, IEEE Transactions on Circuits and Systems 45 (1999) 105-119; L.A. Zadeh, A new direction in AI - toward a computational theory of perceptions, AI Magazine 22 (1) (2001) 73-84]. In fact, one of the principal contributions of fuzzy logic - a contribution which is widely unrecognized - is its high power of precisiation.
   Fuzzy logic is much more than a logical system. It has many facets. The principal facets are: logical, fuzzy-set-theoretic, epistemic and relational. Most of the practical applications of fuzzy logic are associated with its relational facet.
   In this paper, fuzzy logic is viewed in a nonstandard perspective. In this perspective, the cornerstones of fuzzy logic and its principal distinguishing features - are: graduation, granulation, precisiation and the concept of a generalized constraint.
   A concept which has a position of centrality in the nontraditional view of fuzzy logic is that of precisiation. Informally, precisiation is an operation which transforms an object, p, into an object, p*, which in some specified sense is defined more precisely than p. The object of precisiation and the result of precisiation are referred to as precisiend and precisiand, respectively. In fuzzy logic, a differentiation is made between two meanings of precision - precision of value, v-precision, and precision of meaning, m-precision. Furthermore, in the case of m-precisiation a differentiation is made between mh-precisiation, which is human-oriented (nonmathematical), and mm-precisiation, which is machine-oriented (mathematical). A dictionary definition is a form of mh-precisiation, with the definiens and definiendum playing the roles of precisiend and precisiand, respectively. Cointension is a qualitative measure of the proximity of meanings of the precisiend and precisiand. A precisiand is cointensive if its meaning is close to the meaning of the precisiend.
   A concept which plays a key role in the nontraditional view of fuzzy logic is that of a generalized constraint. If X is a variable then a generalized constraint on X, GC(X), is expressed as X isr R, where R is the constraining relation and r is an indexical variable which defines the modality of the constraint, that is, its semantics. The primary constraints are: possibilistic, (r = blank), probabilistic (r = p) and veristic (r = v). The standard constraints are: bivalent possibilistic, probabilistic and bivalent veristic. In large measure, science is based on standard constraints.
   Generalized constraints may be combined, qualified, projected, propagated and counterpropagated. The set of all generalized constraints, together with the rules which govern generation of generalized constraints, is referred to as the generalized constraint language, GCL. The standard constraint language, SCL, is a subset of GCL.
   In fuzzy logic, propositions, predicates and other semantic entities are precisiated through translation into GCL. Equivalently, a semantic entity, p, may be precisiated by representing its meaning as a generalized constraint.
   By construction, fuzzy logic has a much higher level of generality than bivalent logic. It is the generality of fuzzy logic that underlies much of what fuzzy logic has to offer. Among the important contributions of fuzzy logic are the following:
   1. FL-generalization. Any bivalent-logic-based theory, T, may be FL-generalized, and hence upgraded, through addition to T of concepts and techniques drawn from fuzzy logic. Examples: fuzzy control, fuzzy linear programming, fuzzy probability theory and fuzzy topology.
   2. Linguistic variables and fuzzy if-then rules. The formalism of linguistic variables and fuzzy if-then rules is, in effect, a powerful modeling language which is widely used in applications of fuzzy logic. Basically, the formalism serves as a means of summarization and information compression through the use of granulation.
   3. Cointensive precisiation. Fuzzy logic has a high power of cointensive precisiation. This power is needed for a formulation of cointensive definitions of scientific concepts and cointensive formalization of human-centric fields such as economics, linguistics, law, conflict resolution, psychology and medicine.
   4. NL-Computation (computing with words). Fuzzy logic serves as a basis for NL-Computation, that is, computation with information described in natural language. NL-Computation is of direct relevance to mechanization of natural language understanding and computation with imprecise probabilities. More generally, NL-Computation is needed for dealing with second-order uncertainty, that is, uncertainty about uncertainty, or uncertainty(2) for short.
   In summary, progression from bivalent logic to fuzzy logic is a significant positive step in the evolution of science. In large measure, the real-world is a fuzzy world. To deal with fuzzy reality what is needed is fuzzy logic. In coming years, fuzzy logic is likely to grow in visibility, importance and acceptance. (c) 2008 Elsevier Inc. All rights reserved.
RI Zadeh, Lotfi A./A-6147-2012
SN 0020-0255
EI 1872-6291
PD JUL 1
PY 2008
VL 178
IS 13
BP 2751
EP 2779
DI 10.1016/j.ins.2008.02.012
UT WOS:000256323900001
ER

PT J
AU Jiang, ZC
   Zhang, YQ
   Hu, BA
   Liu, XL
   Yu, QF
AF Jiang Zhuocan
   Zhang Yueqiang
   Hu Biao
   Liu Xiaolin
   Yu Qifeng
TI A Calibration Method for Extrinsic Parameters of Monocular Laser Speckle
   Projection System
SO ACTA OPTICA SINICA
AB Objective Laser speckle projection systems have been widely used in various fields, including but not limited to three-dimensional (3D) reconstruction, industrial detection, and gesture recognition. According to the number of infrared cameras, laser speckle projection systems are generally divided into two categories: the binocular mode and the monocular mode. A binocular laser speckle projection system consists of a laser speckle projector and two infrared cameras. The feature information provided by random speckle patterns is sufficient to match images in textureless areas, which significantly improves the accuracy and stability of binocular stereo vision systems. Moreover, speckle patterns in the infrared spectrum minimize the impact of the ambient light. However, the cost of binocular laser speckle projection systems is typically high, and the calibration process is complex. Compared with their binocular counterparts, monocular laser speckle projection systems are more compact and cost-effective. Due to the lack of reference speckle patterns, monocular laser speckle projection systems generally use a precise range finder to capture speckle images at different standard distances in advance. The measurement process is complex, and the deviation of the optical axis cannot be corrected online. To solve the aforementioned problems, this paper proposes a calibration method for the extrinsic parameters of monocular laser speckle projection systems. The virtual speckle image of the projector is generated by calculating the pose relationship between the infrared camera and the laser speckle projector. Only a calibration board with corner features is required in the proposed calibration process, rather than the precise range finder. With this method, a monocular laser speckle projection system becomes equivalent to a binocular stereo vision system with speckle images.
   Methods First, a simple calibration board with corner features is designed. These features only occupy a small part of the calibration board, which leaves sufficient area for the speckle pattern. The plane equation of the calibration board in the camera coordinate system is calculated by extracting the coordinates of corner features in the image. Then, the laser speckle projector projects a random speckle pattern to the calibration board in different poses, and the infrared camera captures speckle images. Next, the digital image correlation (DIC) method is utilized to determine the corresponding speckle points in different speckle images. According to the plane equations of the calibration board, those speckle points are projected to corresponding planes, whose 3D coordinates can be obtained in the camera coordinate system. The straight lines fitted by corresponding speckle points pass through the center of the laser transmitter in the projector, which is regarded as the optical center of the projector. Therefore, the optical center and axis of the projector in the camera coordinate system are estimated by fitting corresponding lines. Finally, the pose relationship between the camera and the projector is solved and optimized. The virtual speckle image of the projector is generated by constructing the equation of planar homography. Through the aforementioned process, a monocular laser speckle projection system can be equivalent to a binocular stereo vision system with speckle images.
   Results and Discussions To verify the feasibility and accuracy of the proposed method, this paper establishes a monocular laser speckle projection system (Fig. 5). Firstly, the extrinsic parameters of the monocular laser speckle projection system are calibrated according to the aforementioned process. Then, the corresponding speckle points in the camera images are projected to the virtual imaging plane of the projector, and the offset error of corresponding points is calculated. The average offset errors mainly vary from 0. 10 pixel to 0. 18 pixel. The projection points of corresponding speckle points on the virtual imaging plane tend to be one point. The result shows that the calibration accuracy of the rotation matrix and the translation vector is high. Next, fourteen displacement experiments are further conducted in the range of 2-9 mm. The measured curve is basically consistent with the ideal curve (Fig. 7). The measurement errors of displacement are less than 0. 16 mm. Furthermore, we conduct the 3D reconstruction experiment of standard spheres with known geometric parameters. The radius measurement errors of the two standard spheres are 0. 0916 mm and 0. 1274 mm, respectively. Their root-mean-square (RMS) errors are less than 0. 1346 mm (Table 1). Finally, the ORBBEC's Astra-Pro is selected to demonstrate the depth measurement accuracy of the proposed method. Regardless of the average offset error or the depth distribution range, the plane reconstruction results of the proposed method are significantly better than those of Astra-Pro ( Table 2). Simultaneously, the depth variation of the target model reconstructed by the method is smoother, and its density of point cloud is larger (Fig. 11). Hence, it can be easily concluded that the proposed method is able to calibrate the monocular laser speckle projection system effectively and achieve high-precision depth measurement.
   Conclusions In this paper, a simple and efficient calibration method for the extrinsic parameters of monocular laser speckle projection systems is proposed. The 3D coordinates of the corresponding speckle points are calculated by adjusting the pose of the calibration board. Then, the relationship between the infrared camera and the laser speckle projector is solved and optimized to generate the virtual speckle image of the projector. The pose relationship of the monocular laser speckle projection system can be easily calibrated with the help of a calibration board with corner features, which improves calibration efficiency and reduces calibration costs. Generating the virtual speckle images of the projector enables the monocular laser speckle projection system to be equivalent to a binocular stereo vision system with speckle images, which significantly improves depth measurement accuracy. Simultaneously, the deviation of the optical axis can be corrected online. The experimental results show that the measurement errors of displacement and sphere radii are less than 0. 16 mm and 0. 13 mm, respectively. Within a certain depth range, the reconstruction results of the proposed method are significantly better than those of Astra-Pro. The proposed method can well improve the calibration efficiency and depth measurement accuracy of monocular laser speckle projection systems.
SN 0253-2239
PD FEB
PY 2023
VL 43
IS 3
AR 0315001
DI 10.3788/AOS221451
UT WOS:000934651300019
ER

PT J
AU Tang, JL
   Xu, XW
   Chen, TX
   Gao, N
   Cao, JW
   Li, L
AF Tang Jilong
   Xu Xiongwei
   Chen Tianxiang
   Gao Na
   Cao Jiewei
   Li Lin
TI Preparation of Self-supporting Al Filter
SO ACTA PHOTONICA SINICA
AB The self-supporting metal film is very important in the observation of X-ray and extreme ultraviolet bands. When the X-ray and extreme ultraviolet telescopes are used to observe the corresponding bands, the detector is usually disturbed by visible and infrared light and other stray light, they affect its detection accuracy and performance. Therefore, it is necessary to add filters in front of the detector, the filters are usually hundreds of nanometers of metal film. The preparation of self-supporting thin films is usually deposited on some special substrates, and then the substrate is removed to obtain the required self-supporting thin films. At present, there are two main methods for obtaining self-supporting films, one is substrate etching; the other is the release agent. The substrate etching process is complex, and the method of release agent is simple and easy to implement. Therefore, this study uses the method of release agent to prepare the self-supporting Al filter. The release agent used in the past is very easy to dissolve, such as NaCl, CsI, etc., and they are easy to bring defects to the film deposited on the release agent. In this paper, AZ50XT photoresist and polyvinyl alcohol are used as release agents. They have good film-forming properties, stable performance, and are good release agent materials. To make the self-supporting Al film have a regular shape and easy to test, it is necessary to prepare a nickel supporting frame. We copied the nickel frame pattern on clean Si wafer by two mask lithography, and then prepared the nickel frame by the micro-electroforming process. After preparation, the nickel frame was removed for use. The thickness of the nickel frame is about 60 similar to 70 mu m, the inner diameter is 16 mm, the outer diameter is 28 mm. The interface between the nickel frame and the sample is smooth and easy to bond. In the experiment, AZ50XT photoresist and polyvinyl alcohol release layers were prepared on clean Si substrate by spin coater, and then dried on the heating table. After the sample was cooled to room temperature, placed it in the magnetron sputtering coating machine with background vacuum of 5.0 / 10(-4) Pa to deposit Al film, the deposition thickness was 80 nm. The thickness of Al film was monitored by crystal oscillation film thickness meter, and the step measuring instrument was used to verify whether the thickness of Al film deposited was 80 nm. The test results show the average thickness of Al film is 80.75 nm, it was within the allowable error range. After the deposition of Al film was completed, the nickel frame has adhered to the surface of Al film by epoxy resin adhesive. After the epoxy resin adhesive was completely cured , the sample was put into acetone or deionized water for release, the self-supporting Al filter was obtained. The defects and pinholes on the surface of the filter were analyzed by scanning electron microscopy and a CMOS camera. The analysis results showed that the prepared Al filter surface was uniform and dense, with only a few pinholes. The optical properties of the prepared filter were characterized by Ultraviolet-visible spectrophotometer, soft X-ray transmittance test system and synchrotron radiation device.
   The transmittance of Al filters prepared by two kinds of release agents is higher than 0.02% in the ultraviolet band, and lower than 0.02% in the visible and infrared bands, they basically meet the requirements of use. The transmittance of Al filter prepared by AZ50XT photoresist is lower than that prepared by polyvinyl alcohol, because the preparation process of photoresist is advanced and the uniformity is better, the surface of the release layer prepared with it is flat and smooth, and there is almost no pinhole on the surface of Al film. The polyvinyl alcohol solution can produce gel during the preparation process, and there is a little gel residue even after repeated filtration, therefore, the prepared release layers have defects, resulting in a few pinholes on the surface of Al film. Compared with the polyimide-aluminum filters commonly used in space X-ray detection, the self-supporting single-layer Al filter has better ability to suppress visible and infrared light. In the 200 similar to 400 nm band, the Al filter without polyimide support has higher transmittance to ultraviolet light, so it is more conducive to extreme ultraviolet detection. The soft X-ray transmittance test system is built by our laboratory, the system consists of an X-ray light tube (target is Ag, window is Be) , a vacuum chamber, a translation stage and an SDD detector. Under the given tube voltage and tube current, the transmittance of the film in the soft X-ray band is characterized by measuring the ratio of the transmitted light intensity to the incident light intensity. Due to the existence of Be window in the X-ray tube, low-energy photons are shielded, and no low-energy photons are emitted from the X-ray tube. Therefore, the energy section below 1.6 keV can be ignored. The transmittance of the filter is higher than 90% in the energy range of 1.6 similar to 10 keV. In order to verify the reliability of the test results, we compare the test results with the theoretical calculation results. The transmittance curve is consistent with the theoretical results, which meets the application requirements. Finally, the transmittance of the filter in the energy range of 50 similar to 250 eV was measured by synchrotron radiation device. The test results show the highest transmittance of the prepared two filters in this energy range can reach 53% and 35%, respectively. Due to the oxidation of the Al filter surface and the residue of the release agent, the actually measured transmittance is much lower than the theoretical value.
SN 1004-4213
PD JUN
PY 2022
VL 51
IS 6
AR 0631001
DI 10.3788/gzxb20225106.0631001
UT WOS:000828597800040
ER

PT J
AU [Anonymous]
AF [Anonymous]
TI Research on multimorbidity in primary care. Selected abstracts from the
   EGPRN meeting in Tampere, Finland, 9-12 May 2019 All abstracts of the
   conference can be found at the EGPRN website:
   www.egprn.org/page/conference-abstracts
SO EUROPEAN JOURNAL OF GENERAL PRACTICE
AB Current primary care in Finland is based on the Primary Health Care Act (1972), which addressed numerous new tasks to all municipalities. All of them had to find a new health centre organization, which provides a wide range of health services, including prevention and public health promotion. Multiple tasks require multiprofessional staff, and thus, the Finnish health centre personnel consisted not only of GPs but of public health nurses, midwives, physiotherapists, psychologists, social workers, dentists, etc. During the next decade, there have been some changes but the idea of multiprofessional structure has remained. According to the QUALICOPC study (2012) Finnish GPs are still co-located with several other healthcare professionals compared to most of the European countries; even compared to other Nordic countries which otherwise have many similarities in their primary healthcare. During the last 10 or 15 years, healthcare providers and researchers have recognized a new challenge: our current systems do not meet the needs of patients with multiple health and social problems-and the proportion of these patients is increasing all the time as the population is getting older. One could suppose that preconditions of handling multimorbidity would be excellent in multiprofessional surroundings like ours, but actually, a person with multiple problems is a challenge there, too. Multiprofessional organization in primary care does not guarantee proper care of patients with multiple diseases, if we do not acknowledge the challenge and revise our systems. We have to develop new ways of collaboration and new models of integrated care. The problematic part is secondary care, which is organized with logic of one medical speciality per visit. In Tampere University Hospital district, we have created a care pathway model, which defines the roles of primary healthcare and secondary care. Nationwide, we have recently started to prepare national guidelines for the care of patients with multimorbidity. What we need more in the future is more research on new practices and models.
   Background: Most patients with antihypertensive medication do not achieve their blood pressure (BP) target. Several barriers to successful hypertension treatment are well identified but we need novel ways of addressing them. Research question: Can using a checklist improve the quality of care in the initiation of new antihypertensive medication? Methods: This non-blinded, cluster-randomized, controlled study was conducted in eight primary care study centres in central Finland, randomized to function as either intervention (n = 4) or control sites (n = 4). We included patients aged 30-75 years who were prescribed antihypertensive medication for the first time. Initiation of medication in the intervention group was carried out with a nine-item checklist, filled in together by the treating physician and the patient. The treating physician managed hypertension treatment in the control group without a study-specific protocol. Results: In total, 119 patients were included in the study, of which 118 were included in the analysis (n = 59 in the control group, n = 59 in the intervention group). When initiating medication, an adequate BP target was set for 19% of the patients in the control group and for 68% in the intervention group. Shortly after the appointment, only 14% of the patients in the control group were able to remember the adequate BP target, compared with 32% in the intervention group. The use of the checklist was also related to more regular agreement on the next follow-up appointment (64% in the control group vs 95% in the intervention group). Conclusion: Even highly motivated new hypertensive patients in Finnish primary care have significant gaps in their treatment-related skills. The use of a checklist for initiation of antihypertensive medication was related to substantial improvement in these skills. Based on our findings, the use of a checklist might be a practical tool for clinicians initiating new antihypertensive medications.
   Background: Immediate feedback is underused in the French medical education curriculum, specifically with video-recorded consultation. Research question: The objective of this study was to evaluate the feasibility and the interest in this teaching method as a training and assessment tool in the learning process of general practitioner (GP) trainees. Methods: During the period November 2017 to October 2018, trainees in ambulatory training courses collected quantitative data about recording consultations with a video camera: numbers of recordings, feedback, patients' participation refusals, and information about the learning process and competencies. The trainees' level of satisfaction was measured by means of a questionnaire at the end of their traineeship. Results: Sixty-seven trainees were recruited and 44 of them 65.7% actively participated in the study; 607 video recordings and 243 feedback with trainers were performed. Few patients (18.5%) refused the video-recording. Most trainees considered video recording with immediate feedback to be a relevant learning tool. It made it possible for the participants to observe their difficulties and their achievements. 'Relation, communication, patient-centred care' was the most built competency, non-verbal communication, in particular. Time was the main limiting factor of this teaching method. Most trainees were in favour of its generalization in their university course. Conclusion: Video recording with immediate feedback in real-time consultation needs to be adapted to training areas and depends on time and logistics. This teaching method seems to be useful in the development of communication skills. It could lift the barriers of the trainer's physical presence near GP trainees during immediate feedback in real-time consultation. It could help trainees to build their competencies while enhancing the place of immediate feedback in the general practice curriculum. It could also constitute an additional tool for the certification of GP trainees.
   Background: Perinatal depression has been associated with psychiatric morbidity in mothers and their offspring. This study assessed the prevalence of perinatal depressive symptoms in a large population of women and investigated associations of these symptoms with demographic and clinical factors. Research question: Which factors (including sociodemographic, medical, lifestyle, and laboratory test) are associated with perinatal depression? Methods: All members of Maccabi Health Services who completed the Edinburgh Postnatal Depression Scale (EPDS) during 2015-2016 were included in the study. Odds ratios (ORs) were calculated for associations of sociodemographic, medical, lifestyle, and laboratory test factors with perinatal depressive symptoms, according to a score >10 on the EPDS. Results: Of 27 912 women who filled the EPDS, 2029 (7.3%) were classified as having peripartum depression. In a logistic regression analysis, the use of antidepressant medications, particularly for a period greater than three months, Arab background, current or past smoking, a diagnosis of chronic diabetes and age under 25 years were all associated with increased ORs for perinatal depression; while Orthodox Jewish affiliation, residence in the periphery and higher haemoglobin level were associated with lower ORs. Incidences of depression were 17.4% in women with a history of antidepressant medication, 16% among women with diabetes, and 11.8% among current smokers. Conclusion: Several demographic, medical, and lifetime factors were found to be substantially more prevalent among women with symptoms of perinatal depression than those without. Encouraging women to complete the EPDS during and following pregnancy may help identify women in need of support.
   Background: Regulating the quality and effectiveness of the work of general practitioners is essential for a sound healthcare system. In the Republic of Macedonia this is regulated by the Health Insurance Fund through a system of penalties/sanctions. Research question: The goal of this study is to evaluate the types and effectiveness of the sanctions used on primary care practitioners. Methods: This is a quantitative research study for which we used an anonymous survey with 18 questions. This survey was distributed to 443 randomly selected general practitioners from different parts of Macedonia and 438 of them responded. For the quantitative data, we used the Pearson's chi-squared test, correlation and descriptive statistics. Part of the survey is qualitative, consisting of comments and opinions of the general practitioners. Results: From the participants, 336 were female and 102 were male. The doctors' gender was not associated with sanctioning. Most general practitioners were in the age categories of 30-39 and 40-49 years. The participants' age had a significant influence on sanctioning-older doctors were sanctioned more frequently. Out of 438 participants, 33.3% were specialists in family medicine and 66.7% general practitioners. Specialists in family medicine were sanctioned significantly more frequently than general practitioners. Doctors that worked in the hospital or 19 km from the nearest hospital were significantly more frequently sanctioned. The three most common reasons for sanctions were financial consumption of prescriptions and referrals above the agreed amount, higher rate of sick leaves and/or justification of sick leaves and unrealized preventative goals or education. 'Financial sanction by scale' was the most common type of sanction: 49.8% of participants. Doctors who followed the guidelines, but who were exposed to violence were sanctioned significantly more frequently. Conclusion: We can observe that age, speciality, the distance of the workplace from the nearest hospital and violence influence sanctioning.
   Background: Biases are major barriers to external validity of studies, reducing evidence. Among these biases, the definition and the reality of the Hawthorne effect (HE) (or observation bias) remains controversial. According to McCambridge in a review from 2013, the Hawthorne effect is a behaviour change occurring when the subject is being observed during a scientific study. This effect would be multifactorial, and he suggests the term 'effects of research participation.' However, the reviewed studies were conflicting and evidence is sparse. Research question: We updated McCambridge's review to actualize the definition of the HE. Methods: McCambridge's most recent article dated back to January 3, 2012. We focused on the articles published between January 1, 2012 and August 10, 2018 searching Medline. We used the sole keyword 'Hawthorne Effect.' The search was filtered based on the dates, the availability of an abstract and the languages English and French. We included articles defining or evaluating the HE. Articles citing the effect without defining it or irrelevant to the topic were excluded. Two independent readers searched and analysed the articles. Discrepancies were solved by consensus. Results: Out of 106 articles, 42 articles were included. All the articles acknowledged an observation bias, considered as significant or not, depending on the population (education, literacy), the methods and the variable of interest. It was a psychological change, limited in time. The HE was defined as a change of behaviour related to direct or indirect observation of the subjects or the investigators, to their previous selection and commitment in the study (written agreement) and to social desirability. Despite observations, articles were conflicting. Some do confirm the existence of the HE, others deny it. Meta-analysis is ongoing. Conclusion: No formal consensus regarding the definition of the effect has been reached so far. However, the authors agree on its implication as an experimental artefact.
   Background: Polypharmacy and multimorbidity are on the rise. Consequently, general practitioners (GPs) treat an increasing number of multimorbid patients with polypharmacy. To limit negative health outcomes, GPs should search for inappropriate medication intake in such patients. However, systematic medication reviews are time-consuming. Recent eHealth tools, such as the 'systematic tool to reduce inappropriate prescribing' (STRIP) assistant, provide an opportunity for GPs to get support when conducting such medication reviews. Research question: Can the STRIP assistant as electronic decision support help GPs to optimize medication appropriateness in older, multimorbid patients with polypharmacy? Methods: This cluster randomized controlled trial is conducted in 40 Swiss GP practices, each recruiting 8-10 patients aged >= 65 years, with >= 3 chronic conditions and >= 5 chronic medications (320 patients in total). We compare the effectiveness of using the STRIP assistant for optimizing medication appropriateness to usual care. The STRIP assistant is based on the STOPP/START criteria (version 2) and, for this trial, it is implemented in the Swiss eHealth setting where some GPs already share routine medical data from their electronic medical records in a research database (FIRE). Patients are followed-up for 12 months and the change in medication appropriateness is the primary outcome. Secondary outcomes are the numbers of falls and fractures, quality of life, health economic parameters, patients' willingness to deprescribe as well as implementation barriers and enablers for GPs when using the STRIP assistant. Results: Patient recruitment started in December 2018. This presentation focuses on the study protocol and the challenges faced when testing this new software in Swiss primary care. Conclusion: Finding out whether the STRIP assistant is an effective tool and beneficial for older and multimorbid patients, who are usually excluded from trials, will have an impact on the coordination of chronic care for multimorbid patients in Swiss primary care in this new eHealth environment.
   Background: Workplace violence (WPV) towards healthcare staff is becoming a common problem in different healthcare settings worldwide. Moreover, the prevalence is 16 times higher than in other professions. How often it happened towards young doctors working as general practitioners (GPs) at the beginning of their careers has been rarely studied. Research question: To investigate the frequency and forms of WPV, experienced by the young Croatian GPs from their patients, and violence reporting pattern to the competent institutions. Methods: The cross-sectional study was carried out on 74 GP residents, during their postgraduate study in family medicine in May 2018. A specially designed anonymous questionnaire, developed by Association of Family Physicians of South Eastern Europe, was used to investigate the prevalence and forms of WPV, the narrative description of the traumatic event itself and the process of reporting it. Results: The response rate was 91.9%, female 87%, the median of years working as a GP was 3.5 years. Most of the residents were working in an urban practice (63%), others in the rural and the suburban once (27%, 10%). All GP residents experienced patients' and caregivers' violent behaviour directed towards them. High-intensity violence (e.g. physical violence, sexual harassment) was experienced by 44%, middle intensity (e.g. intimidation, visual sexual harassment) by 84% while all residents experienced verbal violence. Only 13.2% residents reported WPV to the competent institutions. Most of GP residents reported the appearance of the new form of violence: the one over the internet. Conclusion: The high prevalence of all types of violence towards young Croatian doctors is worrisome, as is the fact that violent acts are seldom reported to the competent institutions. Those alarming facts could become a threat to GPs career choosing.
   Background: About 50% of patients adhere to chronic therapy in France. Improving adherence should improve their care. Identifying the patient's difficulties in taking medication is complex for the physician, because there is no gold standard for measuring adherence to medications. How can the general practitioner in his/her practice identify patient compliance? Research question: Analyse studies that develop or validate scales used to estimate adherence in primary care. Methods: A systematic review of the literature from PubMed, the Cochrane Library and PsycINFO databases. The search terms used were the MeSH terms (or adapted to the database's vocabulary): questionnaire, compliance and primary care. All articles were retained whatever the language of writing. Selection criteria were: assessment of the development, validation or reliability of one or more compliance scales; taking place in primary care. One reviewer screened titles, which included the term adherence then abstracts and full text. Only articles evaluating the development, validity or reliability of a primary care adherence rating scale were included in analysis. Results: In total 1022 articles were selected and 18 articles were included. Seventeen adherence scales were identified in primary care, most of which targeted a single pathology, especially hypertension. The most cited scale is the MMAS Morisky medication adherence scale. Three scales were developed for patients with multiple chronic diseases. One scale was developed for patients older than 65 years-the Strathclyde compliance risk assessment tool (SCRAT)-and two scales were developed for adult patients whatever their age-the instrument developed by Sidorkiewicz et al., and the DAMS, diagnostic adherence to medication scale. Conclusion: Two scales have been developed and validated in primary care to assess patient adherence with multiple chronic diseases: the DAMS and the instrument developed by Sidorkiewicz et al. A simple, reliable, reproducible primary care scale would assess the impact of actions developed to improve adherence: motivational interviewing, patient therapeutic education, and the ASALeE protocol.
   Background: Multimorbidity prevalence increases with age while declining quality of life (QoL) is one of its major consequences. Research question: The study aims to: (1) Assess the relationship between increasing number of diseases and QoL. (2) Identify the most frequently occurring patterns of diseases and how they relate to QoL. (3) Observe how these associations differ across different European countries and regions. Methods: Cross-sectional data analysis performed on wave six of the population-based survey of health, ageing and retirement in Europe (SHARE) (n = 68 231). Data were collected in 2015 among population 50+ years old in 17 European countries and Israel. Multimorbidity is defined as the co-occurrence of two or more chronic conditions. Conditions were self-declared and identified through an open-end questionnaire containing 17 prelisted conditions plus conditions added by participants. Control, autonomy, self-realization and pleasure questionnaire (CASP-12v) was used to evaluate QoL. Association between increasing number of diseases and QoL was assessed with linear regression. Factor analysis is being conducted to identify patterns of diseases to evaluate their impact on QoL further. Multilevel analysis will take into account differences between countries and regions. Confounding was searched with directed acyclic graph (DAG) method and included age, sex, education, socio-economic status, behavioural habits, social support and healthcare parameters. Results: Participants (49.09%) had two or more diseases. Maximum number of diseases per person was 13, mean number was 1.9. Unadjusted preliminary analysis showed that on average QoL decreases by -1.27 (95%CI: -1.29, -1.24) with each added new condition across Europe. The decline appears to be the steepest in Spain, -1.61 (95%CI: -1.71, -1.51), and the least so in Israel, -0.67 (95%CI: -0.82, -0.52). Conclusion: Ongoing analysis will identify disease patterns, which may have the highest impact on QoL, as well as to elucidate the role of confounders in the relationship between increasing number of diseases and disease patterns with QoL.
   Background: The burden and preventive potential of disease is typically estimated for each non-communicable disease (NCD) separately but NCDs often co-occur, which hampers reliable quantification of their overall burden and joint preventive potential in the population. Research questions: What is the lifetime risk of developing any NCD? Which multimorbidity clusters of NCDs cause the greatest burden? To what extent do three key shared risk factors, namely smoking, hypertension and being overweight, influence this risk, life-expectancy and NCD-multimorbidity? Methods: Between 1990 and 2012 we followed NCD-free participants aged >= 45 years at baseline from the Dutch prospective Rotterdam study for incidents of stroke, heart disease, diabetes, chronic respiratory disease, cancer, and neurodegenerative disease. We quantified (co-)occurrence and remaining lifetime risk of NCDs in a competing risk framework, and studied the effects of smoking, hypertension, and being overweight on lifetime risk and life expectancy. Results: During follow-up of 9061 participants, 814 participants were diagnosed with stroke, 1571 with heart disease, 625 with diabetes, 1004 with chronic respiratory disease, 1538 with cancer, and 1065 with neurodegenerative disease. Among those, 1563 participants (33.7%) were diagnosed with multiple diseases. The lifetime risk of any NCD from the age of 45 onwards was 94.0% (95%CI: 92.9-95.1) for men and 92.8% (95%CI: 91.8-93.8) for women. Absence of shared risk factors was associated with a 9.0-year delay (95%CI: 6.3-11.6) in the age at onset of any NCD. Furthermore, overall life expectancy for participants without risk factors was 6.0 years (95%CI: 5.7-7.9) longer than those with these risk factors. Participants without these risk factors spent 21.6% of their remaining lifetime with NCDs, compared to 31.8% for those with risk factors. Conclusion: Nine out of 10 individuals aged 45 years and older will develop at least one NCD during their remaining lifetime. A third was diagnosed with multiple NCDs during follow-up. Absence of three common shared risk factors related to compression of morbidity of NCDs.
   Background: This study examined if using electronic reminders increases the rate of diagnosis recordings in the patient chart system following visits to a general practitioner (GP). The impact of electronic reminders was studied in the primary care of a Finnish city. Research question: How effective is the reminder of the information system in improving the diagnostic level of primary care? Which is better and how: financial incentives or reminders? Methods: This was an observational retrospective study based on a before-and-after design and was carried out by installing an electronic reminder in the computerized patient chart system to improve the recording of diagnoses during GP visits. The quality of the recorded diagnoses was observed before and after the intervention. The effect of this intervention on the recording of diagnoses was also studied. Results: Before intervention, the level of recording diagnoses was about 40% in the primary care units. After four years, the recording rate had risen to 90% (p < 0.001). The rate of change in the recording of diagnoses was highest during the first year of intervention. In the present study, most of the visits concerned mild respiratory infections, elevated blood pressure, low back pain and type II diabetes. Conclusion: An electronic reminder improved the recording of diagnoses during the visits to GPs. The present intervention produced data, which reflects the distribution of diagnoses in real clinical life in primary care and thus provides valid data about the public.
   Background: Child abuse is widespread, occurs in all cultures and communities and remains undiscovered in 90% of the cases. In total, 80% of reported child abuse concerns emotional ill-treatment. In the Netherlands, at least 3% (118 000) of children are victims of child abuse resulting in 50 deaths each year. Only 1-3% of abuse cases are reported by general practitioners (GPs) to the Child Protective Services agency (CPS). To explain this low reporting rate, we examined GPs' experiences with child abuse. Research question: How does the suspicion of child abuse arise in GPs' diagnostic reasoning? How do they act upon their suspicion and what kind of barriers do they experience in their management? Methods: In total 26 GPs (16 female) participated in four focus groups. We used purposive sampling to include GPs with different levels of experience in rural and urban areas spread over the Netherlands. We used NVivo for thematic content analysis. Results: Suspected child abuse arose based on common triggers and a gut feeling that 'something is wrong here'. GPs acted upon their suspicion by gathering more data by history taking and physical examination. They often found it challenging to decide whether a child was abused because parents, despite their good intentions, may lack parenting skills and differ in their norms and values. GPs reported clear signs of sexual abuse and physical violence to CPS. However, in less clear-cut cases they followed-up and built a supporting network around the family. Most GPs highly valued the patient-doctor relationship while recognizing the risk of pushing boundaries. Conclusion: A low child abuse reporting rate by GPs to CPS does not mean a low detection rate. GPs use patients' trust in their doctor to improve a child's situation by involving other professionals.
   Background: The number of people suffering from multiple chronic conditions, multimorbidity, is rising. For society, multimorbidity is known to increase healthcare expenses through more frequent contacts, especially with the primary sector. For the individual, an increasing number of medical conditions are associated with lower quality of life (QoL). However, there is no statistically validated condition-specific patient-reported outcome measure (PROM) for the assessment of QoL among patients with multimorbidity. A validated PROM is essential in order to measure effect in intervention studies for this patient group. Research question: (1) To identify items covering QoL among patients with multimorbidity in a Danish context. (2) To develop and validate a PROM for assessment of QoL among patients with multimorbidity. (3) To utilize the final PROM in a large group of patients with multimorbidity to measure their QoL when living with different combinations and severity of multimorbidity. Methods: Phase 1: qualitative individual and focus group interviews with patients with multimorbidity to identify relevant QoL items. Phase 2: validation of the items through a draft questionnaire sent by email to around 200-400 patients with multimorbidity. Phase 3: psychometric validation of the draft questionnaire securing items with the highest possible measurement quality. Phase 4: assessment of QoL among approximately 2000 patients with multimorbidity from the Danish Lolland-Falster study. Results: There are no results yet. Currently, the interview guide is under development. Conclusion: Despite the rising number of patients with multimorbidity and the known inverse relationship between a patient's number of medical conditions and their quality of life, there is no statistically validated condition-specific PROM for assessment of QoL among this group. Our aim is that this project's developed and validated PROM will be used in future intervention studies as a valid measure of QoL among patients with multimorbidity.
   Background: Through a systematic review of the literature and qualitative research across Europe, the European General Practitioners Research Network (EGPRN) has designed and validated a comprehensive definition of multimorbidity. It is a concept considering all the biopsychosocial conditions of a patient. This concept encompasses more than 50 variables and is consequently difficult to use in primary care. Consideration of adverse outcomes (such as death or acute hospitalization) could help to distinguish which variables could be risk factors of decompensation within the definition of multimorbidity. Research question: Which criteria in the EGPRN concept of multimorbidity could detect outpatients at risk of death or acute hospitalization (i.e. decompensation) in a primary care cohort at 24-months of follow-up? Methods: Primary care outpatients (131) answering to EGPRN's multimorbidity definition were included by GPs, during two periods of inclusion in 2014 and 2015. At 24 months follow-up, the status 'decompensation' or 'nothing to report' was collected. A logistic regression following a Cox model was performed to achieve the survival analysis and to identify potential risk factors. Results: At 24 months follow-up, 120 patients were analysed. Three different clusters were identified. Forty-four patients, representing 36.6% of the population, had either died or been hospitalized more than seven consecutive days. Two variables were significantly associated with decompensation: Number of GPs encounters per year (HR: 1.06; 95%CI: 1.03-1.10, p <0.001), and total number of diseases (HR: 1.12; 95%CI: 1.03-1.33; P = 0.039). Conclusion: To prevent death or acute hospitalization in multimorbid outpatients, GPs may be alert to those with high rates of GP encounters or a high number of illnesses. These results are consistent with others in medical literature.
   Background: A study of casual versus causal comorbidity in family medicine in three practice populations from the Netherlands, Malta and Serbia. Research question: (1) What is the observed comorbidity of the 20 most common episodes of care in three countries? (2) How much of the observed comorbidity is likely to be casual versus causal? Methods: Participating family doctors (FDs) in the Netherlands, Malta and Serbia recorded details of all patient contacts in an episode of care structure using electronic medical records based on the International Classification of Primary Care, collecting data on all elements of the doctor-patient encounter, including the diagnostic labels (episode of care labels, EoCs). Comorbidity was measured using the odds ratio of both conditions being incident or rest-prevalent in the same patient in one-year data frames, as against not. Results: Comorbidity in family practice expressed as odds ratios between the 41 most prevalent (joint top 20) episode titles in the three populations. Specific associations were explored in different age groups to observe the changes in odds ratios with increasing age as a surrogate for a temporal or biological gradient. Conclusion: After applying accepted criteria for testing the causality of associations, it is reasonable to conclude that most of the observed primary care comorbidity is casual. It would be incorrect to assume causal relationships between co-occurring diseases in family medicine, even if such a relationship might be plausible or consistent with current conceptualizations of the causation of disease. Most observed comorbidity in primary care is the result of increasing illness diversity.
   Background: The concept of therapeutic alliance emerged in the beginning of the twentieth century and came from psychoanalysis. This notion was then extended to the somatic field and aims to replace the paternalistic model in the doctor-patient relationship. The EGPRN TATA group selected the WAI SR as the most reliable and reproducible scale to assess therapeutic alliance. To use it within Europe, it was necessary to translate it into most European languages. The following study aimed to assess the linguistic homogeneity of five of these translations. Research question: Are the translations of the WAI SR homogeneous between Spain, Poland, Slovenia, France and Italy? Methods: Forward-backward translations were achieved in five participating countries (Spain, Poland, France, Slovenia and Italy). Using a Delphi procedure, a global homogeneity check was then performed by comparing the five backward translations during a physical meeting involving GP teachers/researchers from many European countries; the heterogeneity of the participants' origins was a token of reliability. Results: In the assessment of the five translations, 107 experts participated. A consensus was obtained in one to two Delphi rounds for each. During the 'homogeneity check,' some discrepancies were noted with the original version and were discussed with the local teams. This last stage permitted to highlight cultural discrepancies and real translation issues and to correct if needed. Conclusion: Five homogeneous versions of the WAI SR are now available in five European languages. They will be helpful to evaluate therapeutic alliance at different levels: for GPs in daily practice, for students during the initial and continuous training, and for further research in these five countries.
   Background: The patient enablement instrument (PEI) is an established patient-reported outcome measure (PROM) that reflects the quality of a GP appointment. It is a six-item questionnaire, addressed to the patient immediately after a consultation. Research question: The study aimed to evaluate whether a single-item measure (the Q1), based on the PEI, or a single question extracted from the PEI itself (the Q2) could replace the PEI when measuring patient enablement among Finnish healthcare centre patients. Methods: The study design included (1) a pilot study with brief interviews with the respondents, (2) a questionnaire study before and after a single appointment with a GP, and (3) a telephone interview two weeks after the appointment. The correlations between the measures were examined. The sensitivity, specificity and both positive and negative predictive values for the Q1 and the Q2 were calculated, with different PEI score cut-off points. Results: Altogether 483 patients with completed PEIs were included in the analyses. The correlations between the PEI and the Q1 or the Q2 were 0.48 and 0.84, respectively. Both the Q1 and the Q2 had high sensitivity and negative predictive value in relation to patients with lower enablement scores. The reliability coefficients were 0.24 for the Q1 and 0.76 for the Q2. Conclusion: The Q2 seems to be a valid and reliable way to measure patient enablement. The Q1 seems to be less correlated with the PEI, but it also has high negative predictive value in relation to low enablement scores.
   Multimorbidity challenges existing healthcare organization and research, which remains disease and single-condition focused. Basic science approaches to multimorbidity have the potential to identify important shared mechanisms by which diseases we currently think of as distinct might arise, but there is a pressing need for more applied and health services research to understand better and manage multimorbidity now. There are several recent clinical guidelines, which make recommendations for managing multimorbidity or related issues for patients such as polypharmacy and frailty. However, the evidence base underpinning these recommendations is often weak, and these guidelines, therefore, also help define a research agenda. A key problem for researchers and health services is that multimorbidity is very heterogeneous, in that 'intermittent low back pain plus mild eczema' presents very different challenges to researchers and health services compared to 'active psychosis plus severe heart failure'. Identifying important but tractable research questions is therefore not always straightforward. This presentation will identify important gaps in the evidence, and illustrate how they might be filled. The focus will be on two areas where there is consensus that better evidence is needed to inform care design and delivery: (1) organizational interventions to implement more coordinated and holistic care; and (2) interventions to improve medicines management in people with multimorbidity and polypharmacy. These illustrate both the potential for imaginative research, but also the scale.
   Background: The accumulation of multiple chronic diseases (multimorbidity) and multiple prescribed medications (polypharmacy) over time may influence the extent to which an individual maintains health and well-being in later life. Research question: This research aims to describe the patterns (sequence and timing) of multimorbidity and polypharmacy that accumulate over time among primary healthcare patients in Canada. Methods: Data are derived from the Canadian primary care sentinel surveillance network (CPCSSN) electronic medical record (EMR) database that holds >= 1 million longitudinal, de-identified records. Multimorbidity will be identified with 20 categories, cut-off points of >= 2 and >= 3 chronic conditions and the International Classification of Disease (ICD) classification system. Polypharmacy will be identified using the cut-off points of >= 5 and >= 10 medication classes and the Anatomical Therapeutic Chemical (ATC) classification system. Analyses will be conducted using Java and Stata 14.2 software. Results: The prevalence of chronic diseases and prescribed medications will be presented, as well as the patterns that are observed among adults and older adults in Canada. The most frequent patterns (combinations and permutations) of multimorbidity and polypharmacy will be presented, stratified by sex and age category. The relationships with other factors, such as the presence of frailty, disability or increased health service use, will be examined. As well, the methodological challenges to identifying the presence and sequence of multimorbidity and polypharmacy in national, longitudinal data will be discussed. Conclusion: This research will explore the profiles of multimorbidity and polypharmacy in mid- and late-life using a national, longitudinal database. These findings can be used strategically to inform healthcare delivery and to contribute to the understanding of multimorbidity and polypharmacy in the international literature. Reducing the burden of prescribed medications and the harms of polypharmacy are key tasks within the context of multimorbidity.
   Background: Multimorbidity and polypharmacy have become the norm for general practitioners (GPs). Ideally, GPs search for inappropriate medication and, if necessary, deprescribe. However, it remains challenging to deprescribe given time constraints and little backup from guidelines. Furthermore, barriers and enablers to deprescribing among patients have to be accounted for. Research question: To identify barriers and enablers to deprescribing in older patients with polypharmacy. Methods: We surveyed among patients >70 years with multimorbidity (>2 chronic conditions) and polypharmacy (>4 regular medicines). We invited Swiss GPs to recruit eligible patients, each of whom completed a paper-based survey on demography, medications and chronic conditions. We applied the revised patients' attitudes towards deprescribing (rPATD) questionnaire and added 12 additional questions and two open questions to assess barriers and enablers towards deprescribing. Results: We analysed the first 221 responses received so far and full results will be presented at the conference. Participants were 79.3 years in mean (SD 5.8) and 48% female. Thirty-one percent lived alone, and 85% prepared their medication themselves, all others required help. Seventy-six percent of participants took 5-9 regular medicines and 24% took >= 10 up to 22 medicines. Participants (76%) were willing to deprescribe one or more of their medicines and 78% did not have any negative experience with deprescribing. Age and gender were not associated with their willingness to deprescribe. Important barriers to deprescribing were satisfaction with drugs (96%), long-term drugs (56%) and noticing positive effects when taking them (92%). When it comes to deprescribing, 89% of participants wanted as much information as possible on their medicines. Having a good relationship with their GP was a further key factor to them (85%). Conclusion: Most older adults are willing to deprescribe. They would like to be informed about their medicines and want to discuss deprescribing to achieve shared decision-making with the GP they trust.
   Background: With growing populations of patients with multimorbidity, general practitioners need insight into which patients in their practice are most in need for person-centred integrated care ('high-need' patients). Using data from electronic primary care medical records to automatically create a list of possible 'high need' patients could be a quick and easy first step to assist GPs in identifying these patients. Research question: Can 'high need' patients with multimorbidity be identified automatically from their primary care medical records? Methods: Pseudonymized medical records of patients with multimorbidity (>= 2 chronic diseases) were analysed. Data was derived from the Nivel primary care database, a large registry containing data routinely recorded in electronic health records. This includes data on healthcare use, health problems and treatment. Logistic regression analysis was conducted to predict outcomes (frequent contact with the general practice, ER visits and unplanned hospital admissions). Predictors were age, sex, healthcare use in the previous year, morbidity and medication use. Results: In total, 245 065 patients with multimorbidity were identified, of which 48% were above the age of 65 and 57% female. More than 42% had five GP contacts in the previous year and 62% used five or more different medications. Frequent contact with the general practice could be reliably predicted using only the number of contacts in the previous year (AUC: 0.82). Adding all other predictors (including specific chronic conditions) only improved the predictive value of the model marginally (AUC: 0.84). Identifying patients with a high risk for ER visits and unplanned hospital admissions proved more difficult (AUC: 0.67 and 0.70, respectively). Conclusion: 'High need' patients with multimorbidity can be automatically selected from primary care medical records using only the number of contacts with the general practice in the previous year. Composing a list of these patients can help GPs to identify those eligible for person-centred integrated care.
   Background: Chronic diseases usually have a long duration and slow progression and, as a result, they tend to aggregate in multimorbidity patterns (MPs) during the life course and/or due to shared underlying pathophysiological pathways. Knowledge of how MPs progress over time is necessary to develop effective prevention management strategies. Research question: What are the most likely MPs over time? Which longitudinal shifts from one pattern to another occur during follow-up? Methods: A prospective longitudinal study based on electronic health records was conducted during 2012-2016 in Catalonia, Spain. For people aged >= 65 years, we extracted data on demographics and diagnostic codes for chronic diseases (ICD-10). Machine-learning techniques were applied for the identification of disease clusters using fuzzy c-means analysis to obtain initial clusters. To estimate longitudinal MPs and their progression for each individual a hidden Markov model was fitted, estimating: (1) the transition probability matrix between clusters; (2) the initial cluster probability; (3) the most likely trajectory for each individual. The prevalence of disease in each cluster, observed/expected ratios (O/E ratios) and disease exclusivity was determined for each MP. Criteria used to designate cluster: O/E ratio >= 2. Results: In total, 916 619 individuals were included. Ten MPs were identified. The cluster including the most prevalent diseases was designated non-specific (42.0% of individuals). The remaining nine clusters included the following anatomical systems: ophthalmologic and mental diseases (19.3%), osteometabolic (7.9%), cardio-circulatory (6.6%), and others. Most patients, minimum 59.2%, remained in the same cluster during the study period. The highest transitions to the mortality state were observed in the cardio-circulatory (37.1%) and nervous (31.8%) MPs. Conclusion: Ten significant longitudinal MPs were found. The application of sophisticated statistical techniques ideally suited the study of the MPs and allowed for characterization over time. This method is useful to establish a probabilistic evolution of MPs.
   Background: Quality of life is an essential theme for quantitative surveys in primary care. Treatments and procedures need to be assessed on whether they change patients' quality of life. This has led to the creation of evaluation scales. The purpose of this study was to determine reproducibility and efficiency of 11 previously selected quality of life scales (selected with a systematic review) for the general population. Research question: What is the best possible reproducible and efficient quality of life scale for the general population? Methods: The search was conducted from November 2017 to April 2018 in PubMed and Cochrane databases, according to the PRISMA (preferred reporting items for systematic reviews and meta-analyses) protocol. The inclusion criteria were the psychometric qualities for each of the 11 scales studied. Articles dealing with subpopulations or those not written in IMRAD format were excluded. The collected values were reproducibility and efficiency. Results: Out of 206, 46 selected articles were included. Cronbach's alpha by domain and Pearson's coefficient were the most analysed psychometrics. No valid efficiency data was obtained. The internal consistency was over 0.7 for the SF-36, SF12v2 and EQ-5D scales. The Pearson coefficient was over 0.4 for the SF36v2, SF-12 and SF-12v2 scales. The Cohen's kappa ranged from 0.4 to 0.80 for the EQ-5D questionnaire. Conclusion: No scale is fully validated. Reproducibility values were incomplete (Cronbach's alpha and Pearson's most expressed). No efficiency data was found. The most validated scales are the SF family and the EQ-5D. Researchers and clinicians should be aware of these limitations when choosing a quality of life scale. They should return to the scales' designs to choose the one that underlines the type of quality of life they want to assess as no external validity is available.
   Background: Previous studies have shown an increased rate of infection among patients with diabetes; however, it is unclear from these studies if the level of HbA1c is correlated with infection. Research question: This study aimed to examine the association between glycaemic control of type 2 diabetes patients and the incidence of infections. Methods: An HMO database was used to identify all DM patients. The first HbA1c test during the period of the study was selected for each patient; then an infection diagnosis was searched in the 60 days that followed the test. We compared the HbA1c test results that were followed by an infection to those that were not. After applying exclusion criteria: having cancer, receiving immunosuppressive medication, undergoing dialysis treatment, anaemia less than 9 mg%, and G6PD deficiency, there remained 33 637 patients in the cohort. The study period was October 2014 to September 2017. The following information was collected: age, gender, socio-economic index, BMI, use of hypoglycaemic and steroid medication in the 90 days before infection, and comorbid conditions (IHD, PVD, CVA, CCF, asthma, COPD, Parkinson's disease, dementia, CRF). Results: In total, 804 patients had an infection within 60 days following an HbA1c test. For cellulitis, cholecystitis, herpes zoster, pneumonia and sinusitis the HbA1c was higher than those patients that had no infection (for cellulitis 7.603 vs 7.243). When factored into logistic regression analysis, we found that other chronic diseases increased the risk of infection between 29 and 60%. Each increase of a gram of HbA1c increased the risk by 8.5%. Use of steroids in the 90 days before the infection increases the chance of infection by 734%. Conclusion: Increasing HbA1c and comorbidity both increase the risk of infection among type 2 diabetics but use of oral or injectable steroids is a much more significant risk factor.
RI Baldissera, Annalisa/AHD-6334-2022; DSILVA, BROOKE/HCI-4879-2022; Fazli,
   Ghazal/AAE-8320-2022; Blondeel, Sofie/AAE-5307-2022
SN 1381-4788
EI 1751-1402
PD JUL 3
PY 2019
VL 25
IS 3
BP 164
EP 175
DI 10.1080/13814788.2019.1643166
UT WOS:000481779500010
ER

PT J
AU [Anonymous]
AF [Anonymous]
TI General Practice and the Community: Research on health service, quality
   improvements and training. Selected abstracts from the EGPRN Meeting in
   Vigo, Spain, 17-20 October 2019 Abstracts
SO EUROPEAN JOURNAL OF GENERAL PRACTICE
AB Background: Social isolation, loneliness and anxiety-depressive states are emerging health conditions in the elderly. Research question: To assess whether a 4-month programme of physical activity in a group improves the emotional, social and quality of life situation in a sample of subjects over 64 years old people. Methods: Multi-centre randomized clinical trial of two groups. Study population: Patients older than 64 years assigned to three primary care teams from different locations. Inclusion criteria: Submit a score <32 on the DUKE-UNC-11 social support scale, or >12 on the Beck Depression Scale, or >10 on the Generalized Anxiety Scale (GAD-7), at the start of the study. The intervention group participated in a group physical activity program for 4-months that consisted of progressively walking sessions two days a week, 60-150 minutes long depending on the physical condition of each participant. Results: Enrolled were 94 patients who met the inclusion criteria. Mean age was 74 years (SD 5.18) and 76.6% were women. No significant differences were found at the beginning of the study between the two groups in relation to the outcome of the scales evaluated. Once the intervention was completed, improvement in the quality of life and social support was detected in the intervention group (p<.05). Both groups improved the depression and anxiety clinic but the improvement in the participants of the intervention group was higher. Those with initial depression improved 8.6 points on the scale, compared to the control that improved 3.3 points, with the final average of 17.4. Those who presented initial anxiety improved 8 points (final average: 7.5 points, cut-off point for the diagnosis of anxiety 10), compared to the control that improved 5.1 points. Conclusion: The results of this study indicate that the program developed has positive effects on improving the quality of life, social support and depression and anxiety clinic.
   Background: Tourism represents 45% gross domestic product in Balearic Islands. Working as a hotel housekeeper (HH) has been associated with important morbidity, especially musculoskeletal, chronic pain, a significant number of sick leaves, a high consumption of medication, poor psychological well-being and worse quality of life. Research question: Explore perceptions and opinions regarding the HH's work and health problems. Estimate and evaluate HH's health determinants, the exposition to several occupational risk factors, their lifestyles and health problems and their quality of life. Methods: Design: mixed methods: (1) exploratory qualitative study (QS) including 10 semi-structured interviews and six focus groups; (2). descriptive study (DS): individual interviews and clinical medical records. Inclusion criteria: older than 18 years, had worked during the last summer season in the Balearic Islands. Analysis: QS: transcription and content analysis; DS: descriptive statistical analysis. Results: QS: Identified positive aspects of their work: timetables, relationship with co-workers, attending clients. Highlighted negative aspects: working conditions, hard physical workload, stressful duties and insufficiently rewarded. HH associated their health problems with their work; coping strategies: self-medication or visiting their general practitioner. DS: 1.043 HH included. Mean age 43.3 years, mean working years as HH 10.7 years. Mean rooms/day: 18.1 (+/- 6.5); mean beds/day: 44.6 (+/- 20.7). HH reported often pain during the last summer season: 68.2% (IC 95% 65.3-71.0) low back pain; 60.9% (IC 95% 57.8-63.8) wrist and hands; 55.3% (IC 95% 52.2-58.3) cervical. 41.6% and 35.1% self-reported regular and poor health status, respectively. Conclusion: HH perceived hard and stressful working conditions, partly justified by the number of rooms and beds made per day. They also perceived health problems related to their work. HH frequently reported pain during the last summer season. Moreover, they perceive regular or poor health status, weaker than women from the same social class do.
   Background: Gender-based violence (GBV) is a public health and human rights issue, being highly prevalent (12-51%), repetitive and having a severe impact on women's health, with a high sanitary and social cost. Primary care has a key role in detection and management. There is low detection and delay in diagnosis. There is a lack of preparation to recognize abuse, especially in the approach and action after detection. Greater awareness and sensitization is required. Research question: Can a brief specific training intervention in GBV imparted to primary health care professionals in their primary health centre increase knowledge, improve attitudes and skills? Methods: A cluster-randomized clinical trial was carried out in Vigo area primary health centres with at least 20 health care professionals. A basal evaluation was made through a validated inquiry (PREMIS), which they had to retake after three months. In the intervention centres, a clinical session was imparted. pResults: Out of 264 primary health care professionals, 145 participated. There was a 63.5% loss out of 145 professionals. A statistically significant difference was detected in the field of knowledge, increasing an average of two points on a scale from 0 to 5 in these aspects: how to make appropriate questions; connections between GBV and pregnancy; why do not they leave their partners; risk determination and phases of GBV. There was also a decrease in the idea that if the patient does not recognize gender violence, there is very little that can be done. No significant differences were detected in the detection and follow-up. Conclusion: Significant differences were found in the knowledge and attitude sections after performing the intervention to the professionals. The results support the implementation of continuous brief training on GBV in primary care.
   Background: Out-of-hours (OOH) primary care is a topic of great interest in European countries. Reasons for this are similar across borders: to guarantee continuity of care with decreasing numbers of health care workers and to guard equity in OOHcare for all patients. In OOHcare research, valid and accessible research data are needed to fill the knowledge gap. iCAREdata aims to offer valid and immediately available information from OOHcare. Research question: How feasible is it to collect, store and link data of different OOH services in Belgium and to improve data quality registration? How useful are aggregated data to inform stakeholders, to evaluate (the quality of) services in OOH care and the effects of interventions? Methods: As a first achievement, data flows, encryption and encoding were carefully designed and implemented. Solid cooperation with the federal eHealth web services as a trusted third party was crucial. Ethical approval and approval by the data protection authority was obtained. Clear agreements were established concerning access control. A strict code of conduct was agreed upon. A steering committee was established to guard the procedures. Results: First data were collected in 2015. iCAREdata now receives +/- 3000 unique patient contacts per weekend, spread over 14 general practice cooperatives, and covering about a quarter of the Flemish population. Aggregated data, directly processed, are provided weekly on . This portal site offers an overview of, among others, the latest diagnostics, drug prescriptions and workload. iCAREdata project also collects data from emergency departments in hospitals and community pharmacists and link them to evaluate further OOH primary care. Conclusion: Developing a research database on OOHcare is feasible. The iCAREdata project succeeds in an automated output every week, offering insights on the evolution of morbidity, services and effects of interventions. Careful validation and interpretation of the data is a crucial ongoing challenge.
   Background: More than half of decompensations of heart failure are attended in primary care setting. No score that helps to ascertain the short-term prognosis in these patients. Research question: To develop and validate a short-term score (30 days) to predict hospitalizations or death in patients attended in primary care as a consequence of decompensation of heart failure, based on variables easily measurable in primary care setting Methods: Prospective multinational cohort study including patients treated because of a heart failure decompensation in primary care setting. There were a derivation (Spain) and a validation cohort (nine European countries). Results: The derivation cohort included 561 patients, women were 56%, mean age was 82.2 (SD 8.03) years and 31.5% of patients were hospitalized or died in the first month. In the validation cohort, 238 patients were included, women were 54%, mean age was 79.0 (10.4) years and 26.9% of patients were hospitalized or died in the first month. According to the multivariate models, sex, age, hospital admission due to heart failure the previous year, and a heart rate greater than 100 beats/minute, orthopnoea, paroxysmal nocturnal dyspnoea, NYHA functional stage III or IV, saturation of oxygen lower than 90% or an increase in the dyspnoea at the consultation with the General practitioner were included in the HEFESTOS-SCORE. The multivariate model including these variables showed a good calibration (Hosmer-Lemeshow p=.35) and discrimination (AUC 0.81, 95% CI 0.77-0.85). In the validation cohort, the model presented an adequate external validation with good calibration (Hosmer-Lemeshow p=.35) and discrimination (AUC 0.74, 95% CI 0.67-0.82). Conclusion: The HEFESTOS-SCORE, based on clinical and demographical variables easily measurable in primary care is a useful tool to stratify the short-term hospitalization and mortality in patients attended because of a heart failure decompensation.
   Background: Despite recommendations against long-term benzodiazepine (BZD) use, they are often prescribed during months or years in primary care. Research question: To determine facilitators and barriers that explain the variation in implementation of a primary care educational and feedback intervention targeted to general practitioners (GPs) to reduce BZDs prescriptions. Methods: A hybrid type I clinical trial: qualitative data to evaluate the implementation outcomes. Three health districts of Spain: Balearic Islands, Tarragona-Reus district (Catalonia) and Arnau de Vilanova lliria district (Valencia). Forty stakeholders (GPs) participated in five focus groups; they were selected based on their effectiveness of the intervention results: high (three groups) or low (two groups) and individual interviews to two GP of low efficiency. The Consolidated Framework for Implementation Research (CFIR) was used to guide collection and analysis of qualitative data. Two researchers evaluated the qualitative data of the focus groups by the Codebook and Rating Rules of CFIR, independently. Results: Of the 31 CFIR constructs assessed, three constructs strongly distinguished between GPs with low versus high success of the intervention (intervention complexity, individual state of change, key stakeholders engaging), seven additional constructs weakly distinguished (adaptability, external policy and incentives, implementation climate, compatibility, relative priority, self-efficacy, formally appointed internal implementation leaders), 10 had insufficient data to assess and 11 were non-related to the success of the intervention. Conclusion: We identified the constructs that explain the variation in the effectiveness of the intervention; this information is relevant to redesign successful implementation strategies focused on these constructs to implement the BENZORED intervention in health services.
   Background: Despite recommendations against long-term benzodiazepine (BZD) use, they are often prescribed during months or years in primary care. Research question: To evaluate the effectiveness of a primary care educational and feedback intervention targeted to general practitioners (GPs) to reduce BZDs prescriptions. Methods: Design: A two-arm parallel cluster randomized clinical trial. Settings: Primary Healthcare centres from three health districts of Spain: Balearic Islands (IbSalut), Catalonia (Institut Catala de la Salut; Tarragona-Reus district) and Community of Valencia (Conselleria de Salut Universal; Arnau de Vilanova lliria district). Participants: All GPs from the health districts included were invited to participate. Ninety percent of the GPs accepted to participate. Intervention: GPs received an educational two hours workshop training about the rationale for prescribing BZDs and deprescribing strategies for long-term BZD users, audit and monthly feedback about their prescription and access to a support web page with information to help them and leaflets to give to the patients. Control group: GPs did not receive any component of the intervention. Outcomes: Defined daily dose (DDD)/1000 inhabitants/year (DHD) of BZDs prescribed by GP at 12 months. Proportion of long-term BZD users (>6 months) and in patients aged 65 or more at 12 months. Statistical analysis: Generalized mixed linear random effect models to account for clustering at the level of healthcare centre and all analyses were based on an intention to treat principle. Results: We included 749 GPs and 49 (6.5%) were lost to follow-up. Adjusted difference between groups in DHD at 12 months was -3.26 (-4.87;-1.65), p<.001. The differences in the proportion of long-term BZD users was -0.39 (-0.58;-0.19), p<.001 and in patients older than 65 was -0.87 (-1.35;-0.26), p=.004. Conclusion: An educational and feedback intervention targeted to GPs is effective to reduce BZD prescription in primary care.
   Background: Patients who might also go to the general practitioner (GP) frequently consult emergency departments (ED). This leads to decreased efficiency, high workload at the ED and additional costs for both government and patient. Research question: The primary outcome is the proportion of patients who enter the ED and are handled by the GP after triage. Secondary outcomes: Referral rate to the ED by the GP, proportion of patients not following the triage advice, compliance of the nurse to the triage-instructions and health insurance expenditures. Furthermore, facilitators and barriers will be studied and an incident analysis will be performed. Methods: This is a randomised controlled trial with weekends serving as clusters. Patients presenting at the ED during OOH are triaged and allocated to either ED or GP by a trained nurse using an extension to the Manchester Triage System (MTS). During control clusters, all patients remain at the ED. Data are collected using a database for OOH care (iCAREdata). Results: So far, 296 out of 2733 (11%) patients were allocated to the GP. Two-thirds (194) of these patients did go to the GP leading to a primary outcome of 7% for 14 intervention weekends. Only eight patients were referred back to the ED. Compliance of the nurse to the extended MTS was 93%, in 6% of the cases the nurse chose ED instead of GPC and in less than one percent GPC instead of ED. The nurses chose higher urgency categories and more discriminators, leading to the GP during intervention clusters. Using an automated system, these results are updated weekly, on our poster, we will show more results that are complete. Conclusion: These first results reveal a low efficiency but a high safety of the intervention. More prolonged data collection combined with a process analysis and cost efficiency study is necessary before definitive conclusions can be drawn.
   Background: Low back pain is a multifactorial condition with individual and societal impact. Psychosocial factors play a larger prognostic roll. Therefore, earlier multidisciplinary treatment strategy (physical, psychological and social/occupational) could be applied to search improvement in fear-avoidance beliefs with positive effect in the evolution of low back pain. Research question: Evaluate the effectiveness of a biopsychosocial multidisciplinary intervention (physiotherapy, cognitive-behavioural and pharmacological therapy) through the changes in fear-avoidance beliefs (FABs), in working population with sub-acute non-specific LBP, compared to usual clinical care at 3 and 12 months. Methods: A cluster randomised clinical trial, conducted in 39 Primary Health Care Centres (PHCC) in Barcelona. Participants between 18 and 65 years old (n = 369; control group =188, PHCC 26 and intervention group =181, PHCC 13). Control group received usual care, according to guidelines. Intervention group received usual care plus a biopsychosocial multidisciplinary intervention (sessions 10 hours/total). The main outcome was the Fear-Avoidance Beliefs questionnaire (FABQ). Other outcomes: Evolution to chronicity. Assessment at baseline, 3 and 12 months. Analysis was by intention to treat and analyst blinded. Multiple imputations. Results: Of the 369 enrolled patients with LBP, 421 (84.0%) provided data at the three months of follow-up, and 387 (77.2%) at 12 months. Mean age of study subjects at baseline was 45.1 (SD: 10.4) years-old and 61.2% were women. At baseline, there were no differences. Both groups showed a decrease in FABQ (FAB physical and FAB-work) at three months and twelve months, with a significant difference at long-term. At FAB-physical performance, there was no significant difference over the follow-up time and at FAB-Work, a substantial difference at 12 months between groups. Conclusion: A multidisciplinary biopsychosocial intervention showed a positive effect in FABs by improving fear behaviours and avoidance at work.
   Community participation in primary healthcare is enshrined in international policies since the 1970s and has been re-emphasised since then, most recently in the 2018 WHO Astana Declaration (). The concept comes from a social justice perspective. It emphasises that the participation of communities who experience poverty and social exclusion is essential to the development of primary health care services shaping these services and making them relevant to those with the greatest need. This is important if we are to address the well-documented Inverse Care Law. There is, however, a translational gap between policy and practice. The stability of policies for community participation in primary healthcare is patchy. The implementation of policies into conventional ways of working is patchy. Where implementation has occurred, the coverage of community participation initiatives can be patchy - not all community members are involved. The literature shows a pattern of exclusion whereby so-called 'hard to reach' groups are not adequately involved in primary healthcare decision-making. This is the case for refugees and migrants who arrive to settle and integrate into host countries in Europe. The recent WHO Strategy and Action Plan for Refugee and Migrant Health (2016; ) is a call for action to disrupt this pattern of exclusion and improve the health of refugees and migrants. Drawing on the rich tradition of participatory health research is a valuable way forward because it provides important concepts, tools and techniques for research that is more inclusive and primary care practice. This presentation will describe innovative examples of success in family practice settings from around Europe. These have brought together refugees and migrants with primary care stakeholders and enabled them to work together to introduce and sustain changes in clinical practice. This evidence can be used to guide and strengthen community participation in primary healthcare, for all.
   Background: Community participation is essential for effective implementation of research programmes in primary healthcare (PHC) but also appropriate interpretation of results and optimal delivery of subsequent care. Stakeholder engagement undertaken under defined and evaluated frameworks may be key for the establishment of concrete collaboration and communication between communities and other parties involved in research. This abstract aims to report on community and stakeholder engagement methodologies, plans and activities of European research projects conducted in Crete, Greece. Research question: Could a consensus be reached regarding the methods and tools for enhancing stakeholder engagement in community-oriented PHC research? Methods: Examined programmes included RESTORE (FP7), FRESH AIR (Horizon2020) and VIGOUR (Health Programme). Identified methodologies included Normalisation Process Theory, Participatory Learning and Action, Five Steps of Stakeholders' Engagement, establishment of Stakeholder Engagement Groups under the 9 C's model (commissioners, customers, collaborators, contributors, channels, commentators, consumers, champions, competitors) and Structured Democratic Dialogue. These were implemented to a range of stakeholders, including community members, patients, migrants, Roma populations, healthcare professionals and policy-makers. Qualitative research (focus groups, individual interviews) and Thematic Content Analysis were used for design and analysis of engagement activities. Results: In RESTORE, migrants and other stakeholders selected guidelines and training supporting cross-cultural communication in PHC consultations, based on their own needs and expectations. Community members, healthcare professionals and healthcare authorities were actively involved in FRESH AIR by identifying local priorities and contextual factors for designing project interventions, providing access to communities and supporting dissemination of project achievements. In VIGOUR, multidisciplinary stakeholders were brought together and formulated a joint ambition statement for the future of integrated care in Crete. Conclusion: Various stakeholder engagement methods with documented effects are currently available. Their systematic identification, appraisal, synthesis and consolidation may serve with enhancing community participation in PHC, sustaining research results and translating findings into appropriate actions.
   Background: Screening for prostate cancer remains controversial, implying a trade-off between benefits and harms, and a shared decision-making process has been advocated. Decision aids are evidence-based tools that improve decision quality. For limited-resource countries, translating and making cultural adaptations to high-quality decision aids is a reasonable alternative to developing new ones. Research question: We aimed to translate and culturally adapt an English language patient decision aid addressing prostate cancer screening, so that Portuguese men can use it. Methods: We followed the European Centre for Disease Prevention and Control's (ECDC) five-step, stakeholder-based approach to adapting health communication materials: (1) selection of materials and process coordinators; (2) early review; (3) translation and back translation; (4) comprehension testing with cognitive semi-structured interviews; (5) proofreading. Cognitive interviews were conducted with 15 men, ages 55-69, from the Oporto district local community to refine the decision aid after its translation. Content analysis was performed using Ligre (TM) software. Results: Five main themes are presented: informational content, information comprehension, socio-cultural appropriateness, feelings and primary message, and personal perspective concerning prostate cancer screening. For each theme, illustrative quotes extracted from men's interviews are presented. Most men found the translated version of the decision aid to be clear, comprehensive and appropriate for its target population, albeit some suggested that medical terms could be a barrier. The data collected from men's interviews allowed the researchers to clarify concepts and expand existing content. Conclusion: The final version of the decision aid can be used in the real world clinical setting and our ECDC based approach can be replicated by other workgroups to translate and culturally adapt decision aids.
   What are we talking about when we talk about value? In 2006, Michael Porter and Elizabeth O. Teisberg published; Redefining Health Care, Creating Value-Based Competition on Results, Harvard Business School Press. Affirming that payers and providers, including doctors and nurses, are very concerned in demonstrating that they work a lot, and very little, or nothing, in assessing what their work contributes to the health of people and communities. Michael Porter is famous in the business world for his work on competitiveness based on the value of products and services. He has introduced this concept in the provision of health services, all summarised in a phrase: Health systems should seek to obtain the maximum possible value for the health of people for every dollar they spend. However, to define the value in healthcare, the patient must be introduced into the equation, so, in Porterian terms, the value is the perception that people have about clinical effectiveness and the costs of therapeutic processes. Clinical effectiveness is measurable from epidemiology (to be readmitted to a hospital fewer times or living longer); value, on the other hand, is reflected by people's experience. We need to ask questions such as do patients with advanced diseases want to live longer, or they want to enjoy the highest quality of life possible. Depending on the response, we can develop different delivery models. What is value-based healthcare? According to NEJM Catalyst, Value-based healthcare is a healthcare delivery model in which providers, including hospitals and physicians, are paid based on patient health outcomes (). Under value-based care agreements, providers are rewarded for helping patients improve their health, reduce the effects and incidence of chronic disease, and live healthier lives in an evidence-based way. How to achieve a Value-Based Healthcare Model The following six drivers are the key to make a primary health care system a value-based healthcare model: 1. Prioritising patient-centred care. 2. From clinical pathways to care delivery value chains. 3. Promoting the right care and reducing medical overuse. 4. Turning a fragmented model into another integrated model. 5. Creating the enabling environment for healthcare transformation. 6. Fostering community health. How to develop a Value-Based Community Health Michael Marmot states that if the determinants of health are mostly social, solutions must also be social, so to improve the quality of community life, political systems require economic, housing, education, security and infrastructure programs (Am J Public Health. 2014;104:S517-S519). Nevertheless, the healthcare system must know how to adjust resources according to the social circumstances of each community and to understand how to provide a health-oriented vision of all the social programs. On the other hand, community health is an intervention model that aims to improve the health of a defined community that should operate from primary care services to adjust their actions to the social reality of each territory.
   Background: Geriatric care needs to be increased with growing elderly populations. The Borgholm jurisdiction in the Baltic Island of oland (Kalmar region) has an older than average senior population and had difficulties recruiting primary care physicians (PCPs) resulting in high elderly hospital care consumption. Research question: Could a new model of geriatric care be able to decrease the hospital care needs of Borgholm as compared to the rest of Kalmar region? Methods: A new model of care was developed in Borgholm 2016-2017 where the PCP list was limited to 1000 patients, daily slots for PCP home care visits could be booked by community nurses or ambulance nurses and PCPs had daily anticipatory care planning contacts with Kalmar hospital staff. Results: Between 2014 and 2018, Borgholm home care patients >75 years old increased by 70% vs. a 2% decrease for the rest of Kalmar region. Similarly, Kalmar emergency department visits decreased by 19% in Borgholm vs. 9% increase for the rest of Kalmar. Also, Kalmar hospital care episodes decreased 7% in Borgholm vs. 13% increase for the rest of Kalmar; Kalmar hospital outpatient visits decreased 8% in Borgholm vs. 21% increase for the rest of Kalmar; total care consumption for >75 years old decreased 4% in Borgholm vs. a 10% increase for the rest of Kalmar region. Conclusion: A new geriatric care model consisting of a comprehensive collaboration between strengthened primary care and community care, hospital care and ambulance care was associated with a reduction in total care consumption for senior citizens in a rural Swedish jurisdiction.
   Background: Primary health care, the general practitioner, plays a critical role for early identification and care of patients with dementia. Early diagnosis of dementia allows starting therapy and improving the quality of life of the patients. Research question: To estimate the prevalence and care of patients with dementia in North Macedonia. Methods: Forty-six general practitioners (GPs) surgeries from 20 cities in Macedonia took part in the project. All individuals age over 65 years with a diagnosis of dementia were identified from GP electronic disease registers. Results: Based on the diagnosis, 450 (3.5%) patients were identified from a total population of 12,926 over 65s. The most common dementia was Alzheimer's dementia 294 (65.3%) followed by vascular dementia 27.11%. The average age of respondents in the study was 77.5 +/- 8.2 years, with 50% patients under the age of 79 years, 65.6% were female and 68.4% were with elementary school. In the entire sample, most of the patients diagnosed with dementia 195 (43.3%) said they lived with another family member. The most common risk factor was hypertension (85.1%), followed by stroke/ transitory ischemic attacks (29.3%) and equal percentage, i.e. 26.4% of patients had high levels of cholesterol and diabetes. To 242 (53.8%) acetylcholinesterase inhibitors were prescribed (donepezil, rivastigmine, galantamine), 77 (17.1%) memantine, while 247 (54.9%) another OTC therapy. 227 (50.4%) reported that they did not receive treatment. An additional analysis of the reasons for not receiving treatment was made on this sample of patients who did not receive treatment. It was found that in the majority of these patients (more than 50%) the reason for not receiving therapy was that it was not prescribed, in 142 (62.6%). Conclusion: This is the first national representative study of dementia prevalence in North Macedonia. Those data can provide information for healthcare needs people with dementia.
   Background: Emotional experience for medical students during clinical internships is often ignored. Yet, its influence on professional skills is certain. Research question: 'What is the emotional experience of second and third-year medical students during their first clinical internship? How do they perceive the management of their experience by their supervisors?' Methods: A qualitative study was conducted with 12 students in their second or third year of medical training at the University of Lille, in France, between 2016 and 2019. Interviews were carried out comprehensively for a total of 17 hours. Following a grounded theory approach, the analysis terminated when data were sufficient to offer a conclusive model. Results: Emotional experience during clinical internship was rich and intense. It was most often ignored and was not taken into account in the development of professional skills. The organized management was deficient. Informal training existed: when a wilful student met a dedicated teacher. Students would have welcomed a possibility to experience intense emotions in a protective environment, and only then in an empowering environment. They expressed the same desire about early exchanges on the experiences of the internship. A modelling of the informants' emotional experiences was realized in the form of three diagrams. Conclusion: Students ask to be challenged to face patients, and then to be listened to about it. Possible interventions are trauma prevention and detection of malaise in the workplace; teaching of humanist values; providing experience and reflexivity through new pedagogical means (such as cinema, theatre, literature, writing), or relational means (such as exchange groups, companionship, solidarity commitment, immersive internships and tutoring); and training supervisors.
   Background: Oral anticoagulants (OAC) reduce the risk for stroke and death from all causes in patients with non-valvular atrial fibrillation (NVAF). Research question: To explore adherence rates to OAC among patients with NVAF and to compare head-to-head adherence rate of different medications in long-term chronic use. Methods: We conducted a population-based cohort study Clalit Health Services, Israel. All patients, 30 years and over, with a diagnosis of NVAF before 2016 and were treated with OAC were included. We included patients that filled at least one prescription per year in the three consecutive years 2016-2018. We analysed all prescriptions that were filled for the medications from 1 January 2017 to 31 December 2017. We considered purchasing of at least nine monthly prescriptions during 2017 as 'good medication adherence.' Results: Twenty-six thousand and twenty-nine patients with NVAF who were treated with OAC were identified. Ten thousand and two hundred and eighty-four (39.5%) were treated with apixaban, 6321 (24.3%) were treated with warfarin, 6290 (24.1%) were treated with rivaroxaban 3134 (12.0%) were treated with dabigatran. Rates of good medication adherence were 88.9% for rivaroxaban, 84.9% for apixaban, 83.6% for dabigatran and 55.8% for warfarin (p<.0001). Good adherence with OAC was associated with lower LDL cholesterol and glucose levels. Advanced age was associated with higher adherence rates (p<.001). SES was not associated with medication adherence. Conclusion: Adherence rates to DOAC among patients with NVAF are high and are higher than the adherence rate to warfarin. It should be taken into consideration when choosing OAC treatment for NVAF.
   Background: In France, cervical cancer screening by pap-smears should be conducted triennially. Screening statistics are based on the number of cytology examinations of smears reimbursed by the Health Insurance appearing in the claim databases. The percentage of screened women is lower based on these data than on declarative surveys. If surveys are overestimating the number of screened women, it is likely that claim databases underestimate it. Research question: The primary objective was to determine the underestimation of screened women in claim databases. The secondary purpose was to estimate the proportion of female patients not reachable by their GP for a cervical cancer screening in an organized screening trial. Methods: The population was the 6327 female patients aged 30-65 years of the 24 GP investigators of the PaCUDAHL-Ge trial. We compared the lists of their female patients that had no cytology of Pap test reimbursed during the three prior years, extracted from the Health Insurance claim databases in 2015 and 2018. We selected the patients appearing on both lists meaning they had not responded to the invitation of their GP to be screened in the trial. We searched in the GPs' records valid reasons not to be screened (hysterectomy, history of cervical lesion, pregnancy, other conditions making screening irrelevant) or evidence of screening. Results: The total number of 'unscreened' women in 2018 was 2731, 1737 patients appeared on both lists, 1522 could be included for analysing, 65 had been screened, 95 had hysterectomy, three had a history of cervical lesion, nine were pregnant and 10 had other conditions making screening irrelevant, 166 patients were lost to view. Conclusion: Based on GPs' records, health insurance claim databases underestimate the number of screened women by 7.6%. The percentage of patients not responding to the invitation of their GP to be screened in the PaCUDAHL-Ge trial is 24.18%.
   Background: Over the past decade, the amount of digital data created by humans with or without connected tools has grown exponentially. The field of primary care (PC) did not escape this digitization, nor the use of Big Data algorithms. To evaluate the results of Big Data research in PC it seemed useful to identify which algorithms are used. Research question: What are the algorithms used for Big Data research in PC research and how are they described? Methods: Systematic review of the literature according to the recommendations of the PRISMA guide. A search equation using the following MeSH terms 'big data, data mining, Algorithms, Artificial Intelligence, Machine learning, Deep Learning, Neural Networks Natural Language Processing, general practice, electronic health records, health records' has been applied to the PUBMED database. After a selection of the titles and article summaries according to the inclusion criteria, the full versions of the eligible articles were read and analysed. Referenced articles of the sources articles were added to the analysis. The algorithms described in the articles were extracted and analysed. Results: In total, 778 articles were identified, 169 were eligible for full reading and 26 articles were finally selected. The algorithms listed in the articles are poorly described. The description is usually limited to a general explanation about how the algorithm works. Seven articles gave a partial description of the algorithm; a logic diagram was given in four articles and the codes in only two. Actually, only one article fully describes the algorithm with its mathematical description, its code and its logic diagram. Conclusion: Big Data algorithms in PC are not satisfactorily described. The lack of reproducibility is not compatible with a consistent scientific approach. Researchers should provide more information about the way they extract and analyse their data to give their readers more confidence in Big Data.
   Background: As a collaborative project of the Family Practice Depression and Multimorbidity group of European General Practice Research Network, the Hopkins Symptom Checklist-25 (HSCL-25) scale was identified as valid, reproducible, effective and easy to use. Subsequently, it has been translated and adapted to 13 languages, including Castilian. Currently, the scale is being validated in different languages. Research question: What are the psychometric properties of the Spanish version of HSCL-25 (HSCL-25e) for depression detection in Primary Care? Methods: HSCL-25e was administered to outpatients recruited by their physicians in six health centres involved in Spanish EIRA3 study, a trial to promote healthy behaviours in people aged from 45 to 75. Patients complimented HSCL-25 themselves. Sample size was calculated with R package (pROC). Statistical analysis: responsiveness was analysed with missing data and detecting ceiling and floor effects for the items. Principal component analysis (PCA) was done to determine the dimensions of HSCL-25e. Item-total correlation, Cronbach's alpha (global and dimensions coefficient) and squared multiple correlation were carried out to calculate internal consistency. Results: Seven hundred and sixty-nine patients out of 806 complimented HSCL-25e, 738 answered to all of the items. No patterns of missing answers were found. No ceiling effects, expected floor effect in item 18. Item 17 was the most consistent one and item 24 was the lower one. All items showed positive discrimination index for both cut-off points (1.55 and 1.75). PCA indicated two factors; 13 items corresponding to depression dimension and the other 12 items corresponding to anxiety subscale. Global Cronbach's alpha was 0.92 (0.88 calculated for depression dimension and 0.84 for anxiety dimension). Conclusion: The HSCL-25e has excellent psychometric properties when applied to Primary Care population. It has two dimensions as the original version, although the items included are not exactly the same. There are more item coincidences with the French version.
   Background: Cardiovascular diseases (CVDs) are the first mortality cause worldwide with 17.5 million death in 2012. Spices (Scaling-up Packages of Interventions for CVD prevention in selected sites in Europe and Sub-Saharan Africa) gathered five countries around CVD primary prevention interventions, especially for populations with low access to prevention and health care system. In France, a rural area where people were more deprived and with a low settlement of general practitioners (GPs) fitted with the project. Research question: What are the barriers and the facilitators for cardiovascular primary prevention implementation from caregivers and patients' point of view of a deprived rural area? Methods: Semi-structured interviews were conducted until theoretical saturation of data. Purposive samplings of GPs, patients, patients' families, nurses and pharmacists were designed. Five interview guides explored cardiovascular prevention, cardiovascular health promotion in the setting, actors of CVD prevention, capacities for CVD prevention, patients' and healthcare professionals' representations, barriers and facilitators in implementing CVD prevention, possible solutions. Guides were adapted concurrently to the analysis. A blinded thematic analysis and a mind-mapping were achieved for each group. Results: Thirteen GPS, 11 pharmacists', 14 nurses, 12 patients' and 12 patients' family members' interviews were achieved. Professionals highlighted a disconnection between them and national prevention programs, lack of time, payment and training for CVD prevention. Countryside was either protective or aggressive regarding CVD risk balancing gardening and space against isolation and lack of structures. GPs had poor connections with the community. Patients described their recklessness and feeling of invulnerability until their CVD appeared. Families could be a barrier to CVD prevention and lifestyle change. Risky behaviours were handed down from one generation to another. Conclusion: Innovative interventions for Spices should focus on these community specificities and individual behavioural strategies in contrast with the six national plans addressing CVD in France. These plans solely concentrate on dissemination of prevention messages and knowledge, which is of little use according to this survey.
RI Campos, Caroline/AAT-5847-2021; Freienberg, Selina/AAV-8829-2021; Fazli,
   Ghazal/AAE-8320-2022
SN 1381-4788
EI 1751-1402
PY 2020
VL 26
IS 1
BP 42
EP 50
DI 10.1080/13814788.2020.1719994
UT WOS:000517857100001
ER

EF